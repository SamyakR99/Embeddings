{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8787c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torchvision.transforms import InterpolationMode\n",
    "BICUBIC = InterpolationMode.BICUBIC\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _ = clip.load(\"CS-RN101\", device=device)\n",
    "preprocess_img =  Compose([Resize((224, 224), interpolation=BICUBIC), ToTensor(),\n",
    "    Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))])\n",
    "\n",
    "\n",
    "preprocess_target =  Compose([Resize((224, 224), interpolation=BICUBIC), ToTensor()])\n",
    "\n",
    "class VOCSegmentationCustom(VOCSegmentation):\n",
    "    def __getitem__(self, index):\n",
    "        img = self.images[index]\n",
    "        target = self.masks[index]\n",
    "\n",
    "        img = Image.open(img) #.convert('RGB')\n",
    "        \n",
    "        cv2_img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "        pre_target = Image.open(target)\n",
    "        target = Image.open(target) #.convert('L')\n",
    "        \n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target, pre_target = self.transforms(img, target, pre_target)\n",
    "\n",
    "        return img, cv2_img, target, pre_target\n",
    "\n",
    "# Load the dataset with the preprocess transformation\n",
    "test_dataset = VOCSegmentationCustom(\n",
    "    root='path/to/VOC2012', year='2012', image_set='val', download=False, \n",
    "    transforms=lambda img,target,pre_target: (preprocess_img(img), preprocess_target(target), preprocess_target(pre_target))\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d521bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC classes, excluding the background class\n",
    "voc_classes = [\n",
    "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \n",
    "    \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \n",
    "    \"dining table\", \"dog\", \"horse\", \"motorbike\", \"person\", \n",
    "    \"potted plant\", \"sheep\", \"sofa\", \"train\", \"tv/monitor\"\n",
    "]\n",
    "\n",
    "all_texts = voc_classes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0c6ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# def calculate_iou(mask1, mask2):\n",
    "#     # Ensure masks are boolean tensors\n",
    "#     mask1 = mask1.bool()\n",
    "#     mask2 = mask2.bool()\n",
    "\n",
    "#     # Compute intersection and union\n",
    "#     intersection = (mask1 & mask2).sum().item()\n",
    "#     union = (mask1 | mask2).sum().item()\n",
    "\n",
    "#     # Compute IoU\n",
    "#     iou = intersection / union if union != 0 else 0\n",
    "#     return iou\n",
    "\n",
    "# threshold = 0.5\n",
    "# miou_list = []\n",
    "# with torch.no_grad():\n",
    "#     i = 0\n",
    "#     for images, cv2_img, targets, pre_target in tqdm(test_loader):\n",
    "        \n",
    "\n",
    "#         ############### This code below will give the GT classes/labels present in the image #############\n",
    "#         sa = pre_target[0].permute(1,2,0).cpu().detach().numpy()\n",
    "#         unique_pixels = np.unique(sa) *255\n",
    "#         true_label = unique_pixels[1:-1]\n",
    "        \n",
    "#         ############### get image and text features: and obtain similarity maps (same as Surgery) #############\n",
    "        \n",
    "#         images = images.to(device)\n",
    "#         targets = targets.to(device)\n",
    "        \n",
    "#         image_features = model.encode_image(images)\n",
    "        \n",
    "#         features = image_features @ text_features.t()\n",
    "#         similarity = clip.clip_feature_surgery(image_features, text_features)\n",
    "#         similarity_map = clip.get_similarity_map(similarity[:, 1:, :], 224)\n",
    "\n",
    "#         #### Now for each class in each image we find the IoU ####\n",
    "#         miou = []\n",
    "#         for idx in true_label:\n",
    "            \n",
    "\n",
    "#             ## This gives the simialrity between prompts of class idx and image\n",
    "#             preds = similarity_map[:,:,:,int(idx-1)].view_as(targets)\n",
    "#             ## Next is just making the pixel values as the class number or the idx. Commonly done in segmentation problems. The class number (0 or 1 - aeroplane will have pixel values as 0 or 1)\n",
    "#             preds = (preds> threshold).float()\n",
    "#             preds = preds * idx\n",
    "        \n",
    "#             preds = preds.to(torch.uint8)\n",
    "#             preds_img =preds[0].permute(1,2,0).cpu().detach().numpy() \n",
    "#             # print('PREDS', np.unique(preds.cpu().detach().numpy()))\n",
    "            \n",
    "#             # plt.imshow(preds_img)  ## 0 is the classname\n",
    "#             # plt.show()\n",
    "#             ## Here, in the target we seperate the index, that is segment only the class or idx. \n",
    "            \n",
    "#             tar_img = targets[0].permute(1,2,0).cpu().detach().numpy() * 255\n",
    "#             tar_img_mask = (tar_img == idx).astype(int) * idx\n",
    "            \n",
    "#             tar_img_mask = 255 * tar_img_mask.astype(np.uint8)\n",
    "#             # print('tar_img_mask', np.unique(tar_img_mask))\n",
    "            \n",
    "#             # plt.imshow(tar_img_mask)\n",
    "#             # plt.show()\n",
    "\n",
    "#             mask1 = torch.tensor(preds_img, dtype  = torch.uint8)\n",
    "#             mask2 = torch.tensor(tar_img_mask, dtype=torch.uint8)\n",
    "\n",
    "#             ## iou is between prediction and target for one class or index. \n",
    "\n",
    "#             iou = calculate_iou(mask1, mask2)\n",
    "#             print('IoU for class {} is {}'.format(idx,iou))\n",
    "#             miou.append(iou)\n",
    "        \n",
    "#         miou_list.append(np.mean(np.array(miou)))\n",
    "#         print('-'*25)    \n",
    "#         # i = i+1\n",
    "#         # if i >5:\n",
    "#         #     break        \n",
    "# # mean_iou = np.nanmean(iou_scores)\n",
    "# # print(f'Mean IoU on PASCAL VOC 2012 test set: {mean_iou:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d08d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.nanmean(np.array(miou_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99be2da5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def calculate_iou(pred, target, num_classes=21):\n",
    "#     ious = []\n",
    "#     pred = pred.view(-1)\n",
    "#     target = target.view(-1)\n",
    "    \n",
    "#     for cls in range(num_classes):\n",
    "#         pred_inds = pred == cls\n",
    "#         target_inds = target == cls\n",
    "#         intersection = torch.sum(pred_inds[target_inds])\n",
    "#         union = torch.sum(pred_inds) + torch.sum(target_inds) - intersection\n",
    "        \n",
    "#         if union == 0:\n",
    "#             ious.append(float('nan'))\n",
    "#         else:\n",
    "#             ious.append(float(intersection) / max(union, 1))\n",
    "    \n",
    "#     return np.nanmean(ious)\n",
    "\n",
    "\n",
    "# miou_list = []\n",
    "\n",
    "# threshold = 0.5\n",
    "# with torch.no_grad():\n",
    "#     i = 0\n",
    "#     for images, cv2_img, targets, pre_target in tqdm(test_loader):\n",
    "        \n",
    "#         iou_scores = []\n",
    "\n",
    "#         ############### This code below will give the GT classes/labels present in the image #############\n",
    "#         sa = pre_target[0].permute(1,2,0).cpu().detach().numpy()\n",
    "#         unique_pixels = np.unique(sa) *255\n",
    "#         filtered_numbers = [num for num in unique_pixels if num not in [0, 255]]\n",
    "\n",
    "#         true_label = filtered_numbers\n",
    "\n",
    "        \n",
    "#         ############### get image and text features: and obtain similarity maps (same as Surgery) #############\n",
    "        \n",
    "#         images = images.to(device)\n",
    "#         targets = targets.to(device)\n",
    "        \n",
    "#         image_features = model.encode_image(images)\n",
    "        \n",
    "#         features = image_features @ text_features.t()\n",
    "#         similarity = clip.clip_feature_surgery(image_features, text_features)\n",
    "#         similarity_map = clip.get_similarity_map(similarity[:, 1:, :], 224)\n",
    "\n",
    "#         #### Now for each class in each image we find the IoU ####\n",
    "#         mask_idx = []\n",
    "#         mask_target = []\n",
    "\n",
    "#         for idx in true_label:\n",
    "\n",
    "#             ## This gives the simialrity between prompts of class idx and image\n",
    "#             preds = similarity_map[:,:,:,int(idx-1)].view_as(targets)\n",
    "\n",
    "#             ## Next is just making the pixel values as the class number or the idx. Commonly done in segmentation problems. The class number (0 or 1 - aeroplane will have pixel values as 0 or 1)\n",
    "#             preds = (preds> threshold).float()\n",
    "#             preds = preds * idx\n",
    "        \n",
    "#             preds = preds.to(torch.uint8)\n",
    "#             preds_img =preds[0].permute(1,2,0).cpu().detach().numpy() \n",
    "#             # print('PREDS', np.unique(preds.cpu().detach().numpy()))\n",
    "            \n",
    "#             # plt.imshow(preds_img)  ## 0 is the classname\n",
    "#             # plt.show()\n",
    "\n",
    "#             ## Here, in the target we seperate the index, that is segment only the class or idx. \n",
    "            \n",
    "#             tar_img = targets[0].permute(1,2,0).cpu().detach().numpy() * 255\n",
    "#             tar_img_mask = (tar_img == idx).astype(int) * idx\n",
    "            \n",
    "#             tar_img_mask = tar_img_mask.astype(np.uint8)\n",
    "#             # print('tar_img_mask', np.unique(tar_img_mask))\n",
    "            \n",
    "#             # plt.imshow(tar_img_mask)\n",
    "#             # plt.show()\n",
    "\n",
    "#             mask1 = torch.tensor(preds_img, dtype  = torch.uint8)\n",
    "#             mask2 = torch.tensor(tar_img_mask, dtype=torch.uint8)\n",
    "\n",
    "#             mask_idx.append(mask1)\n",
    "#             mask_target.append(mask2)\n",
    "        \n",
    "        \n",
    "#         summed_mask_idx = torch.sum(torch.stack(mask_idx), dim=0)\n",
    "#         summed_mask_target = torch.sum(torch.stack(mask_target), dim=0)\n",
    "#         # plt.imshow(summed_mask_idx)  ## 0 is the classname\n",
    "#         # plt.show()\n",
    "#         # print('summed_mask_idx', np.unique(summed_mask_idx.cpu().detach().numpy()))\n",
    "#         # # plt.imshow(summed_mask_target)  ## 0 is the classname\n",
    "#         # # plt.show()\n",
    "#         # print('summed_mask_target', np.unique(summed_mask_target.cpu().detach().numpy()))\n",
    "        \n",
    "#         # ## iou is between prediction and target for one class or index. \n",
    "#         for p in range(224):\n",
    "#             iou = calculate_iou(summed_mask_idx[p].cpu(), summed_mask_target[p].cpu(), num_classes=21)\n",
    "#             iou_scores.append(iou)\n",
    "        \n",
    "#         mean_iou = np.nanmean(iou_scores)\n",
    "#         miou_list.append(mean_iou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6a1fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.nanmean(np.array(miou_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "985c0b4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## Experiment:\n",
    "\n",
    "# def calculate_iou(pred, target, num_classes=21):\n",
    "#     ious = []\n",
    "#     pred = pred.view(-1)\n",
    "#     target = target.view(-1)\n",
    "    \n",
    "#     for cls in range(num_classes):\n",
    "#         pred_inds = pred == cls\n",
    "#         target_inds = target == cls\n",
    "#         intersection = torch.sum(pred_inds[target_inds])\n",
    "#         union = torch.sum(pred_inds) + torch.sum(target_inds) - intersection\n",
    "        \n",
    "#         if union == 0:\n",
    "#             ious.append(float('nan'))\n",
    "#         else:\n",
    "#             ious.append(float(intersection) / max(union, 1))\n",
    "    \n",
    "#     return np.nanmean(ious)\n",
    "\n",
    "\n",
    "# miou_list = []\n",
    "\n",
    "# threshold = 0.5\n",
    "# with torch.no_grad():\n",
    "#     i = 0\n",
    "#     for idy, (images, cv2_img, targets, pre_target) in enumerate(tqdm(test_loader)):\n",
    "        \n",
    "#         iou_scores = []\n",
    "\n",
    "#         ############### This code below will give the GT classes/labels present in the image #############\n",
    "#         sa = pre_target[0].permute(1,2,0).cpu().detach().numpy()\n",
    "#         unique_pixels = np.unique(sa) *255\n",
    "#         filtered_numbers = [num for num in unique_pixels if num not in [0, 255]]\n",
    "\n",
    "#         true_label = filtered_numbers\n",
    "\n",
    "#         ############### get image and text features: and obtain similarity maps (same as Surgery) #############\n",
    "\n",
    "#         images = images.to(device)\n",
    "#         targets = targets.to(device)\n",
    "\n",
    "#         image_features = model.encode_image(images)\n",
    "\n",
    "#         features = image_features @ text_features.t()\n",
    "#         similarity = clip.clip_feature_surgery(image_features, text_features)\n",
    "#         similarity_map = clip.get_similarity_map(similarity[:, 1:, :], 224)\n",
    "\n",
    "#         #### Now for each class in each image we find the IoU ####\n",
    "#         mask_idx = []\n",
    "#         mask_target = []\n",
    "#         for idx in true_label:\n",
    "\n",
    "#             ## This gives the simialrity between prompts of class idx and image\n",
    "#             preds = similarity_map[:,:,:,int(idx-1)].view_as(targets)\n",
    "\n",
    "#             ## Next is just making the pixel values as the class number or the idx. Commonly done in segmentation problems. The class number (0 or 1 - aeroplane will have pixel values as 0 or 1)\n",
    "#             preds = (preds> threshold).float()\n",
    "#             preds = preds * idx\n",
    "\n",
    "#             preds = preds.to(torch.uint8)\n",
    "#             preds_img =preds[0].permute(1,2,0).cpu().detach().numpy()\n",
    "            \n",
    "\n",
    "#             ## Here, in the target we seperate the index, that is segment only the class or idx. \n",
    "\n",
    "#             tar_img = targets[0].permute(1,2,0).cpu().detach().numpy() * 255\n",
    "#             tar_img_mask = (tar_img == idx).astype(int) * idx\n",
    "\n",
    "#             tar_img_mask = tar_img_mask.astype(np.uint8)\n",
    "#             # print('tar_img_mask', np.unique(tar_img_mask))\n",
    "\n",
    "#             mask1 = torch.tensor(preds_img, dtype=torch.uint8)\n",
    "#             mask2 = torch.tensor(tar_img_mask, dtype=torch.uint8)\n",
    "            \n",
    "\n",
    "#             mask_idx.append(mask1)\n",
    "#             mask_target.append(mask2)\n",
    "        \n",
    "        \n",
    "#         summed_mask_idx = torch.sum(torch.stack(mask_idx), dim=0)\n",
    "#         summed_mask_target = torch.sum(torch.stack(mask_target), dim=0)\n",
    "# #         plt.imshow(summed_mask_idx)  ## 0 is the classname\n",
    "# #         plt.show()\n",
    "#         # print('summed_mask_idx', np.unique(summed_mask_idx.cpu().detach().numpy()))\n",
    "# #         plt.imshow(summed_mask_target)  ## 0 is the classname\n",
    "# #         plt.show()\n",
    "#         # print('summed_mask_target', np.unique(summed_mask_target.cpu().detach().numpy()))\n",
    "\n",
    "#         # ## iou is between prediction and target for one class or index. \n",
    "#         iou = calculate_iou(summed_mask_idx.cpu(), summed_mask_target.cpu(), num_classes=21)\n",
    "#         iou_scores.append(iou)\n",
    "\n",
    "#     mean_iou = np.nanmean(iou_scores)\n",
    "#     miou_list.append(mean_iou)\n",
    "#     # print(f'Mean IoU on PASCAL VOC 2012 test set: {mean_iou:.4f}')\n",
    "#     # print('-'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "645f2f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(miou_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "589a750c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1449/1449 [00:40<00:00, 35.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39863354497171954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_iou(pred, target, num_classes=20):\n",
    "    ious = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = torch.sum(pred_inds[target_inds])\n",
    "        union = torch.sum(pred_inds) + torch.sum(target_inds) - intersection\n",
    "        \n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            ious.append(float(intersection) / max(union, 1))\n",
    "    # print(ious)\n",
    "#     print(np.nanmean(ious))\n",
    "    return np.nanmean(ious)\n",
    "\n",
    "\n",
    "miou_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    iou_scores = []\n",
    "    for images, cv2_img, targets, pre_target in tqdm(test_loader):\n",
    "        \n",
    "        ############### This code below will give the GT classes/labels present in the image #############\n",
    "        sa = pre_target[0].permute(1,2,0).cpu().detach().numpy()\n",
    "        unique_pixels = np.unique(sa) *255\n",
    "        filtered_numbers = [num for num in unique_pixels if num not in [0, 255]]\n",
    "\n",
    "        true_label = filtered_numbers\n",
    "        # print(true_label)\n",
    "        \n",
    "        ############### get image and text features: and obtain similarity maps (same as Surgery) #############\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        image_features = model.encode_image(images)\n",
    "\n",
    "        features = image_features @ text_features.t()\n",
    "        similarity = clip.clip_feature_surgery(image_features, text_features)\n",
    "        similarity_map = clip.get_similarity_map(similarity[:, 1:, :], 224)\n",
    "\n",
    "        pred = similarity_map.argmax(dim = -1)\n",
    "        targets = 255.*targets\n",
    "\n",
    "        # target_mask = torch.zeros_like(targets)\n",
    "        # pred_mask = torch.zeros_like(pred)\n",
    "        target_mask_list = []\n",
    "        pred_mask_list = []\n",
    "        for val in true_label:\n",
    "            target_mask_list.append((targets.squeeze(0) == val).to(int)*val)\n",
    "            pred_mask_list.append((pred == val-1).to(int)*val)\n",
    "            \n",
    "        \n",
    "        target_mask = sum(target_mask_list)\n",
    "        pred_mask = sum(pred_mask_list)\n",
    "        \n",
    "        ans = calculate_iou(pred_mask.cpu(), target_mask.cpu())\n",
    "        \n",
    "        miou_list.append(ans)\n",
    "\n",
    "print(np.nanmean(miou_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeecaf5",
   "metadata": {},
   "source": [
    "## Loop Over only the argmax classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e677488",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1449/1449 [00:23<00:00, 60.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012222481767219597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_iou(pred, target, num_classes=21):\n",
    "    ious = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    for cls in range(1, num_classes): ## start from 1 to ignore background\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = torch.sum(pred_inds[target_inds])\n",
    "        union = torch.sum(pred_inds) + torch.sum(target_inds) - intersection\n",
    "        \n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            ious.append(float(intersection) / max(union, 1))\n",
    "#     print(ious)\n",
    "#     print(np.nanmean(ious))\n",
    "    return np.nanmean(ious)\n",
    "\n",
    "\n",
    "miou_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    iou_scores = []\n",
    "    for images, cv2_img, targets, pre_target in tqdm(test_loader):\n",
    "        \n",
    "        ############### This code below will give the GT classes/labels present in the image #############\n",
    "        sa = pre_target[0].permute(1,2,0).cpu().detach().numpy()\n",
    "        unique_pixels = np.unique(sa) *255\n",
    "        filtered_numbers = [num for num in unique_pixels if num not in [0, 255]]\n",
    "\n",
    "        true_label = filtered_numbers\n",
    "        # print(true_label)\n",
    "        \n",
    "        ############### get image and text features: and obtain similarity maps (same as Surgery) #############\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        image_features = model.encode_image(images)\n",
    "\n",
    "        features = image_features @ text_features.t()\n",
    "        similarity = clip.clip_feature_surgery(image_features, text_features)\n",
    "        similarity_map = clip.get_similarity_map(similarity[:, 1:, :], 224)\n",
    "\n",
    "        pred = similarity_map.argmax(dim = -1)\n",
    "        pred = pred + 1 ## making sure that 0 is background\n",
    "        \n",
    "        targets = 255.*targets\n",
    "        targets[targets == 255] = 0 ## only keep 0 for background, 255 was border, made it background\n",
    "\n",
    "        \n",
    "#         print(torch.unique(pred))\n",
    "#         plt.imshow((pred == 1).permute(1,2,0).cpu())\n",
    "#         plt.show()\n",
    "        \n",
    "#         print(torch.unique(targets))\n",
    "#         plt.imshow((targets.squeeze(0) == 1).permute(1,2,0).cpu())\n",
    "#         plt.show()\n",
    "#         break\n",
    "        \n",
    "        target_mask_list = []\n",
    "        pred_mask_list = []\n",
    "            \n",
    "        for val in torch.unique(pred):\n",
    "            target_mask_list.append((targets.squeeze(0) == val).to(int)*(val))\n",
    "            pred_mask_list.append((pred == val).to(int)*val)\n",
    "            \n",
    "        \n",
    "        target_mask = sum(target_mask_list)\n",
    "        pred_mask = sum(pred_mask_list)\n",
    "        \n",
    "#         print(torch.unique(target_mask))\n",
    "        \n",
    "#         plt.imshow(target_mask.permute(1,2,0).cpu())\n",
    "#         plt.show()\n",
    "        \n",
    "#         plt.imshow(pred_mask.permute(1,2,0).cpu())\n",
    "#         plt.show()\n",
    "        \n",
    "        \n",
    "        ans = calculate_iou(pred_mask.cpu(), target_mask.cpu())\n",
    "#         print(ans)\n",
    "        \n",
    "        miou_list.append(ans)\n",
    "\n",
    "print(np.nanmean(miou_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb52c41",
   "metadata": {},
   "source": [
    "## Loop over all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a723d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    text_features = clip.encode_text_with_prompt_ensemble(model, all_texts, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81888df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1449/1449 [00:35<00:00, 40.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009240414121136459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_iou(pred, target, num_classes=21):\n",
    "    ious = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    for cls in range(1, num_classes): ## start from 1 to ignore background\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = torch.sum(pred_inds[target_inds])\n",
    "        union = torch.sum(pred_inds) + torch.sum(target_inds) - intersection\n",
    "        \n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            ious.append(float(intersection) / max(union, 1))\n",
    "#     print(ious)\n",
    "#     print(np.nanmean(ious))\n",
    "    return np.nanmean(ious)\n",
    "\n",
    "miou_list = []\n",
    "        \n",
    "\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    iou_scores = []\n",
    "    for images, cv2_img, targets, pre_target in tqdm(test_loader):\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        image_features = model.encode_image(images)\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        \n",
    "        features = image_features @ text_features.t()\n",
    "        similarity = clip.clip_feature_surgery(image_features, text_features)\n",
    "        similarity_map = clip.get_similarity_map(similarity[:, 1:, :], 224)\n",
    "\n",
    "        pred = similarity_map.argmax(dim = -1)\n",
    "        pred = pred + 1 ## making sure that 0 is background\n",
    "        \n",
    "        targets = 255.*targets\n",
    "        targets[targets == 255] = 0 ## only keep 0 for background, 255 was border, made it background\n",
    "        \n",
    "        target_mask_list = []\n",
    "        pred_mask_list = []\n",
    "            \n",
    "        for val in range(1,21):\n",
    "            target_mask_list.append((targets.squeeze(0) == val).to(int)*(val))\n",
    "            pred_mask_list.append((pred == val).to(int)*val)\n",
    "            \n",
    "        target_mask = sum(target_mask_list)\n",
    "        pred_mask = sum(pred_mask_list)\n",
    "        \n",
    "        ans = calculate_iou(pred_mask.cpu(), target_mask.cpu())\n",
    "        # print(ans)\n",
    "        miou_list.append(ans)\n",
    "\n",
    "print(np.nanmean(miou_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f5e02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1449/1449 [00:30<00:00, 48.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008751337843497195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_iou(pred, target, num_classes=21):\n",
    "    ious = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    for cls in range( num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = torch.sum(pred_inds[target_inds])\n",
    "        union = torch.sum(pred_inds) + torch.sum(target_inds) - intersection\n",
    "        \n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            ious.append(float(intersection) / max(union, 1))\n",
    "#     print(ious)\n",
    "#     print(np.nanmean(ious))\n",
    "    return np.nanmean(ious)\n",
    "\n",
    "\n",
    "miou_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    iou_scores = []\n",
    "    for images, cv2_img, targets, pre_target in tqdm(test_loader):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        image_features = model.encode_image(images)\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "    \n",
    "\n",
    "        features = image_features @ text_features.t()\n",
    "        similarity = clip.clip_feature_surgery(image_features, text_features)\n",
    "        similarity_map = clip.get_similarity_map(similarity[:, 1:, :], 224)\n",
    "\n",
    "        pred = similarity_map.argmax(dim = -1)\n",
    "        pred = pred + 1 ## making sure that 0 is background\n",
    "        \n",
    "        targets = 255.*targets\n",
    "        targets[targets == 255] = 0 ## only keep 0 for background, 255 was border, made it background\n",
    "\n",
    "        \n",
    "        target_mask_list = []\n",
    "        pred_mask_list = []\n",
    "            \n",
    "        for val in torch.unique(pred):\n",
    "            target_mask_list.append((targets.squeeze(0) == val).to(int)*(val))\n",
    "            pred_mask_list.append((pred == val).to(int)*val)\n",
    "            \n",
    "        \n",
    "        target_mask = sum(target_mask_list)\n",
    "        pred_mask = sum(pred_mask_list)\n",
    "        \n",
    "        ans = calculate_iou(pred_mask.cpu(), target_mask.cpu())\n",
    "        \n",
    "        miou_list.append(ans)\n",
    "\n",
    "print(np.nanmean(miou_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c63083",
   "metadata": {},
   "source": [
    "## Ours with segemetnation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c58e6a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "sizes = [512, 384, 256]\n",
    "\n",
    "layers_text = []\n",
    "for i in range(len(sizes) - 2):\n",
    "    layers_text.append(nn.Linear(sizes[i], sizes[i + 1], bias=False))\n",
    "    layers_text.append(nn.BatchNorm1d(sizes[i + 1]))\n",
    "    layers_text.append(nn.ReLU(inplace=True))\n",
    "layers_text.append(nn.Linear(sizes[-2], sizes[-1], bias=False))\n",
    "text_projector = nn.Sequential(*layers_text)\n",
    "\n",
    "size_img = [512, 256]\n",
    "layers_img = []\n",
    "# for i in range(len(sizes) - 2):\n",
    "#     layers_img.append(nn.Linear(size_img[i], size_img[i + 1], bias=False))\n",
    "#     layers_img.append(nn.BatchNorm1d(197))\n",
    "#     layers_img.append(nn.ReLU(inplace=True))\n",
    "layers_img.append(nn.Linear(size_img[-2], size_img[-1], bias=False))\n",
    "text_projector = nn.Sequential(*layers_text).to(device)\n",
    "\n",
    "image_projector = nn.Sequential(*layers_img).to(device)\n",
    "\n",
    "\n",
    "# model_path = '/home/samyakr2/Redundancy/DualCoOp/output/coco_with_SSL_90_0.003R/model_best.pth.tar'\n",
    "model_path = '/home/samyakr2/Redundancy/DualCoOp/output/coco_with_SSL_90_0.002R/model_best.pth.tar'\n",
    "state_dict = torch.load(model_path)\n",
    "\n",
    "projector_weights_text = {}\n",
    "projector_weights_img = {}\n",
    "\n",
    "for kays in state_dict['state_dict'].keys():\n",
    "    if 'text_projector' in kays:\n",
    "        projector_weights_text[kays[15:]] = state_dict['state_dict'][kays]\n",
    "    if 'image_projector' in kays:\n",
    "        projector_weights_img[kays[16:]] = state_dict['state_dict'][kays]\n",
    "\n",
    "\n",
    "text_projector.load_state_dict(projector_weights_text)\n",
    "image_projector.load_state_dict(projector_weights_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da0d65",
   "metadata": {},
   "source": [
    "### CLIP Surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf7cbc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1449/1449 [00:25<00:00, 56.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008664675723514367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_iou(pred, target, num_classes=21):\n",
    "    ious = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    for cls in range( num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = torch.sum(pred_inds[target_inds])\n",
    "        union = torch.sum(pred_inds) + torch.sum(target_inds) - intersection\n",
    "        \n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            ious.append(float(intersection) / max(union, 1))\n",
    "#     print(ious)\n",
    "#     print(np.nanmean(ious))\n",
    "    return np.nanmean(ious)\n",
    "\n",
    "\n",
    "miou_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    iou_scores = []\n",
    "    text_feats = clip.encode_text_with_prompt_ensemble(model, all_texts, device)\n",
    "        \n",
    "    for images, cv2_img, targets, pre_target in tqdm(test_loader):\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        image_features = model.encode_image(images)\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        \n",
    "\n",
    "        # text_features = text_projector(text_feats)\n",
    "        text_features = text_feats\n",
    "    \n",
    "\n",
    "        features = image_features @ text_features.t()\n",
    "        similarity = clip.clip_feature_surgery(image_features, text_features)\n",
    "        similarity_map = clip.get_similarity_map(similarity[:, 1:, :], 224)\n",
    "\n",
    "\n",
    "        pred = similarity_map.argmax(dim = -1)\n",
    "        pred = pred + 1 ## making sure that 0 is background\n",
    "        \n",
    "        targets = 255.*targets\n",
    "        targets[targets == 255] = 0 ## only keep 0 for background, 255 was border, made it background\n",
    "        \n",
    "        target_mask_list = []\n",
    "        pred_mask_list = []\n",
    "            \n",
    "        for val in range(1,21):\n",
    "            target_mask_list.append((targets.squeeze(0) == val).to(int)*(val))\n",
    "            pred_mask_list.append((pred == val).to(int)*val)\n",
    "            \n",
    "        target_mask = sum(target_mask_list)\n",
    "        pred_mask = sum(pred_mask_list)\n",
    "        \n",
    "        ans = calculate_iou(pred_mask.cpu(), target_mask.cpu())\n",
    "        miou_list.append(ans)\n",
    "\n",
    "print(np.nanmean(miou_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a8f48",
   "metadata": {},
   "source": [
    "### Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d49c763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1449/1449 [00:24<00:00, 58.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005253465879289109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_iou(pred, target, num_classes=21):\n",
    "    ious = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    for cls in range( num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = torch.sum(pred_inds[target_inds])\n",
    "        union = torch.sum(pred_inds) + torch.sum(target_inds) - intersection\n",
    "        \n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            ious.append(float(intersection) / max(union, 1))\n",
    "#     print(ious)\n",
    "#     print(np.nanmean(ious))\n",
    "    return np.nanmean(ious)\n",
    "\n",
    "\n",
    "miou_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    iou_scores = []\n",
    "    text_feats = clip.encode_text_with_prompt_ensemble(model, all_texts, device)\n",
    "        \n",
    "    for images, cv2_img, targets, pre_target in tqdm(test_loader):\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        image_features = model.encode_image(images)\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        img_feat = image_projector(image_features)\n",
    "        image_features = img_feat\n",
    "\n",
    "        text_features = text_projector(text_feats)\n",
    "    \n",
    "\n",
    "        features = image_features @ text_features.t()\n",
    "        # similarity = clip.clip_feature_surgery(image_features, text_features)\n",
    "        # similarity_map = clip.get_similarity_map(similarity[:, 1:, :], 224)\n",
    "\n",
    "        similarity_map = clip.get_similarity_map(features[:, 1:, :], 224)\n",
    "\n",
    "        pred = similarity_map.argmax(dim = -1)\n",
    "        pred = pred + 1 ## making sure that 0 is background\n",
    "        \n",
    "        targets = 255.*targets\n",
    "        targets[targets == 255] = 0 ## only keep 0 for background, 255 was border, made it background\n",
    "        \n",
    "        target_mask_list = []\n",
    "        pred_mask_list = []\n",
    "            \n",
    "        for val in range(1,21):\n",
    "            target_mask_list.append((targets.squeeze(0) == val).to(int)*(val))\n",
    "            pred_mask_list.append((pred == val).to(int)*val)\n",
    "            \n",
    "        target_mask = sum(target_mask_list)\n",
    "        pred_mask = sum(pred_mask_list)\n",
    "        \n",
    "        ans = calculate_iou(pred_mask.cpu(), target_mask.cpu())\n",
    "        miou_list.append(ans)\n",
    "\n",
    "print(np.nanmean(miou_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbdbdbc",
   "metadata": {},
   "source": [
    "#### Ours + CLIP surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04c3502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1449/1449 [00:24<00:00, 58.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019492460235222737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_iou(pred, target, num_classes=21):\n",
    "    ious = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    for cls in range( num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = torch.sum(pred_inds[target_inds])\n",
    "        union = torch.sum(pred_inds) + torch.sum(target_inds) - intersection\n",
    "        \n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            ious.append(float(intersection) / max(union, 1))\n",
    "#     print(ious)\n",
    "#     print(np.nanmean(ious))\n",
    "    return np.nanmean(ious)\n",
    "\n",
    "\n",
    "miou_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    iou_scores = []\n",
    "    text_feats = clip.encode_text_with_prompt_ensemble(model, all_texts, device)\n",
    "        \n",
    "    for images, cv2_img, targets, pre_target in tqdm(test_loader):\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        image_features = model.encode_image(images)\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        img_feat = image_projector(image_features)\n",
    "        image_features = img_feat\n",
    "\n",
    "        text_features = text_projector(text_feats)\n",
    "    \n",
    "\n",
    "        features = image_features @ text_features.t()\n",
    "        similarity = clip.clip_feature_surgery(image_features, text_features)\n",
    "        similarity_map = clip.get_similarity_map(similarity[:, 1:, :], 224)\n",
    "\n",
    "        pred = similarity_map.argmax(dim = -1)\n",
    "        pred = pred + 1 ## making sure that 0 is background\n",
    "        \n",
    "        targets = 255.*targets\n",
    "        targets[targets == 255] = 0 ## only keep 0 for background, 255 was border, made it background\n",
    "        \n",
    "        target_mask_list = []\n",
    "        pred_mask_list = []\n",
    "            \n",
    "        for val in range(1,21):\n",
    "            target_mask_list.append((targets.squeeze(0) == val).to(int)*(val))\n",
    "            pred_mask_list.append((pred == val).to(int)*val)\n",
    "            \n",
    "        target_mask = sum(target_mask_list)\n",
    "        pred_mask = sum(pred_mask_list)\n",
    "        \n",
    "        ans = calculate_iou(pred_mask.cpu(), target_mask.cpu())\n",
    "        miou_list.append(ans)\n",
    "\n",
    "print(np.nanmean(miou_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea2aa3f",
   "metadata": {},
   "source": [
    "## Explanablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21f5d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "def min_max_normalize(segmentation_map):\n",
    "    \"\"\"\n",
    "    Normalize the segmentation map to the range [0, 1] using Min-Max normalization.\n",
    "    \"\"\"\n",
    "    min_val = np.min(segmentation_map)\n",
    "    max_val = np.max(segmentation_map)\n",
    "    return (segmentation_map - min_val) / (max_val - min_val)\n",
    "\n",
    "def compute_iou(ground_truth, binarized_map):\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IoU) between the ground truth and the binarized segmentation map.\n",
    "    \"\"\"\n",
    "    # Flatten the arrays to compute IoU\n",
    "    return jaccard_score(ground_truth.flatten(), binarized_map.flatten(), average='macro')\n",
    "\n",
    "def find_best_threshold(normalized_map, ground_truth, step_size=0.01):\n",
    "    \"\"\"\n",
    "    Perform a grid search to find the optimal threshold for a single class segmentation map.\n",
    "    \"\"\"\n",
    "    best_threshold = 0\n",
    "    best_iou = 0\n",
    "\n",
    "    thresholds = np.arange(0, 1 + step_size, step_size)\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        binarized_map = (normalized_map >= threshold).astype(np.uint8)\n",
    "        iou = compute_iou(ground_truth, binarized_map)\n",
    "\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold, best_iou\n",
    "\n",
    "def process_segmentation_map(segmentation_map, ground_truth, num_classes, step_size=0.01):\n",
    "    \"\"\"\n",
    "    Apply the grid search strategy to find the best threshold for each class in the segmentation map.\n",
    "    \"\"\"\n",
    "    best_thresholds = []\n",
    "    best_ious = []\n",
    "\n",
    "    for class_idx in num_classes:\n",
    "        # Extract the segmentation map and ground truth for the current class\n",
    "        print()\n",
    "        class_segmentation_map = segmentation_map[class_idx]\n",
    "        class_ground_truth = ground_truth[class_idx]\n",
    "\n",
    "        # Normalize the segmentation map\n",
    "        normalized_map = min_max_normalize(class_segmentation_map)\n",
    "\n",
    "        # Find the best threshold using grid search\n",
    "        best_threshold, best_iou = find_best_threshold(normalized_map, class_ground_truth, step_size)\n",
    "\n",
    "        best_thresholds.append(best_threshold)\n",
    "        best_ious.append(best_iou)\n",
    "\n",
    "        print(f\"Class {class_idx + 1}: Best Threshold = {best_threshold:.2f}, Best IoU = {best_iou:.4f}\")\n",
    "\n",
    "    return best_thresholds, best_ious\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a3ae7e",
   "metadata": {},
   "source": [
    "### CLIP Surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85e15ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1449 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'image_projector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m image_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_image(images)\n\u001b[1;32m     16\u001b[0m image_features \u001b[38;5;241m=\u001b[39m image_features \u001b[38;5;241m/\u001b[39m image_features\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 17\u001b[0m img_feat \u001b[38;5;241m=\u001b[39m image_projector(image_features)\n\u001b[1;32m     18\u001b[0m image_features \u001b[38;5;241m=\u001b[39m img_feat\n\u001b[1;32m     20\u001b[0m text_features \u001b[38;5;241m=\u001b[39m text_projector(text_feats)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_projector' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    iou_scores = []\n",
    "    text_feats = clip.encode_text_with_prompt_ensemble(model, all_texts, device)\n",
    "        \n",
    "    for images, cv2_img, targets, pre_target in tqdm(test_loader):\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        image_features = model.encode_image(images)\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        img_feat = image_projector(image_features)\n",
    "        image_features = img_feat\n",
    "\n",
    "        text_features = text_projector(text_feats)\n",
    "    \n",
    "\n",
    "        features = image_features @ text_features.t()\n",
    "        similarity = clip.clip_feature_surgery(image_features, text_features)\n",
    "        similarity_map = clip.get_similarity_map(similarity[:, 1:, :], 224)\n",
    "\n",
    "        targets = 255.*targets\n",
    "        targets[targets == 255] = 0 ## only keep 0 for background, 255 was border, made it background        \n",
    "        num_classes = torch.unique(target_mask)[1:].tolist()\n",
    "\n",
    "        \n",
    "        best_thresholds, best_ious = process_segmentation_map(pred_mask.cpu(), target_mask.cpu(), num_classes)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "483f3d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae9eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
