nohup: ignoring input
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/multilabel/data/coco14
data_split = train2014
PARTIAL_PORTION= 0.900000
INPUT.SIZE = 448
loading annotations into memory...
Done (t=6.91s)
creating index...
index created!
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/multilabel/data/coco14
data_split = val2014
PARTIAL_PORTION= 0.900000
INPUT.SIZE = 448
loading annotations into memory...
Done (t=3.01s)
creating index...
index created!
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/multilabel/data/coco14
data_split = val2014
PARTIAL_PORTION= 0.900000
INPUT.SIZE = 448
loading annotations into memory...
Done (t=3.22s)
creating index...
index created!
Loading CLIP (backbone: RN101)
Building dualcoop
Freeze the backbone weights
Freeze the attn weights
image_encoder.attnpool.positional_embedding
image_encoder.attnpool.k_proj.weight
image_encoder.attnpool.k_proj.bias
image_encoder.attnpool.q_proj.weight
image_encoder.attnpool.q_proj.bias
image_encoder.attnpool.v_proj.weight
image_encoder.attnpool.v_proj.bias
image_encoder.attnpool.c_proj.weight
image_encoder.attnpool.c_proj.bias
train.py --config_file configs/models/rn101_ep50.yaml --datadir /home/samyakr2/multilabel/data/coco14 --dataset_config_file /home/samyakr2/Redundancy/DualCoOp/configs/datasets/coco.yaml --input_size 448 --lr 0.002 --loss_w 0.05 -pp 0.9 --csc --max_epochs 51
Namespace(prefix='', resume=None, pretrained=None, auto_resume=False, datadir='/home/samyakr2/multilabel/data/coco14', input_size=448, train_input_size=None, num_train_cls=100, test_input_size=None, thre=0.5, single_prompt='pos', output_dir='', print_freq=100, val_freq_in_epoch=-1, evaluate=False, config_file='configs/models/rn101_ep50.yaml', dataset_config_file='/home/samyakr2/Redundancy/DualCoOp/configs/datasets/coco.yaml', positive_prompt=None, negative_prompt=None, n_ctx_pos=None, n_ctx_neg=None, lr=0.002, loss_w=0.05, csc=True, logit_scale=100.0, gamma_neg=2.0, gamma_pos=1.0, portion=1.0, partial_portion=0.9, mask_file=None, train_batch_size=None, stop_epochs=None, max_epochs=51, finetune=False, finetune_backbone=False, finetune_attn=False, finetune_text=False, base_lr_mult=None, backbone_lr_mult=None, text_lr_mult=None, attn_lr_mult=None, val_every_n_epochs=1, warmup_epochs=1, top_k=3)
DualCoop(
  (image_encoder): ModifiedResNet_conv_proj(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (attnpool): AttentionConv(
      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (c_proj): Linear(in_features=2048, out_features=512, bias=True)
    )
  )
  (text_encoder): TextEncoder(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (clip_model): CLIP_conv_proj(
    (visual): ModifiedResNet_conv_proj(
      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
      (relu): ReLU(inplace=True)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (attnpool): AttentionConv(
        (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (c_proj): Linear(in_features=2048, out_features=512, bias=True)
      )
    )
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (text_projector): Sequential(
    (0): Linear(in_features=512, out_features=384, bias=False)
    (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=384, out_features=256, bias=False)
  )
  (image_projector): Sequential(
    (0): Linear(in_features=512, out_features=256, bias=False)
  )
  (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 3
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
    SHUFFLE: False
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    PARTIAL_PORTION: 0.9
    PORTION: 1.0
    SAMPLER: RandomSampler
    SHUFFLE: True
  VAL:
    BATCH_SIZE: 100
    SHUFFLE: False
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  MASK_FILE: None
  NAME: coco
  NUM_LABELED: -1
  NUM_SHOTS: -1
  ROOT: /home/samyakr2/multilabel/data/coco14
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  TARGET_DOMAINS: ()
  TEST_GZSL_SPLIT: val2014
  TEST_SPLIT: val2014
  TRAIN_SPLIT: train2014
  VAL_GZSL_SPLIT: val2014
  VAL_PERCENT: 0.1
  VAL_SPLIT: val2014
  ZS_TEST: instances_val2014_gzsi_48_17.json
  ZS_TEST_UNSEEN: instances_val2014_unseen_48_17.json
  ZS_TRAIN: instances_train2014_seen_48_17.json
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (448, 448)
  TEST:
    SIZE: (448, 448)
  TRAIN:
    SIZE: (448, 448)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MLCCLIP:
  FLOAT: False
  NEGATIVE_PROMPT: 
  POSITIVE_PROMPT: 
MODEL:
  BACKBONE:
    NAME: RN101
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  ATTN_LR_MULT: 0.1
  BACKBONE_LR_MULT: 0.1
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 51
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: ./output
RESUME: 
SEED: -1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 100
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COOP_MLC:
    ASL_GAMMA_NEG: 2.0
    ASL_GAMMA_POS: 1.0
    CSC: True
    LS: 100.0
    NEGATIVE_PROMPT_INIT: 
    N_CTX_NEG: 16
    N_CTX_POS: 16
    POSITIVE_PROMPT_INIT: 
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FINETUNE: False
  FINETUNE_ATTN: False
  FINETUNE_BACKBONE: False
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: 
  RESNET_IMAGENET:
    DEPTH: 50
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
/home/samyakr2/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:836: UserWarning: non-inplace resize is deprecated
  warnings.warn("non-inplace resize is deprecated")
Train: [0/2566]	Time 1.930 (1.930)	Loss 13.92 (13.92)	mAP 8.70 (8.70)
Train: [100/2566]	Time 0.323 (0.338)	Loss 5.45 (6.98)	mAP 12.61 (9.68)
Train: [200/2566]	Time 0.328 (0.332)	Loss 5.86 (6.31)	mAP 7.96 (10.03)
Train: [300/2566]	Time 0.330 (0.331)	Loss 4.73 (5.97)	mAP 11.32 (10.33)
Train: [400/2566]	Time 0.328 (0.331)	Loss 5.94 (5.77)	mAP 17.12 (10.76)
Train: [500/2566]	Time 0.331 (0.330)	Loss 4.89 (5.63)	mAP 13.16 (11.11)
Train: [600/2566]	Time 0.332 (0.330)	Loss 5.05 (5.52)	mAP 16.20 (11.46)
Train: [700/2566]	Time 0.330 (0.330)	Loss 4.49 (5.43)	mAP 14.55 (11.91)
Train: [800/2566]	Time 0.331 (0.330)	Loss 4.50 (5.34)	mAP 21.09 (12.37)
Train: [900/2566]	Time 0.331 (0.330)	Loss 4.32 (5.27)	mAP 17.96 (12.88)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 4.49 (5.21)	mAP 15.91 (13.31)
Train: [1100/2566]	Time 0.332 (0.331)	Loss 4.87 (5.15)	mAP 21.94 (13.72)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 4.48 (5.10)	mAP 22.21 (14.20)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 4.81 (5.05)	mAP 18.41 (14.64)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 4.40 (5.00)	mAP 23.40 (15.08)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 3.68 (4.95)	mAP 24.90 (15.51)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 4.49 (4.91)	mAP 23.95 (15.96)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 5.10 (4.87)	mAP 23.17 (16.38)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 4.46 (4.83)	mAP 25.21 (16.79)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 4.00 (4.79)	mAP 27.35 (17.20)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 4.14 (4.76)	mAP 25.32 (17.57)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 3.95 (4.72)	mAP 27.00 (17.95)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 3.84 (4.69)	mAP 27.20 (18.30)
Train: [2300/2566]	Time 0.332 (0.331)	Loss 4.47 (4.65)	mAP 25.76 (18.63)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 3.46 (4.62)	mAP 27.79 (18.97)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 4.08 (4.59)	mAP 26.23 (19.29)
Train: [1/51]	Time 0.331	Loss 4.57 	mAP 19.50
Test: [0/402]	Time 1.900 (1.900)	Precision 41.61 (41.61)	Recall 38.07 (38.07) 	 P_C 11.27 	 R_C 10.31 	 F_C 8.66 	 P_O 41.61 	 R_O 38.07 	 F_O 39.76
Test: [100/402]	Time 0.500 (0.514)	Precision 42.99 (39.64)	Recall 30.07 (39.89) 	 P_C 24.77 	 R_C 27.09 	 F_C 22.31 	 P_O 40.89 	 R_O 39.27 	 F_O 40.07
Test: [200/402]	Time 0.500 (0.507)	Precision 43.40 (40.99)	Recall 49.78 (42.74) 	 P_C 25.87 	 R_C 27.60 	 F_C 23.87 	 P_O 41.54 	 R_O 42.03 	 F_O 41.78
Test: [300/402]	Time 0.499 (0.504)	Precision 68.17 (42.85)	Recall 52.74 (47.85) 	 P_C 29.07 	 R_C 32.89 	 F_C 29.00 	 P_O 41.69 	 R_O 47.41 	 F_O 44.37
Test: [400/402]	Time 0.500 (0.503)	Precision 18.44 (43.40)	Recall 34.30 (49.09) 	 P_C 30.18 	 R_C 34.06 	 F_C 30.71 	 P_O 42.22 	 R_O 49.13 	 F_O 45.41
Test: [1/51]	  P_C 30.17 	 R_C 34.06 	 F_C 30.71 	 P_O 42.20 	 R_O 49.12 	 F_O 45.40 	 mAP 29.43
Train: [0/2566]	Time 1.035 (1.035)	Loss 3.97 (3.97)	mAP 32.93 (32.93)
Train: [100/2566]	Time 0.331 (0.338)	Loss 5.67 (8.95)	mAP 10.62 (10.12)
Train: [200/2566]	Time 0.331 (0.334)	Loss 4.85 (7.07)	mAP 7.97 (10.29)
Train: [300/2566]	Time 0.331 (0.333)	Loss 5.54 (6.41)	mAP 13.89 (10.79)
Train: [400/2566]	Time 0.331 (0.333)	Loss 5.04 (6.07)	mAP 9.76 (11.14)
Train: [500/2566]	Time 0.332 (0.333)	Loss 4.39 (5.87)	mAP 13.00 (11.34)
Train: [600/2566]	Time 0.330 (0.332)	Loss 4.85 (5.72)	mAP 11.76 (11.55)
Train: [700/2566]	Time 0.331 (0.332)	Loss 5.77 (5.62)	mAP 14.94 (11.75)
Train: [800/2566]	Time 0.331 (0.332)	Loss 4.97 (5.53)	mAP 15.77 (11.88)
Train: [900/2566]	Time 0.331 (0.332)	Loss 5.71 (5.47)	mAP 15.15 (12.00)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 5.03 (5.41)	mAP 13.80 (12.13)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 4.79 (5.37)	mAP 18.09 (12.27)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 5.29 (5.32)	mAP 12.03 (12.39)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 5.05 (5.28)	mAP 15.11 (12.54)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 4.77 (5.25)	mAP 18.05 (12.71)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 5.12 (5.22)	mAP 17.04 (12.90)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 4.76 (5.18)	mAP 18.09 (13.05)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 5.03 (5.15)	mAP 15.64 (13.23)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 4.45 (5.12)	mAP 15.92 (13.40)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 4.93 (5.09)	mAP 17.77 (13.62)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 4.14 (5.06)	mAP 17.80 (13.84)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 4.46 (5.03)	mAP 23.36 (14.07)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 4.12 (4.99)	mAP 15.66 (14.35)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 4.51 (4.96)	mAP 20.72 (14.62)
Train: [2400/2566]	Time 0.332 (0.331)	Loss 3.88 (4.93)	mAP 22.26 (14.89)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 3.73 (4.89)	mAP 22.18 (15.19)
Train: [2/51]	Time 0.331	Loss 4.88 	mAP 15.39
Test: [0/402]	Time 1.735 (1.735)	Precision 73.12 (73.12)	Recall 19.32 (19.32) 	 P_C 6.89 	 R_C 2.64 	 F_C 3.28 	 P_O 73.12 	 R_O 19.32 	 F_O 30.56
Test: [100/402]	Time 0.499 (0.510)	Precision 31.03 (49.10)	Recall 5.88 (17.07) 	 P_C 12.30 	 R_C 4.29 	 F_C 4.31 	 P_O 54.72 	 R_O 19.02 	 F_O 28.23
Test: [200/402]	Time 0.499 (0.505)	Precision 95.59 (52.59)	Recall 28.14 (17.88) 	 P_C 12.79 	 R_C 3.77 	 F_C 4.00 	 P_O 57.42 	 R_O 19.41 	 F_O 29.01
Test: [300/402]	Time 0.499 (0.503)	Precision 82.84 (59.40)	Recall 27.61 (20.97) 	 P_C 12.75 	 R_C 3.95 	 F_C 4.43 	 P_O 63.54 	 R_O 22.01 	 F_O 32.69
Test: [400/402]	Time 0.499 (0.502)	Precision 32.93 (60.89)	Recall 15.70 (22.34) 	 P_C 12.93 	 R_C 4.22 	 F_C 4.72 	 P_O 64.60 	 R_O 23.49 	 F_O 34.45
Test: [2/51]	  P_C 12.93 	 R_C 4.22 	 F_C 4.72 	 P_O 64.57 	 R_O 23.48 	 F_O 34.44 	 mAP 17.32
Train: [0/2566]	Time 1.010 (1.010)	Loss 4.22 (4.22)	mAP 17.66 (17.66)
Train: [100/2566]	Time 0.330 (0.337)	Loss 4.05 (4.02)	mAP 21.64 (23.67)
Train: [200/2566]	Time 0.330 (0.334)	Loss 4.21 (4.01)	mAP 27.02 (23.81)
Train: [300/2566]	Time 0.330 (0.333)	Loss 4.24 (4.00)	mAP 21.97 (24.11)
Train: [400/2566]	Time 0.330 (0.332)	Loss 3.74 (3.97)	mAP 25.77 (24.39)
Train: [500/2566]	Time 0.331 (0.332)	Loss 3.51 (3.93)	mAP 27.18 (24.78)
Train: [600/2566]	Time 0.331 (0.332)	Loss 4.52 (3.91)	mAP 28.48 (25.16)
Train: [700/2566]	Time 0.331 (0.332)	Loss 3.30 (3.88)	mAP 23.07 (25.45)
Train: [800/2566]	Time 0.331 (0.332)	Loss 3.86 (3.86)	mAP 27.69 (25.78)
Train: [900/2566]	Time 0.332 (0.332)	Loss 3.31 (3.84)	mAP 30.72 (26.07)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 3.76 (3.82)	mAP 27.88 (26.32)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 3.88 (3.79)	mAP 22.61 (26.61)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 3.35 (3.78)	mAP 29.78 (26.82)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 3.13 (3.76)	mAP 30.57 (27.12)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 3.68 (3.74)	mAP 30.38 (27.33)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 3.18 (3.71)	mAP 28.03 (27.61)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 3.41 (3.69)	mAP 28.54 (27.83)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 3.63 (3.67)	mAP 30.82 (28.06)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 3.43 (3.65)	mAP 28.52 (28.25)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.80 (3.63)	mAP 32.89 (28.52)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 3.40 (3.61)	mAP 29.63 (28.72)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 3.03 (3.59)	mAP 30.27 (28.95)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 3.39 (3.58)	mAP 37.49 (29.16)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 3.29 (3.56)	mAP 38.11 (29.39)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 3.18 (3.55)	mAP 32.95 (29.61)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 3.08 (3.53)	mAP 29.30 (29.83)
Train: [3/51]	Time 0.331	Loss 3.52 	mAP 29.97
Test: [0/402]	Time 1.795 (1.795)	Precision 62.90 (62.90)	Recall 33.24 (33.24) 	 P_C 14.56 	 R_C 9.41 	 F_C 9.79 	 P_O 62.90 	 R_O 33.24 	 F_O 43.49
Test: [100/402]	Time 0.500 (0.512)	Precision 75.56 (65.18)	Recall 22.22 (48.71) 	 P_C 45.90 	 R_C 24.06 	 F_C 26.39 	 P_O 64.29 	 R_O 47.67 	 F_O 54.75
Test: [200/402]	Time 0.500 (0.506)	Precision 84.40 (66.10)	Recall 51.52 (43.90) 	 P_C 52.81 	 R_C 24.93 	 F_C 28.31 	 P_O 65.49 	 R_O 43.97 	 F_O 52.61
Test: [300/402]	Time 0.499 (0.504)	Precision 82.68 (67.54)	Recall 47.51 (45.28) 	 P_C 56.93 	 R_C 28.63 	 F_C 32.47 	 P_O 66.90 	 R_O 44.57 	 F_O 53.50
Test: [400/402]	Time 0.500 (0.503)	Precision 45.45 (67.24)	Recall 23.26 (45.54) 	 P_C 56.90 	 R_C 30.06 	 F_C 33.86 	 P_O 67.02 	 R_O 45.60 	 F_O 54.27
Test: [3/51]	  P_C 56.90 	 R_C 30.05 	 F_C 33.85 	 P_O 67.00 	 R_O 45.58 	 F_O 54.26 	 mAP 45.11
Train: [0/2566]	Time 1.009 (1.009)	Loss 2.90 (2.90)	mAP 33.49 (33.49)
Train: [100/2566]	Time 0.331 (0.338)	Loss 2.98 (3.14)	mAP 38.09 (36.39)
Train: [200/2566]	Time 0.331 (0.334)	Loss 3.56 (3.09)	mAP 31.31 (36.43)
Train: [300/2566]	Time 0.332 (0.333)	Loss 3.06 (3.07)	mAP 41.53 (36.59)
Train: [400/2566]	Time 0.332 (0.333)	Loss 3.03 (3.05)	mAP 35.39 (36.64)
Train: [500/2566]	Time 0.332 (0.333)	Loss 2.68 (3.02)	mAP 30.29 (36.72)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.35 (3.00)	mAP 36.37 (36.82)
Train: [700/2566]	Time 0.333 (0.332)	Loss 3.10 (2.98)	mAP 47.69 (37.04)
Train: [800/2566]	Time 0.332 (0.332)	Loss 3.16 (2.96)	mAP 36.15 (37.15)
Train: [900/2566]	Time 0.332 (0.332)	Loss 2.70 (2.95)	mAP 36.14 (37.24)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 3.03 (2.93)	mAP 37.09 (37.50)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 2.36 (2.91)	mAP 37.86 (37.58)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 3.25 (2.90)	mAP 39.98 (37.71)
Train: [1300/2566]	Time 0.332 (0.332)	Loss 3.35 (2.88)	mAP 36.36 (37.89)
Train: [1400/2566]	Time 0.330 (0.332)	Loss 2.42 (2.87)	mAP 33.56 (38.04)
Train: [1500/2566]	Time 0.332 (0.332)	Loss 2.77 (2.85)	mAP 35.07 (38.13)
Train: [1600/2566]	Time 0.332 (0.332)	Loss 2.57 (2.83)	mAP 43.61 (38.28)
Train: [1700/2566]	Time 0.331 (0.332)	Loss 2.79 (2.82)	mAP 47.32 (38.40)
Train: [1800/2566]	Time 0.333 (0.332)	Loss 2.33 (2.80)	mAP 45.15 (38.52)
Train: [1900/2566]	Time 0.331 (0.332)	Loss 2.18 (2.79)	mAP 41.31 (38.66)
Train: [2000/2566]	Time 0.330 (0.332)	Loss 2.87 (2.77)	mAP 45.07 (38.75)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 2.13 (2.76)	mAP 42.51 (38.85)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.30 (2.75)	mAP 37.71 (38.95)
Train: [2300/2566]	Time 0.332 (0.331)	Loss 2.48 (2.74)	mAP 34.49 (39.08)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.87 (2.73)	mAP 42.68 (39.17)
Train: [2500/2566]	Time 0.333 (0.331)	Loss 2.34 (2.72)	mAP 39.52 (39.25)
Train: [4/51]	Time 0.331	Loss 2.71 	mAP 39.31
Test: [0/402]	Time 1.740 (1.740)	Precision 74.85 (74.85)	Recall 69.32 (69.32) 	 P_C 21.93 	 R_C 20.37 	 F_C 20.09 	 P_O 74.85 	 R_O 69.32 	 F_O 71.98
Test: [100/402]	Time 0.500 (0.512)	Precision 81.99 (71.92)	Recall 86.27 (75.33) 	 P_C 60.28 	 R_C 55.35 	 F_C 54.55 	 P_O 70.25 	 R_O 73.35 	 F_O 71.77
Test: [200/402]	Time 0.500 (0.506)	Precision 72.87 (72.07)	Recall 81.39 (75.56) 	 P_C 61.66 	 R_C 57.68 	 F_C 57.23 	 P_O 70.79 	 R_O 74.20 	 F_O 72.46
Test: [300/402]	Time 0.500 (0.504)	Precision 80.11 (71.96)	Recall 70.15 (75.16) 	 P_C 65.50 	 R_C 63.85 	 F_C 62.46 	 P_O 69.69 	 R_O 73.71 	 F_O 71.64
Test: [400/402]	Time 0.501 (0.503)	Precision 50.75 (71.09)	Recall 59.30 (75.21) 	 P_C 67.70 	 R_C 65.17 	 F_C 64.50 	 P_O 68.92 	 R_O 74.13 	 F_O 71.43
Test: [4/51]	  P_C 67.69 	 R_C 65.16 	 F_C 64.49 	 P_O 68.90 	 R_O 74.12 	 F_O 71.42 	 mAP 69.25
Train: [0/2566]	Time 0.972 (0.972)	Loss 1.87 (1.87)	mAP 45.62 (45.62)
Train: [100/2566]	Time 0.331 (0.337)	Loss 2.04 (2.43)	mAP 39.90 (42.30)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.90 (2.43)	mAP 39.95 (41.84)
Train: [300/2566]	Time 0.330 (0.333)	Loss 2.32 (2.41)	mAP 34.07 (41.99)
Train: [400/2566]	Time 0.332 (0.332)	Loss 2.46 (2.41)	mAP 47.62 (42.04)
Train: [500/2566]	Time 0.331 (0.332)	Loss 2.00 (2.39)	mAP 41.04 (42.00)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.20 (2.39)	mAP 46.06 (42.09)
Train: [700/2566]	Time 0.332 (0.332)	Loss 2.30 (2.39)	mAP 44.49 (42.07)
Train: [800/2566]	Time 0.331 (0.332)	Loss 2.24 (2.38)	mAP 44.29 (42.02)
Train: [900/2566]	Time 0.332 (0.332)	Loss 2.16 (2.37)	mAP 45.39 (42.11)
Train: [1000/2566]	Time 0.332 (0.332)	Loss 2.26 (2.37)	mAP 43.24 (42.18)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 2.05 (2.37)	mAP 33.47 (42.30)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 2.66 (2.36)	mAP 37.84 (42.33)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.99 (2.36)	mAP 42.63 (42.37)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.71 (2.35)	mAP 43.30 (42.34)
Train: [1500/2566]	Time 0.333 (0.331)	Loss 2.21 (2.35)	mAP 42.44 (42.39)
Train: [1600/2566]	Time 0.332 (0.331)	Loss 1.93 (2.34)	mAP 52.70 (42.36)
Train: [1700/2566]	Time 0.333 (0.331)	Loss 1.23 (2.33)	mAP 33.25 (42.36)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.90 (2.33)	mAP 41.52 (42.37)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.51 (2.32)	mAP 53.64 (42.40)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 2.69 (2.32)	mAP 40.19 (42.41)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.07 (2.32)	mAP 37.39 (42.46)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.33 (2.31)	mAP 37.61 (42.45)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.30 (2.31)	mAP 48.19 (42.48)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.93 (2.31)	mAP 49.06 (42.48)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 2.03 (2.30)	mAP 45.58 (42.52)
Train: [5/51]	Time 0.331	Loss 2.30 	mAP 42.55
Test: [0/402]	Time 1.812 (1.812)	Precision 75.74 (75.74)	Recall 72.73 (72.73) 	 P_C 23.32 	 R_C 24.98 	 F_C 23.21 	 P_O 75.74 	 R_O 72.73 	 F_O 74.20
Test: [100/402]	Time 0.501 (0.513)	Precision 86.75 (78.65)	Recall 85.62 (76.57) 	 P_C 66.78 	 R_C 58.82 	 F_C 59.37 	 P_O 76.80 	 R_O 74.98 	 F_O 75.88
Test: [200/402]	Time 0.499 (0.507)	Precision 84.38 (79.97)	Recall 81.82 (76.46) 	 P_C 67.71 	 R_C 61.11 	 F_C 62.20 	 P_O 78.48 	 R_O 75.37 	 F_O 76.90
Test: [300/402]	Time 0.499 (0.504)	Precision 82.67 (79.33)	Recall 72.39 (76.42) 	 P_C 72.59 	 R_C 67.36 	 F_C 68.12 	 P_O 77.28 	 R_O 75.27 	 F_O 76.26
Test: [400/402]	Time 0.500 (0.503)	Precision 60.12 (78.00)	Recall 60.47 (76.54) 	 P_C 74.43 	 R_C 68.64 	 F_C 70.01 	 P_O 75.97 	 R_O 75.71 	 F_O 75.84
Test: [5/51]	  P_C 74.43 	 R_C 68.63 	 F_C 70.00 	 P_O 75.96 	 R_O 75.70 	 F_O 75.83 	 mAP 75.37
Train: [0/2566]	Time 1.039 (1.039)	Loss 2.37 (2.37)	mAP 37.89 (37.89)
Train: [100/2566]	Time 0.330 (0.338)	Loss 1.92 (2.20)	mAP 40.32 (43.55)
Train: [200/2566]	Time 0.330 (0.334)	Loss 2.44 (2.20)	mAP 41.18 (43.34)
Train: [300/2566]	Time 0.330 (0.333)	Loss 2.33 (2.20)	mAP 53.38 (43.28)
Train: [400/2566]	Time 0.330 (0.333)	Loss 1.66 (2.22)	mAP 33.97 (43.28)
Train: [500/2566]	Time 0.332 (0.332)	Loss 1.92 (2.21)	mAP 38.97 (43.15)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.32 (2.20)	mAP 38.53 (43.23)
Train: [700/2566]	Time 0.330 (0.332)	Loss 3.05 (2.20)	mAP 51.67 (43.30)
Train: [800/2566]	Time 0.332 (0.332)	Loss 1.84 (2.20)	mAP 39.47 (43.29)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.33 (2.19)	mAP 45.28 (43.25)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 2.01 (2.18)	mAP 48.09 (43.29)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 1.89 (2.18)	mAP 39.70 (43.29)
Train: [1200/2566]	Time 0.332 (0.332)	Loss 1.91 (2.18)	mAP 45.10 (43.31)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 2.71 (2.18)	mAP 40.49 (43.29)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.03 (2.18)	mAP 44.11 (43.26)
Train: [1500/2566]	Time 0.332 (0.331)	Loss 2.38 (2.17)	mAP 37.49 (43.30)
Train: [1600/2566]	Time 0.332 (0.331)	Loss 2.06 (2.17)	mAP 45.69 (43.34)
Train: [1700/2566]	Time 0.332 (0.331)	Loss 2.07 (2.17)	mAP 49.32 (43.33)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.10 (2.17)	mAP 47.88 (43.33)
Train: [1900/2566]	Time 0.332 (0.331)	Loss 2.68 (2.16)	mAP 37.10 (43.31)
Train: [2000/2566]	Time 0.332 (0.331)	Loss 1.87 (2.16)	mAP 36.34 (43.31)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.72 (2.16)	mAP 40.97 (43.32)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 1.81 (2.16)	mAP 47.16 (43.35)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.88 (2.16)	mAP 45.85 (43.36)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.83 (2.15)	mAP 39.13 (43.36)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.88 (2.15)	mAP 42.73 (43.39)
Train: [6/51]	Time 0.331	Loss 2.15 	mAP 43.38
Test: [0/402]	Time 1.746 (1.746)	Precision 70.28 (70.28)	Recall 77.27 (77.27) 	 P_C 29.74 	 R_C 31.72 	 F_C 29.25 	 P_O 70.28 	 R_O 77.27 	 F_O 73.61
Test: [100/402]	Time 0.500 (0.512)	Precision 81.44 (76.25)	Recall 88.89 (79.35) 	 P_C 61.76 	 R_C 63.71 	 F_C 60.36 	 P_O 75.31 	 R_O 77.35 	 F_O 76.32
Test: [200/402]	Time 0.499 (0.506)	Precision 74.44 (76.19)	Recall 87.01 (80.56) 	 P_C 63.34 	 R_C 67.15 	 F_C 63.50 	 P_O 75.24 	 R_O 79.24 	 F_O 77.19
Test: [300/402]	Time 0.500 (0.504)	Precision 79.51 (76.20)	Recall 80.10 (80.36) 	 P_C 69.14 	 R_C 72.48 	 F_C 69.48 	 P_O 75.11 	 R_O 78.90 	 F_O 76.96
Test: [400/402]	Time 0.500 (0.503)	Precision 63.58 (75.37)	Recall 59.88 (80.22) 	 P_C 71.13 	 R_C 73.95 	 F_C 71.73 	 P_O 74.44 	 R_O 79.06 	 F_O 76.68
Test: [6/51]	  P_C 71.12 	 R_C 73.94 	 F_C 71.72 	 P_O 74.43 	 R_O 79.05 	 F_O 76.67 	 mAP 77.35
Train: [0/2566]	Time 0.981 (0.981)	Loss 2.28 (2.28)	mAP 40.17 (40.17)
Train: [100/2566]	Time 0.331 (0.337)	Loss 2.74 (2.13)	mAP 42.52 (43.97)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.61 (2.09)	mAP 34.20 (43.82)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.43 (2.10)	mAP 49.17 (43.75)
Train: [400/2566]	Time 0.331 (0.333)	Loss 1.94 (2.09)	mAP 46.76 (43.53)
Train: [500/2566]	Time 0.332 (0.333)	Loss 2.76 (2.10)	mAP 51.90 (43.65)
Train: [600/2566]	Time 0.333 (0.332)	Loss 1.98 (2.10)	mAP 50.86 (43.69)
Train: [700/2566]	Time 0.331 (0.332)	Loss 2.40 (2.10)	mAP 52.53 (43.82)
Train: [800/2566]	Time 0.330 (0.332)	Loss 2.19 (2.10)	mAP 45.99 (43.82)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.48 (2.10)	mAP 44.11 (43.86)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.65 (2.09)	mAP 46.48 (43.88)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 1.50 (2.09)	mAP 37.31 (43.86)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 2.30 (2.09)	mAP 41.67 (43.90)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 2.20 (2.09)	mAP 45.37 (43.90)
Train: [1400/2566]	Time 0.331 (0.332)	Loss 2.23 (2.09)	mAP 44.14 (43.88)
Train: [1500/2566]	Time 0.331 (0.332)	Loss 2.66 (2.09)	mAP 35.78 (43.90)
Train: [1600/2566]	Time 0.330 (0.332)	Loss 1.81 (2.09)	mAP 51.42 (43.90)
Train: [1700/2566]	Time 0.331 (0.332)	Loss 1.82 (2.09)	mAP 48.51 (43.90)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.81 (2.09)	mAP 50.56 (43.90)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.33 (2.09)	mAP 49.12 (43.92)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.85 (2.08)	mAP 44.68 (43.93)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 2.10 (2.08)	mAP 41.18 (43.92)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.14 (2.08)	mAP 46.26 (43.93)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.68 (2.08)	mAP 47.27 (43.97)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.07 (2.08)	mAP 47.51 (43.98)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 2.02 (2.08)	mAP 43.20 (43.97)
Train: [7/51]	Time 0.331	Loss 2.08 	mAP 43.97
Test: [0/402]	Time 1.814 (1.814)	Precision 73.60 (73.60)	Recall 78.41 (78.41) 	 P_C 29.45 	 R_C 31.73 	 F_C 28.99 	 P_O 73.60 	 R_O 78.41 	 F_O 75.93
Test: [100/402]	Time 0.500 (0.513)	Precision 81.82 (74.53)	Recall 88.24 (82.55) 	 P_C 63.70 	 R_C 67.49 	 F_C 63.30 	 P_O 72.37 	 R_O 81.45 	 F_O 76.64
Test: [200/402]	Time 0.500 (0.506)	Precision 76.60 (75.30)	Recall 87.88 (82.75) 	 P_C 65.79 	 R_C 70.04 	 F_C 66.41 	 P_O 73.70 	 R_O 81.95 	 F_O 77.60
Test: [300/402]	Time 0.500 (0.504)	Precision 81.22 (75.12)	Recall 79.60 (83.16) 	 P_C 70.32 	 R_C 75.55 	 F_C 71.66 	 P_O 73.30 	 R_O 82.28 	 F_O 77.53
Test: [400/402]	Time 0.501 (0.503)	Precision 55.45 (74.00)	Recall 68.02 (83.14) 	 P_C 71.62 	 R_C 77.08 	 F_C 73.39 	 P_O 72.28 	 R_O 82.49 	 F_O 77.05
Test: [7/51]	  P_C 71.62 	 R_C 77.08 	 F_C 73.39 	 P_O 72.28 	 R_O 82.49 	 F_O 77.04 	 mAP 79.29
Train: [0/2566]	Time 1.026 (1.026)	Loss 2.41 (2.41)	mAP 36.22 (36.22)
Train: [100/2566]	Time 0.330 (0.338)	Loss 2.11 (2.08)	mAP 44.46 (44.74)
Train: [200/2566]	Time 0.331 (0.335)	Loss 1.61 (2.06)	mAP 42.91 (44.36)
Train: [300/2566]	Time 0.331 (0.333)	Loss 2.38 (2.06)	mAP 36.02 (44.41)
Train: [400/2566]	Time 0.330 (0.333)	Loss 2.25 (2.06)	mAP 51.86 (44.41)
Train: [500/2566]	Time 0.332 (0.332)	Loss 2.64 (2.07)	mAP 47.20 (44.38)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.20 (2.06)	mAP 47.07 (44.46)
Train: [700/2566]	Time 0.332 (0.332)	Loss 2.33 (2.07)	mAP 47.84 (44.50)
Train: [800/2566]	Time 0.330 (0.332)	Loss 1.97 (2.07)	mAP 37.01 (44.44)
Train: [900/2566]	Time 0.334 (0.332)	Loss 1.78 (2.05)	mAP 39.90 (44.30)
Train: [1000/2566]	Time 0.332 (0.332)	Loss 2.54 (2.05)	mAP 42.09 (44.28)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 1.81 (2.05)	mAP 45.61 (44.24)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 2.01 (2.04)	mAP 46.12 (44.17)
Train: [1300/2566]	Time 0.332 (0.332)	Loss 1.99 (2.04)	mAP 51.46 (44.20)
Train: [1400/2566]	Time 0.331 (0.332)	Loss 2.42 (2.04)	mAP 44.47 (44.18)
Train: [1500/2566]	Time 0.332 (0.332)	Loss 1.93 (2.04)	mAP 42.65 (44.17)
Train: [1600/2566]	Time 0.331 (0.332)	Loss 2.48 (2.04)	mAP 50.34 (44.14)
Train: [1700/2566]	Time 0.332 (0.332)	Loss 1.56 (2.04)	mAP 39.27 (44.12)
Train: [1800/2566]	Time 0.331 (0.332)	Loss 1.87 (2.04)	mAP 44.31 (44.13)
Train: [1900/2566]	Time 0.332 (0.332)	Loss 2.10 (2.04)	mAP 45.30 (44.13)
Train: [2000/2566]	Time 0.332 (0.332)	Loss 1.81 (2.04)	mAP 44.57 (44.13)
Train: [2100/2566]	Time 0.333 (0.332)	Loss 2.14 (2.04)	mAP 41.59 (44.13)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 2.44 (2.04)	mAP 55.64 (44.16)
Train: [2300/2566]	Time 0.332 (0.331)	Loss 1.97 (2.04)	mAP 45.59 (44.14)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.77 (2.03)	mAP 44.27 (44.13)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.22 (2.04)	mAP 46.99 (44.19)
Train: [8/51]	Time 0.331	Loss 2.04 	mAP 44.18
Test: [0/402]	Time 1.750 (1.750)	Precision 71.81 (71.81)	Recall 76.70 (76.70) 	 P_C 28.49 	 R_C 29.88 	 F_C 28.13 	 P_O 71.81 	 R_O 76.70 	 F_O 74.18
Test: [100/402]	Time 0.499 (0.512)	Precision 84.28 (76.55)	Recall 87.58 (81.89) 	 P_C 62.82 	 R_C 68.31 	 F_C 63.68 	 P_O 74.70 	 R_O 80.63 	 F_O 77.55
Test: [200/402]	Time 0.500 (0.506)	Precision 81.40 (77.13)	Recall 85.28 (82.16) 	 P_C 64.82 	 R_C 70.91 	 F_C 66.20 	 P_O 75.50 	 R_O 81.32 	 F_O 78.30
Test: [300/402]	Time 0.500 (0.504)	Precision 80.72 (75.89)	Recall 83.33 (83.45) 	 P_C 68.91 	 R_C 77.23 	 F_C 71.55 	 P_O 73.94 	 R_O 82.65 	 F_O 78.05
Test: [400/402]	Time 0.500 (0.503)	Precision 55.61 (74.45)	Recall 69.19 (83.67) 	 P_C 70.52 	 R_C 78.85 	 F_C 73.53 	 P_O 72.69 	 R_O 83.11 	 F_O 77.55
Test: [8/51]	  P_C 70.52 	 R_C 78.85 	 F_C 73.53 	 P_O 72.68 	 R_O 83.10 	 F_O 77.54 	 mAP 80.10
Train: [0/2566]	Time 0.955 (0.955)	Loss 2.32 (2.32)	mAP 46.08 (46.08)
Train: [100/2566]	Time 0.330 (0.337)	Loss 2.11 (2.04)	mAP 39.75 (44.14)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.27 (2.02)	mAP 42.51 (44.39)
Train: [300/2566]	Time 0.330 (0.333)	Loss 2.00 (2.02)	mAP 45.59 (44.56)
Train: [400/2566]	Time 0.330 (0.332)	Loss 2.21 (2.02)	mAP 54.63 (44.40)
Train: [500/2566]	Time 0.333 (0.332)	Loss 2.21 (2.02)	mAP 48.03 (44.35)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.50 (2.01)	mAP 39.69 (44.40)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.78 (2.00)	mAP 44.64 (44.39)
Train: [800/2566]	Time 0.332 (0.332)	Loss 1.85 (2.00)	mAP 52.55 (44.33)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.29 (2.01)	mAP 42.63 (44.38)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 2.10 (2.01)	mAP 46.42 (44.33)
Train: [1100/2566]	Time 0.334 (0.331)	Loss 2.27 (2.00)	mAP 39.86 (44.34)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.89 (2.01)	mAP 38.63 (44.32)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 2.67 (2.01)	mAP 47.47 (44.34)
Train: [1400/2566]	Time 0.332 (0.331)	Loss 2.23 (2.01)	mAP 48.64 (44.30)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.81 (2.01)	mAP 39.28 (44.27)
Train: [1600/2566]	Time 0.332 (0.331)	Loss 1.67 (2.00)	mAP 43.44 (44.29)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.74 (2.00)	mAP 45.55 (44.28)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.09 (2.00)	mAP 52.54 (44.29)
Train: [1900/2566]	Time 0.332 (0.331)	Loss 2.18 (2.00)	mAP 42.58 (44.32)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 2.03 (2.00)	mAP 41.48 (44.30)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 2.19 (2.00)	mAP 49.35 (44.31)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.69 (2.00)	mAP 38.95 (44.33)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.91 (2.00)	mAP 42.13 (44.33)
Train: [2400/2566]	Time 0.332 (0.331)	Loss 2.46 (2.00)	mAP 50.15 (44.33)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.87 (2.00)	mAP 48.83 (44.30)
Train: [9/51]	Time 0.331	Loss 2.00 	mAP 44.33
Test: [0/402]	Time 1.835 (1.835)	Precision 68.55 (68.55)	Recall 79.26 (79.26) 	 P_C 29.64 	 R_C 34.44 	 F_C 29.68 	 P_O 68.55 	 R_O 79.26 	 F_O 73.52
Test: [100/402]	Time 0.499 (0.513)	Precision 86.16 (73.49)	Recall 89.54 (82.96) 	 P_C 59.79 	 R_C 69.74 	 F_C 62.57 	 P_O 71.33 	 R_O 81.90 	 F_O 76.25
Test: [200/402]	Time 0.499 (0.506)	Precision 79.52 (73.81)	Recall 85.71 (83.56) 	 P_C 62.91 	 R_C 71.91 	 F_C 65.94 	 P_O 72.12 	 R_O 82.70 	 F_O 77.05
Test: [300/402]	Time 0.500 (0.504)	Precision 79.15 (73.86)	Recall 83.08 (84.23) 	 P_C 67.72 	 R_C 77.71 	 F_C 71.42 	 P_O 72.27 	 R_O 83.32 	 F_O 77.40
Test: [400/402]	Time 0.499 (0.503)	Precision 55.72 (73.05)	Recall 65.12 (84.19) 	 P_C 69.35 	 R_C 79.07 	 F_C 73.25 	 P_O 71.77 	 R_O 83.50 	 F_O 77.19
Test: [9/51]	  P_C 69.35 	 R_C 79.07 	 F_C 73.25 	 P_O 71.76 	 R_O 83.49 	 F_O 77.18 	 mAP 80.34
Train: [0/2566]	Time 0.996 (0.996)	Loss 2.08 (2.08)	mAP 45.61 (45.61)
Train: [100/2566]	Time 0.330 (0.338)	Loss 1.93 (1.99)	mAP 44.44 (44.31)
Train: [200/2566]	Time 0.331 (0.334)	Loss 2.00 (1.97)	mAP 42.43 (44.35)
Train: [300/2566]	Time 0.331 (0.333)	Loss 2.14 (1.99)	mAP 46.68 (44.32)
Train: [400/2566]	Time 0.331 (0.333)	Loss 2.01 (1.97)	mAP 38.38 (44.45)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.85 (1.98)	mAP 44.96 (44.47)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.29 (1.98)	mAP 38.06 (44.27)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.94 (1.98)	mAP 45.76 (44.22)
Train: [800/2566]	Time 0.331 (0.332)	Loss 2.42 (1.98)	mAP 39.93 (44.34)
Train: [900/2566]	Time 0.331 (0.332)	Loss 1.96 (1.98)	mAP 40.67 (44.39)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 1.89 (1.98)	mAP 48.85 (44.42)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 2.28 (1.98)	mAP 44.10 (44.41)
Train: [1200/2566]	Time 0.332 (0.332)	Loss 2.09 (1.98)	mAP 36.04 (44.39)
Train: [1300/2566]	Time 0.332 (0.332)	Loss 1.81 (1.98)	mAP 42.72 (44.43)
Train: [1400/2566]	Time 0.331 (0.332)	Loss 2.86 (1.98)	mAP 47.54 (44.44)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 2.00 (1.99)	mAP 47.15 (44.42)
Train: [1600/2566]	Time 0.332 (0.331)	Loss 1.54 (1.98)	mAP 36.51 (44.41)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.57 (1.98)	mAP 40.73 (44.40)
Train: [1800/2566]	Time 0.333 (0.331)	Loss 2.03 (1.98)	mAP 48.83 (44.37)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.01 (1.98)	mAP 50.40 (44.36)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.98 (1.98)	mAP 47.63 (44.36)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.58 (1.98)	mAP 45.89 (44.35)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.66 (1.98)	mAP 45.02 (44.40)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 2.50 (1.98)	mAP 45.58 (44.42)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.30 (1.98)	mAP 48.12 (44.43)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.82 (1.97)	mAP 46.58 (44.44)
Train: [10/51]	Time 0.331	Loss 1.97 	mAP 44.44
Test: [0/402]	Time 1.747 (1.747)	Precision 79.88 (79.88)	Recall 75.57 (75.57) 	 P_C 29.66 	 R_C 29.52 	 F_C 27.75 	 P_O 79.88 	 R_O 75.57 	 F_O 77.66
Test: [100/402]	Time 0.500 (0.512)	Precision 90.85 (81.69)	Recall 84.31 (78.96) 	 P_C 68.33 	 R_C 64.22 	 F_C 64.02 	 P_O 80.26 	 R_O 77.40 	 F_O 78.80
Test: [200/402]	Time 0.501 (0.506)	Precision 85.28 (82.83)	Recall 85.28 (79.26) 	 P_C 70.35 	 R_C 66.94 	 F_C 67.38 	 P_O 81.72 	 R_O 78.13 	 F_O 79.89
Test: [300/402]	Time 0.499 (0.504)	Precision 85.03 (82.56)	Recall 79.10 (79.65) 	 P_C 75.79 	 R_C 72.66 	 F_C 73.23 	 P_O 81.34 	 R_O 78.44 	 F_O 79.87
Test: [400/402]	Time 0.499 (0.503)	Precision 67.33 (81.52)	Recall 58.72 (79.56) 	 P_C 77.32 	 R_C 74.18 	 F_C 74.94 	 P_O 80.44 	 R_O 78.62 	 F_O 79.52
Test: [10/51]	  P_C 77.33 	 R_C 74.17 	 F_C 74.94 	 P_O 80.44 	 R_O 78.61 	 F_O 79.52 	 mAP 80.76
Train: [0/2566]	Time 0.981 (0.981)	Loss 2.09 (2.09)	mAP 43.92 (43.92)
Train: [100/2566]	Time 0.333 (0.337)	Loss 2.03 (1.96)	mAP 46.60 (44.30)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.87 (1.95)	mAP 38.63 (44.18)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.90 (1.96)	mAP 37.49 (44.28)
Train: [400/2566]	Time 0.332 (0.333)	Loss 2.61 (1.97)	mAP 39.99 (44.44)
Train: [500/2566]	Time 0.332 (0.332)	Loss 1.58 (1.96)	mAP 39.57 (44.36)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.51 (1.97)	mAP 43.27 (44.27)
Train: [700/2566]	Time 0.330 (0.332)	Loss 1.79 (1.96)	mAP 42.12 (44.31)
Train: [800/2566]	Time 0.332 (0.332)	Loss 1.71 (1.96)	mAP 48.87 (44.37)
Train: [900/2566]	Time 0.330 (0.332)	Loss 2.26 (1.96)	mAP 42.93 (44.45)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 2.28 (1.96)	mAP 43.39 (44.42)
Train: [1100/2566]	Time 0.330 (0.332)	Loss 1.59 (1.96)	mAP 46.71 (44.46)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.60 (1.96)	mAP 40.75 (44.48)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.86 (1.95)	mAP 46.83 (44.47)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.90 (1.95)	mAP 41.97 (44.48)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 2.24 (1.95)	mAP 43.60 (44.48)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 2.12 (1.95)	mAP 47.23 (44.49)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.80 (1.95)	mAP 44.22 (44.53)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.08 (1.95)	mAP 44.70 (44.54)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.66 (1.95)	mAP 38.71 (44.55)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.79 (1.95)	mAP 52.08 (44.53)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.72 (1.95)	mAP 48.32 (44.55)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.04 (1.95)	mAP 39.48 (44.52)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 2.37 (1.95)	mAP 43.31 (44.53)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.13 (1.95)	mAP 41.36 (44.52)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.24 (1.95)	mAP 39.67 (44.51)
Train: [11/51]	Time 0.331	Loss 1.95 	mAP 44.52
Test: [0/402]	Time 1.793 (1.793)	Precision 77.65 (77.65)	Recall 76.99 (76.99) 	 P_C 30.48 	 R_C 31.57 	 F_C 29.30 	 P_O 77.65 	 R_O 76.99 	 F_O 77.32
Test: [100/402]	Time 0.500 (0.513)	Precision 86.93 (78.74)	Recall 86.93 (80.62) 	 P_C 65.05 	 R_C 66.28 	 F_C 63.63 	 P_O 77.63 	 R_O 79.13 	 F_O 78.37
Test: [200/402]	Time 0.500 (0.506)	Precision 84.26 (79.09)	Recall 85.71 (81.45) 	 P_C 67.70 	 R_C 69.24 	 F_C 67.19 	 P_O 78.45 	 R_O 80.23 	 F_O 79.33
Test: [300/402]	Time 0.499 (0.504)	Precision 84.32 (78.71)	Recall 81.59 (82.18) 	 P_C 72.98 	 R_C 75.05 	 F_C 72.98 	 P_O 77.90 	 R_O 80.88 	 F_O 79.36
Test: [400/402]	Time 0.500 (0.503)	Precision 59.36 (78.08)	Recall 64.53 (81.92) 	 P_C 74.93 	 R_C 76.40 	 F_C 74.96 	 P_O 77.37 	 R_O 80.90 	 F_O 79.10
Test: [11/51]	  P_C 74.93 	 R_C 76.40 	 F_C 74.96 	 P_O 77.37 	 R_O 80.89 	 F_O 79.09 	 mAP 81.15
Train: [0/2566]	Time 0.999 (0.999)	Loss 1.67 (1.67)	mAP 32.23 (32.23)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.83 (1.90)	mAP 47.65 (45.17)
Train: [200/2566]	Time 0.331 (0.334)	Loss 2.63 (1.92)	mAP 42.05 (44.68)
Train: [300/2566]	Time 0.332 (0.333)	Loss 1.40 (1.92)	mAP 40.75 (44.63)
Train: [400/2566]	Time 0.331 (0.333)	Loss 2.37 (1.93)	mAP 42.92 (44.72)
Train: [500/2566]	Time 0.331 (0.332)	Loss 2.13 (1.93)	mAP 48.91 (44.66)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.33 (1.94)	mAP 42.55 (44.65)
Train: [700/2566]	Time 0.330 (0.332)	Loss 1.74 (1.93)	mAP 45.09 (44.65)
Train: [800/2566]	Time 0.330 (0.332)	Loss 1.93 (1.94)	mAP 38.32 (44.63)
Train: [900/2566]	Time 0.330 (0.332)	Loss 1.62 (1.94)	mAP 41.17 (44.71)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 1.71 (1.94)	mAP 42.62 (44.68)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 1.57 (1.94)	mAP 43.12 (44.63)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.45 (1.94)	mAP 44.45 (44.65)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 2.49 (1.94)	mAP 43.21 (44.60)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.25 (1.94)	mAP 45.16 (44.61)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.76 (1.94)	mAP 33.38 (44.57)
Train: [1600/2566]	Time 0.332 (0.331)	Loss 2.16 (1.93)	mAP 46.62 (44.55)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 2.27 (1.93)	mAP 44.99 (44.52)
Train: [1800/2566]	Time 0.332 (0.331)	Loss 1.86 (1.93)	mAP 40.77 (44.55)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.66 (1.93)	mAP 45.01 (44.53)
Train: [2000/2566]	Time 0.332 (0.331)	Loss 1.48 (1.93)	mAP 46.73 (44.55)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.47 (1.93)	mAP 30.61 (44.52)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 1.62 (1.93)	mAP 46.26 (44.48)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.04 (1.93)	mAP 43.29 (44.47)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.45 (1.93)	mAP 44.35 (44.49)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 1.83 (1.93)	mAP 42.69 (44.49)
Train: [12/51]	Time 0.331	Loss 1.93 	mAP 44.48
Test: [0/402]	Time 1.738 (1.738)	Precision 73.39 (73.39)	Recall 77.56 (77.56) 	 P_C 29.49 	 R_C 30.93 	 F_C 28.56 	 P_O 73.39 	 R_O 77.56 	 F_O 75.41
Test: [100/402]	Time 0.500 (0.512)	Precision 83.75 (76.89)	Recall 87.58 (81.18) 	 P_C 65.73 	 R_C 66.24 	 F_C 63.88 	 P_O 75.53 	 R_O 79.75 	 F_O 77.58
Test: [200/402]	Time 0.500 (0.506)	Precision 72.76 (75.97)	Recall 87.88 (82.64) 	 P_C 68.09 	 R_C 69.60 	 F_C 67.52 	 P_O 75.12 	 R_O 81.44 	 F_O 78.15
Test: [300/402]	Time 0.499 (0.504)	Precision 83.04 (76.31)	Recall 81.59 (83.34) 	 P_C 73.42 	 R_C 75.47 	 F_C 73.40 	 P_O 75.37 	 R_O 82.10 	 F_O 78.59
Test: [400/402]	Time 0.500 (0.503)	Precision 55.61 (75.82)	Recall 66.28 (83.25) 	 P_C 75.60 	 R_C 76.88 	 F_C 75.53 	 P_O 75.06 	 R_O 82.28 	 F_O 78.50
Test: [12/51]	  P_C 75.60 	 R_C 76.88 	 F_C 75.52 	 P_O 75.05 	 R_O 82.27 	 F_O 78.50 	 mAP 81.60
Train: [0/2566]	Time 0.994 (0.994)	Loss 1.82 (1.82)	mAP 48.98 (48.98)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.53 (1.89)	mAP 49.90 (44.60)
Train: [200/2566]	Time 0.332 (0.335)	Loss 2.54 (1.92)	mAP 43.35 (44.89)
Train: [300/2566]	Time 0.331 (0.334)	Loss 2.35 (1.93)	mAP 40.64 (44.71)
Train: [400/2566]	Time 0.332 (0.333)	Loss 1.87 (1.92)	mAP 42.91 (44.59)
Train: [500/2566]	Time 0.332 (0.333)	Loss 2.17 (1.91)	mAP 45.24 (44.43)
Train: [600/2566]	Time 0.332 (0.333)	Loss 1.90 (1.91)	mAP 45.06 (44.57)
Train: [700/2566]	Time 0.330 (0.332)	Loss 1.59 (1.91)	mAP 41.86 (44.54)
Train: [800/2566]	Time 0.333 (0.332)	Loss 2.22 (1.90)	mAP 46.93 (44.58)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.31 (1.90)	mAP 46.40 (44.62)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.44 (1.91)	mAP 42.47 (44.61)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 1.50 (1.91)	mAP 47.60 (44.68)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 1.54 (1.91)	mAP 42.30 (44.61)
Train: [1300/2566]	Time 0.330 (0.332)	Loss 2.22 (1.91)	mAP 49.31 (44.65)
Train: [1400/2566]	Time 0.330 (0.332)	Loss 1.85 (1.91)	mAP 40.13 (44.64)
Train: [1500/2566]	Time 0.331 (0.332)	Loss 1.26 (1.91)	mAP 41.68 (44.62)
Train: [1600/2566]	Time 0.332 (0.332)	Loss 2.09 (1.91)	mAP 37.85 (44.67)
Train: [1700/2566]	Time 0.332 (0.332)	Loss 1.48 (1.91)	mAP 42.54 (44.67)
Train: [1800/2566]	Time 0.331 (0.332)	Loss 1.76 (1.91)	mAP 39.64 (44.68)
Train: [1900/2566]	Time 0.332 (0.332)	Loss 1.49 (1.91)	mAP 45.80 (44.62)
Train: [2000/2566]	Time 0.332 (0.332)	Loss 1.61 (1.91)	mAP 40.11 (44.65)
Train: [2100/2566]	Time 0.331 (0.332)	Loss 1.73 (1.91)	mAP 41.17 (44.67)
Train: [2200/2566]	Time 0.333 (0.332)	Loss 1.53 (1.91)	mAP 45.73 (44.64)
Train: [2300/2566]	Time 0.334 (0.332)	Loss 1.51 (1.91)	mAP 44.83 (44.64)
Train: [2400/2566]	Time 0.332 (0.332)	Loss 2.15 (1.91)	mAP 39.68 (44.63)
Train: [2500/2566]	Time 0.331 (0.332)	Loss 1.60 (1.91)	mAP 38.99 (44.62)
Train: [13/51]	Time 0.332	Loss 1.91 	mAP 44.64
Test: [0/402]	Time 1.786 (1.786)	Precision 68.89 (68.89)	Recall 79.26 (79.26) 	 P_C 27.86 	 R_C 31.50 	 F_C 27.60 	 P_O 68.89 	 R_O 79.26 	 F_O 73.71
Test: [100/402]	Time 0.500 (0.513)	Precision 81.18 (70.07)	Recall 90.20 (85.24) 	 P_C 62.61 	 R_C 69.00 	 F_C 62.91 	 P_O 69.16 	 R_O 83.87 	 F_O 75.81
Test: [200/402]	Time 0.500 (0.506)	Precision 74.10 (70.63)	Recall 89.18 (85.89) 	 P_C 64.90 	 R_C 72.06 	 F_C 66.21 	 P_O 69.81 	 R_O 84.88 	 F_O 76.61
Test: [300/402]	Time 0.500 (0.504)	Precision 77.50 (71.49)	Recall 84.83 (85.79) 	 P_C 69.12 	 R_C 78.50 	 F_C 72.00 	 P_O 70.43 	 R_O 84.79 	 F_O 76.95
Test: [400/402]	Time 0.500 (0.503)	Precision 59.69 (70.75)	Recall 66.28 (85.68) 	 P_C 70.49 	 R_C 79.78 	 F_C 73.54 	 P_O 69.81 	 R_O 84.95 	 F_O 76.64
Test: [13/51]	  P_C 70.50 	 R_C 79.78 	 F_C 73.54 	 P_O 69.81 	 R_O 84.94 	 F_O 76.64 	 mAP 81.71
Train: [0/2566]	Time 0.988 (0.988)	Loss 1.97 (1.97)	mAP 41.76 (41.76)
Train: [100/2566]	Time 0.332 (0.338)	Loss 1.64 (1.91)	mAP 42.25 (44.46)
Train: [200/2566]	Time 0.330 (0.334)	Loss 2.35 (1.92)	mAP 46.41 (44.48)
Train: [300/2566]	Time 0.332 (0.333)	Loss 1.24 (1.91)	mAP 45.71 (44.66)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.84 (1.90)	mAP 49.51 (44.75)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.32 (1.88)	mAP 45.88 (44.77)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.54 (1.88)	mAP 42.24 (44.85)
Train: [700/2566]	Time 0.330 (0.332)	Loss 2.16 (1.88)	mAP 48.37 (44.85)
Train: [800/2566]	Time 0.332 (0.332)	Loss 1.85 (1.88)	mAP 37.27 (44.78)
Train: [900/2566]	Time 0.330 (0.332)	Loss 2.15 (1.89)	mAP 54.30 (44.80)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.99 (1.89)	mAP 38.33 (44.83)
Train: [1100/2566]	Time 0.330 (0.332)	Loss 1.84 (1.89)	mAP 38.89 (44.86)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 2.00 (1.89)	mAP 44.89 (44.86)
Train: [1300/2566]	Time 0.332 (0.331)	Loss 1.42 (1.89)	mAP 43.11 (44.84)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.87 (1.89)	mAP 45.07 (44.89)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.85 (1.89)	mAP 47.25 (44.87)
Train: [1600/2566]	Time 0.332 (0.331)	Loss 1.42 (1.89)	mAP 39.07 (44.86)
Train: [1700/2566]	Time 0.332 (0.331)	Loss 1.55 (1.89)	mAP 48.47 (44.85)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.48 (1.89)	mAP 45.64 (44.80)
Train: [1900/2566]	Time 0.332 (0.331)	Loss 2.53 (1.89)	mAP 51.48 (44.80)
Train: [2000/2566]	Time 0.329 (0.331)	Loss 1.70 (1.89)	mAP 38.48 (44.80)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 1.49 (1.89)	mAP 46.09 (44.77)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.66 (1.89)	mAP 45.97 (44.78)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.81 (1.89)	mAP 41.35 (44.73)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.07 (1.89)	mAP 41.91 (44.72)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 1.86 (1.89)	mAP 40.41 (44.69)
Train: [14/51]	Time 0.331	Loss 1.89 	mAP 44.69
Test: [0/402]	Time 1.747 (1.747)	Precision 73.60 (73.60)	Recall 78.41 (78.41) 	 P_C 29.75 	 R_C 32.72 	 F_C 29.56 	 P_O 73.60 	 R_O 78.41 	 F_O 75.93
Test: [100/402]	Time 0.500 (0.512)	Precision 81.44 (77.59)	Recall 88.89 (82.62) 	 P_C 60.81 	 R_C 70.24 	 F_C 63.36 	 P_O 76.01 	 R_O 81.27 	 F_O 78.55
Test: [200/402]	Time 0.500 (0.506)	Precision 81.53 (77.58)	Recall 87.88 (83.23) 	 P_C 62.98 	 R_C 73.17 	 F_C 66.49 	 P_O 76.43 	 R_O 82.21 	 F_O 79.21
Test: [300/402]	Time 0.500 (0.504)	Precision 81.48 (76.18)	Recall 82.09 (84.50) 	 P_C 68.26 	 R_C 78.59 	 F_C 72.18 	 P_O 74.77 	 R_O 83.48 	 F_O 78.89
Test: [400/402]	Time 0.499 (0.503)	Precision 55.61 (75.23)	Recall 69.19 (84.43) 	 P_C 70.23 	 R_C 80.06 	 F_C 74.19 	 P_O 73.99 	 R_O 83.63 	 F_O 78.52
Test: [14/51]	  P_C 70.23 	 R_C 80.06 	 F_C 74.19 	 P_O 73.98 	 R_O 83.63 	 F_O 78.51 	 mAP 82.06
Train: [0/2566]	Time 1.033 (1.033)	Loss 2.02 (2.02)	mAP 50.81 (50.81)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.91 (1.85)	mAP 50.17 (45.07)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.28 (1.84)	mAP 34.85 (44.65)
Train: [300/2566]	Time 0.331 (0.333)	Loss 2.00 (1.85)	mAP 48.32 (44.93)
Train: [400/2566]	Time 0.332 (0.333)	Loss 2.24 (1.86)	mAP 49.58 (44.88)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.79 (1.87)	mAP 38.28 (44.87)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.85 (1.87)	mAP 51.04 (44.77)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.73 (1.88)	mAP 43.71 (44.84)
Train: [800/2566]	Time 0.331 (0.332)	Loss 1.85 (1.88)	mAP 46.64 (44.89)
Train: [900/2566]	Time 0.330 (0.332)	Loss 1.41 (1.88)	mAP 43.92 (44.90)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 2.12 (1.88)	mAP 42.03 (44.94)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.94 (1.88)	mAP 43.85 (44.94)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 2.27 (1.88)	mAP 48.16 (44.97)
Train: [1300/2566]	Time 0.332 (0.331)	Loss 1.79 (1.88)	mAP 49.44 (44.97)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.62 (1.88)	mAP 43.25 (44.92)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.95 (1.88)	mAP 48.60 (44.89)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.84 (1.88)	mAP 48.85 (44.87)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.78 (1.88)	mAP 44.11 (44.90)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.85 (1.88)	mAP 44.73 (44.86)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.52 (1.88)	mAP 39.33 (44.86)
Train: [2000/2566]	Time 0.332 (0.331)	Loss 1.80 (1.88)	mAP 44.53 (44.86)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.52 (1.88)	mAP 42.68 (44.88)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 2.12 (1.88)	mAP 47.60 (44.88)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.25 (1.88)	mAP 44.16 (44.88)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.23 (1.88)	mAP 43.67 (44.90)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.05 (1.88)	mAP 40.99 (44.90)
Train: [15/51]	Time 0.331	Loss 1.88 	mAP 44.90
Test: [0/402]	Time 1.767 (1.767)	Precision 73.81 (73.81)	Recall 79.26 (79.26) 	 P_C 29.72 	 R_C 33.09 	 F_C 29.64 	 P_O 73.81 	 R_O 79.26 	 F_O 76.44
Test: [100/402]	Time 0.500 (0.512)	Precision 80.84 (73.85)	Recall 88.24 (84.31) 	 P_C 65.07 	 R_C 69.22 	 F_C 64.98 	 P_O 72.29 	 R_O 83.11 	 F_O 77.32
Test: [200/402]	Time 0.501 (0.506)	Precision 83.88 (75.36)	Recall 87.88 (84.33) 	 P_C 67.08 	 R_C 71.80 	 F_C 67.95 	 P_O 74.02 	 R_O 83.52 	 F_O 78.48
Test: [300/402]	Time 0.500 (0.504)	Precision 81.17 (75.83)	Recall 82.59 (84.70) 	 P_C 71.56 	 R_C 77.77 	 F_C 73.40 	 P_O 74.32 	 R_O 83.87 	 F_O 78.81
Test: [400/402]	Time 0.500 (0.503)	Precision 56.86 (74.76)	Recall 67.44 (84.71) 	 P_C 72.92 	 R_C 79.27 	 F_C 75.05 	 P_O 73.36 	 R_O 84.10 	 F_O 78.36
Test: [15/51]	  P_C 72.92 	 R_C 79.27 	 F_C 75.05 	 P_O 73.35 	 R_O 84.10 	 F_O 78.36 	 mAP 82.19
Train: [0/2566]	Time 0.997 (0.997)	Loss 1.38 (1.38)	mAP 36.46 (36.46)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.61 (1.84)	mAP 42.46 (45.48)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.99 (1.88)	mAP 42.28 (45.26)
Train: [300/2566]	Time 0.329 (0.333)	Loss 1.67 (1.88)	mAP 44.37 (44.98)
Train: [400/2566]	Time 0.331 (0.332)	Loss 2.00 (1.89)	mAP 46.15 (44.93)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.56 (1.88)	mAP 45.89 (44.89)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.71 (1.88)	mAP 45.93 (44.90)
Train: [700/2566]	Time 0.330 (0.332)	Loss 2.00 (1.88)	mAP 46.51 (44.93)
Train: [800/2566]	Time 0.331 (0.332)	Loss 1.93 (1.87)	mAP 41.32 (44.91)
Train: [900/2566]	Time 0.332 (0.331)	Loss 1.90 (1.87)	mAP 42.51 (44.87)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 2.04 (1.87)	mAP 41.42 (44.91)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 2.18 (1.87)	mAP 39.24 (44.84)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.82 (1.87)	mAP 47.43 (44.82)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.71 (1.87)	mAP 47.63 (44.83)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.82 (1.87)	mAP 42.52 (44.85)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.71 (1.87)	mAP 50.24 (44.88)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 2.10 (1.87)	mAP 42.93 (44.86)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 2.21 (1.87)	mAP 45.11 (44.91)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.89 (1.87)	mAP 45.51 (44.90)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.98 (1.87)	mAP 46.64 (44.89)
Train: [2000/2566]	Time 0.332 (0.331)	Loss 1.64 (1.87)	mAP 45.11 (44.87)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.85 (1.87)	mAP 42.77 (44.88)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 1.99 (1.87)	mAP 40.39 (44.88)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.94 (1.87)	mAP 46.46 (44.89)
Train: [2400/2566]	Time 0.332 (0.331)	Loss 1.55 (1.87)	mAP 49.84 (44.88)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.92 (1.87)	mAP 46.74 (44.89)
Train: [16/51]	Time 0.331	Loss 1.87 	mAP 44.88
Test: [0/402]	Time 1.750 (1.750)	Precision 83.54 (83.54)	Recall 75.00 (75.00) 	 P_C 32.08 	 R_C 29.03 	 F_C 29.63 	 P_O 83.54 	 R_O 75.00 	 F_O 79.04
Test: [100/402]	Time 0.500 (0.512)	Precision 93.10 (83.54)	Recall 88.24 (78.61) 	 P_C 72.00 	 R_C 63.32 	 F_C 65.11 	 P_O 82.28 	 R_O 76.98 	 F_O 79.54
Test: [200/402]	Time 0.500 (0.506)	Precision 91.08 (84.14)	Recall 83.98 (79.18) 	 P_C 74.65 	 R_C 66.31 	 F_C 69.04 	 P_O 83.34 	 R_O 77.90 	 F_O 80.53
Test: [300/402]	Time 0.500 (0.504)	Precision 86.10 (83.41)	Recall 78.61 (80.26) 	 P_C 78.55 	 R_C 72.59 	 F_C 74.40 	 P_O 82.25 	 R_O 79.08 	 F_O 80.63
Test: [400/402]	Time 0.499 (0.503)	Precision 64.97 (82.39)	Recall 66.86 (80.27) 	 P_C 79.83 	 R_C 74.13 	 F_C 75.98 	 P_O 81.35 	 R_O 79.35 	 F_O 80.34
Test: [16/51]	  P_C 79.84 	 R_C 74.12 	 F_C 75.98 	 P_O 81.35 	 R_O 79.35 	 F_O 80.34 	 mAP 82.55
Train: [0/2566]	Time 1.042 (1.042)	Loss 1.34 (1.34)	mAP 47.74 (47.74)
Train: [100/2566]	Time 0.332 (0.338)	Loss 1.94 (1.85)	mAP 36.30 (44.66)
Train: [200/2566]	Time 0.332 (0.335)	Loss 1.61 (1.86)	mAP 44.50 (44.77)
Train: [300/2566]	Time 0.332 (0.334)	Loss 1.67 (1.87)	mAP 45.61 (44.71)
Train: [400/2566]	Time 0.330 (0.333)	Loss 2.09 (1.87)	mAP 44.77 (44.75)
Train: [500/2566]	Time 0.331 (0.333)	Loss 2.19 (1.87)	mAP 43.23 (44.79)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.09 (1.87)	mAP 45.51 (44.74)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.50 (1.86)	mAP 41.27 (44.69)
Train: [800/2566]	Time 0.331 (0.332)	Loss 1.30 (1.86)	mAP 43.40 (44.79)
Train: [900/2566]	Time 0.332 (0.332)	Loss 1.94 (1.86)	mAP 47.51 (44.87)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 2.19 (1.86)	mAP 42.61 (44.88)
Train: [1100/2566]	Time 0.330 (0.332)	Loss 1.19 (1.86)	mAP 48.87 (44.93)
Train: [1200/2566]	Time 0.330 (0.332)	Loss 1.60 (1.86)	mAP 43.46 (44.95)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.68 (1.85)	mAP 42.69 (44.91)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.76 (1.85)	mAP 47.46 (44.93)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.11 (1.85)	mAP 52.26 (44.90)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 2.06 (1.86)	mAP 50.65 (44.86)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 2.30 (1.86)	mAP 49.75 (44.91)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.86 (1.86)	mAP 47.16 (44.92)
Train: [1900/2566]	Time 0.332 (0.331)	Loss 2.03 (1.86)	mAP 53.18 (44.90)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.59 (1.86)	mAP 39.92 (44.89)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.73 (1.86)	mAP 45.97 (44.91)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.88 (1.86)	mAP 40.89 (44.89)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.77 (1.86)	mAP 48.95 (44.88)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.70 (1.86)	mAP 49.70 (44.90)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.61 (1.86)	mAP 51.66 (44.89)
Train: [17/51]	Time 0.331	Loss 1.85 	mAP 44.89
Test: [0/402]	Time 1.764 (1.764)	Precision 77.12 (77.12)	Recall 77.56 (77.56) 	 P_C 29.35 	 R_C 31.46 	 F_C 29.07 	 P_O 77.12 	 R_O 77.56 	 F_O 77.34
Test: [100/402]	Time 0.500 (0.512)	Precision 88.24 (79.16)	Recall 88.24 (82.22) 	 P_C 67.66 	 R_C 66.06 	 F_C 65.08 	 P_O 77.90 	 R_O 80.75 	 F_O 79.30
Test: [200/402]	Time 0.500 (0.506)	Precision 82.52 (79.72)	Recall 87.88 (82.74) 	 P_C 70.08 	 R_C 69.22 	 F_C 68.77 	 P_O 78.70 	 R_O 81.61 	 F_O 80.13
Test: [300/402]	Time 0.499 (0.504)	Precision 84.11 (79.78)	Recall 80.35 (83.11) 	 P_C 75.35 	 R_C 75.28 	 F_C 74.65 	 P_O 78.67 	 R_O 81.97 	 F_O 80.28
Test: [400/402]	Time 0.500 (0.503)	Precision 59.90 (78.98)	Recall 66.86 (82.91) 	 P_C 77.13 	 R_C 76.63 	 F_C 76.43 	 P_O 78.02 	 R_O 82.00 	 F_O 79.96
Test: [17/51]	  P_C 77.14 	 R_C 76.63 	 F_C 76.43 	 P_O 78.02 	 R_O 81.99 	 F_O 79.96 	 mAP 82.48
Train: [0/2566]	Time 0.976 (0.976)	Loss 1.71 (1.71)	mAP 51.34 (51.34)
Train: [100/2566]	Time 0.331 (0.338)	Loss 2.41 (1.78)	mAP 47.56 (46.02)
Train: [200/2566]	Time 0.331 (0.334)	Loss 2.25 (1.83)	mAP 53.38 (45.27)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.93 (1.82)	mAP 44.63 (45.04)
Train: [400/2566]	Time 0.331 (0.333)	Loss 1.57 (1.82)	mAP 38.36 (45.06)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.67 (1.82)	mAP 35.92 (45.06)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.70 (1.83)	mAP 45.82 (45.00)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.92 (1.84)	mAP 39.06 (45.03)
Train: [800/2566]	Time 0.331 (0.332)	Loss 1.72 (1.83)	mAP 48.47 (45.04)
Train: [900/2566]	Time 0.332 (0.331)	Loss 2.24 (1.84)	mAP 39.84 (45.05)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 2.01 (1.84)	mAP 43.96 (45.04)
Train: [1100/2566]	Time 0.332 (0.331)	Loss 1.71 (1.84)	mAP 45.45 (45.08)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 2.00 (1.85)	mAP 35.96 (45.09)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.79 (1.84)	mAP 38.91 (45.06)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.93 (1.84)	mAP 43.93 (45.04)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 2.27 (1.84)	mAP 49.16 (45.05)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 2.12 (1.84)	mAP 45.66 (45.04)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 2.51 (1.84)	mAP 52.68 (45.03)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 2.41 (1.84)	mAP 43.09 (45.06)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.10 (1.84)	mAP 47.62 (45.06)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.97 (1.84)	mAP 49.73 (45.05)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 2.37 (1.84)	mAP 52.07 (45.04)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.83 (1.84)	mAP 39.40 (45.03)
Train: [2300/2566]	Time 0.332 (0.331)	Loss 2.30 (1.84)	mAP 47.22 (45.05)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.66 (1.84)	mAP 40.36 (45.03)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.82 (1.84)	mAP 47.59 (45.02)
Train: [18/51]	Time 0.331	Loss 1.84 	mAP 45.03
Test: [0/402]	Time 1.762 (1.762)	Precision 75.88 (75.88)	Recall 79.55 (79.55) 	 P_C 28.92 	 R_C 31.32 	 F_C 28.40 	 P_O 75.88 	 R_O 79.55 	 F_O 77.67
Test: [100/402]	Time 0.501 (0.512)	Precision 88.31 (77.88)	Recall 88.89 (82.65) 	 P_C 64.15 	 R_C 69.06 	 F_C 64.58 	 P_O 75.73 	 R_O 81.40 	 F_O 78.46
Test: [200/402]	Time 0.500 (0.506)	Precision 83.61 (79.09)	Recall 86.15 (82.62) 	 P_C 66.51 	 R_C 72.34 	 F_C 68.04 	 P_O 77.53 	 R_O 81.64 	 F_O 79.53
Test: [300/402]	Time 0.500 (0.504)	Precision 82.82 (78.48)	Recall 80.35 (83.54) 	 P_C 71.87 	 R_C 77.78 	 F_C 73.69 	 P_O 76.65 	 R_O 82.56 	 F_O 79.50
Test: [400/402]	Time 0.500 (0.503)	Precision 53.24 (77.35)	Recall 66.86 (83.59) 	 P_C 73.64 	 R_C 79.18 	 F_C 75.53 	 P_O 75.65 	 R_O 82.84 	 F_O 79.08
Test: [18/51]	  P_C 73.64 	 R_C 79.18 	 F_C 75.53 	 P_O 75.65 	 R_O 82.84 	 F_O 79.08 	 mAP 82.68
Train: [0/2566]	Time 1.017 (1.017)	Loss 2.30 (2.30)	mAP 46.75 (46.75)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.43 (1.83)	mAP 42.38 (44.27)
Train: [200/2566]	Time 0.332 (0.334)	Loss 2.10 (1.83)	mAP 45.89 (44.50)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.82 (1.83)	mAP 46.52 (44.68)
Train: [400/2566]	Time 0.331 (0.332)	Loss 2.20 (1.82)	mAP 34.81 (44.71)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.89 (1.83)	mAP 44.21 (44.71)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.81 (1.83)	mAP 51.59 (44.75)
Train: [700/2566]	Time 0.330 (0.332)	Loss 1.60 (1.82)	mAP 42.20 (44.81)
Train: [800/2566]	Time 0.330 (0.332)	Loss 1.51 (1.82)	mAP 45.33 (44.73)
Train: [900/2566]	Time 0.332 (0.331)	Loss 2.10 (1.82)	mAP 41.54 (44.75)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 1.90 (1.82)	mAP 42.00 (44.82)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.73 (1.82)	mAP 49.91 (44.87)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 2.01 (1.82)	mAP 36.94 (44.84)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.92 (1.82)	mAP 45.41 (44.88)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.66 (1.82)	mAP 50.86 (44.97)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.70 (1.83)	mAP 46.90 (45.02)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.56 (1.83)	mAP 50.96 (45.09)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.94 (1.83)	mAP 43.25 (45.09)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.99 (1.83)	mAP 48.41 (45.06)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.87 (1.83)	mAP 43.73 (45.05)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.61 (1.83)	mAP 49.44 (45.04)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.98 (1.83)	mAP 51.55 (45.04)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.93 (1.83)	mAP 45.24 (45.04)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.68 (1.83)	mAP 37.22 (45.05)
Train: [2400/2566]	Time 0.332 (0.331)	Loss 1.36 (1.83)	mAP 47.48 (45.06)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 2.05 (1.83)	mAP 50.00 (45.03)
Train: [19/51]	Time 0.331	Loss 1.83 	mAP 45.04
Test: [0/402]	Time 1.818 (1.818)	Precision 78.45 (78.45)	Recall 77.56 (77.56) 	 P_C 30.23 	 R_C 30.62 	 F_C 29.23 	 P_O 78.45 	 R_O 77.56 	 F_O 78.00
Test: [100/402]	Time 0.500 (0.513)	Precision 91.22 (79.30)	Recall 88.24 (81.49) 	 P_C 71.07 	 R_C 66.00 	 F_C 65.94 	 P_O 77.95 	 R_O 80.06 	 F_O 78.99
Test: [200/402]	Time 0.500 (0.506)	Precision 84.58 (80.52)	Recall 83.12 (81.73) 	 P_C 73.07 	 R_C 68.50 	 F_C 69.25 	 P_O 79.49 	 R_O 80.61 	 F_O 80.05
Test: [300/402]	Time 0.500 (0.504)	Precision 85.60 (80.42)	Recall 79.85 (82.34) 	 P_C 76.16 	 R_C 74.69 	 F_C 74.30 	 P_O 79.27 	 R_O 81.20 	 F_O 80.23
Test: [400/402]	Time 0.500 (0.503)	Precision 60.42 (79.42)	Recall 67.44 (82.42) 	 P_C 77.91 	 R_C 76.19 	 F_C 76.22 	 P_O 78.38 	 R_O 81.54 	 F_O 79.93
Test: [19/51]	  P_C 77.91 	 R_C 76.19 	 F_C 76.22 	 P_O 78.38 	 R_O 81.54 	 F_O 79.93 	 mAP 82.74
Train: [0/2566]	Time 1.030 (1.030)	Loss 1.38 (1.38)	mAP 42.40 (42.40)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.74 (1.84)	mAP 44.76 (44.99)
Train: [200/2566]	Time 0.333 (0.335)	Loss 1.74 (1.82)	mAP 38.89 (44.83)
Train: [300/2566]	Time 0.332 (0.334)	Loss 1.82 (1.82)	mAP 48.17 (44.91)
Train: [400/2566]	Time 0.331 (0.333)	Loss 2.19 (1.83)	mAP 49.82 (44.90)
Train: [500/2566]	Time 0.331 (0.333)	Loss 1.42 (1.83)	mAP 38.42 (44.97)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.74 (1.83)	mAP 45.77 (44.96)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.42 (1.83)	mAP 34.48 (44.90)
Train: [800/2566]	Time 0.332 (0.332)	Loss 1.72 (1.83)	mAP 46.21 (44.87)
Train: [900/2566]	Time 0.332 (0.332)	Loss 1.74 (1.83)	mAP 43.57 (44.81)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.38 (1.84)	mAP 46.43 (44.84)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 1.73 (1.84)	mAP 44.64 (44.79)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 1.91 (1.84)	mAP 41.06 (44.82)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 1.58 (1.84)	mAP 37.89 (44.79)
Train: [1400/2566]	Time 0.332 (0.332)	Loss 1.87 (1.84)	mAP 48.33 (44.89)
Train: [1500/2566]	Time 0.331 (0.332)	Loss 2.09 (1.84)	mAP 43.27 (44.94)
Train: [1600/2566]	Time 0.332 (0.332)	Loss 2.30 (1.84)	mAP 41.56 (44.96)
Train: [1700/2566]	Time 0.332 (0.331)	Loss 1.85 (1.84)	mAP 46.84 (45.00)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.90 (1.83)	mAP 44.63 (45.02)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.95 (1.83)	mAP 40.26 (45.02)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.26 (1.83)	mAP 48.02 (45.01)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 1.99 (1.83)	mAP 49.89 (44.97)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.67 (1.83)	mAP 52.19 (45.01)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.71 (1.83)	mAP 46.85 (45.02)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.21 (1.83)	mAP 41.58 (45.01)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.57 (1.83)	mAP 38.57 (45.02)
Train: [20/51]	Time 0.331	Loss 1.83 	mAP 45.00
Test: [0/402]	Time 1.771 (1.771)	Precision 70.57 (70.57)	Recall 80.40 (80.40) 	 P_C 28.58 	 R_C 32.84 	 F_C 29.09 	 P_O 70.57 	 R_O 80.40 	 F_O 75.17
Test: [100/402]	Time 0.500 (0.512)	Precision 79.43 (70.93)	Recall 90.85 (85.82) 	 P_C 61.04 	 R_C 71.91 	 F_C 63.89 	 P_O 69.34 	 R_O 84.78 	 F_O 76.28
Test: [200/402]	Time 0.501 (0.506)	Precision 75.93 (72.18)	Recall 88.74 (85.96) 	 P_C 63.22 	 R_C 74.55 	 F_C 67.42 	 P_O 70.93 	 R_O 85.17 	 F_O 77.40
Test: [300/402]	Time 0.500 (0.504)	Precision 78.04 (72.27)	Recall 83.08 (86.61) 	 P_C 68.05 	 R_C 80.40 	 F_C 72.85 	 P_O 70.98 	 R_O 85.79 	 F_O 77.68
Test: [400/402]	Time 0.500 (0.503)	Precision 50.64 (71.65)	Recall 68.60 (86.40) 	 P_C 69.46 	 R_C 81.78 	 F_C 74.44 	 P_O 70.53 	 R_O 85.80 	 F_O 77.42
Test: [20/51]	  P_C 69.46 	 R_C 81.78 	 F_C 74.44 	 P_O 70.53 	 R_O 85.79 	 F_O 77.42 	 mAP 82.68
Train: [0/2566]	Time 1.041 (1.041)	Loss 1.35 (1.35)	mAP 42.55 (42.55)
Train: [100/2566]	Time 0.331 (0.338)	Loss 2.04 (1.83)	mAP 55.07 (45.06)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.52 (1.83)	mAP 42.41 (44.78)
Train: [300/2566]	Time 0.331 (0.333)	Loss 2.09 (1.82)	mAP 46.18 (44.75)
Train: [400/2566]	Time 0.331 (0.333)	Loss 1.74 (1.82)	mAP 42.32 (44.67)
Train: [500/2566]	Time 0.331 (0.333)	Loss 1.65 (1.81)	mAP 45.85 (44.68)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.05 (1.82)	mAP 45.57 (44.80)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.86 (1.83)	mAP 44.82 (44.81)
Train: [800/2566]	Time 0.330 (0.332)	Loss 1.54 (1.82)	mAP 46.28 (44.92)
Train: [900/2566]	Time 0.331 (0.332)	Loss 1.99 (1.82)	mAP 41.70 (44.96)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 1.82 (1.82)	mAP 48.39 (44.98)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 1.49 (1.81)	mAP 38.27 (44.96)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.86 (1.81)	mAP 43.85 (44.94)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.55 (1.81)	mAP 35.44 (44.93)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.79 (1.81)	mAP 46.32 (44.96)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.75 (1.81)	mAP 43.07 (44.93)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.81 (1.81)	mAP 41.38 (44.91)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.88 (1.81)	mAP 45.93 (44.95)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.73 (1.81)	mAP 45.97 (44.95)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.35 (1.81)	mAP 40.28 (44.92)
Train: [2000/2566]	Time 0.329 (0.331)	Loss 1.59 (1.82)	mAP 47.33 (44.99)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 1.57 (1.82)	mAP 40.59 (44.97)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 1.99 (1.82)	mAP 45.20 (44.96)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.76 (1.82)	mAP 43.59 (45.00)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.93 (1.82)	mAP 40.03 (45.03)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.54 (1.82)	mAP 47.40 (45.07)
Train: [21/51]	Time 0.331	Loss 1.82 	mAP 45.05
Test: [0/402]	Time 1.834 (1.834)	Precision 77.07 (77.07)	Recall 79.26 (79.26) 	 P_C 31.45 	 R_C 32.14 	 F_C 30.32 	 P_O 77.07 	 R_O 79.26 	 F_O 78.15
Test: [100/402]	Time 0.499 (0.513)	Precision 81.40 (76.88)	Recall 91.50 (83.44) 	 P_C 64.15 	 R_C 68.29 	 F_C 64.38 	 P_O 76.20 	 R_O 81.84 	 F_O 78.92
Test: [200/402]	Time 0.500 (0.506)	Precision 83.20 (77.08)	Recall 87.88 (84.20) 	 P_C 67.03 	 R_C 71.02 	 F_C 67.98 	 P_O 76.47 	 R_O 82.99 	 F_O 79.59
Test: [300/402]	Time 0.500 (0.504)	Precision 82.88 (77.59)	Recall 83.08 (84.38) 	 P_C 72.15 	 R_C 77.26 	 F_C 73.87 	 P_O 76.94 	 R_O 83.11 	 F_O 79.91
Test: [400/402]	Time 0.499 (0.503)	Precision 59.60 (76.74)	Recall 68.60 (84.25) 	 P_C 73.84 	 R_C 78.77 	 F_C 75.78 	 P_O 76.21 	 R_O 83.23 	 F_O 79.57
Test: [21/51]	  P_C 73.84 	 R_C 78.77 	 F_C 75.78 	 P_O 76.21 	 R_O 83.23 	 F_O 79.56 	 mAP 83.10
Train: [0/2566]	Time 1.040 (1.040)	Loss 2.10 (2.10)	mAP 39.43 (39.43)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.80 (1.86)	mAP 45.67 (45.74)
Train: [200/2566]	Time 0.332 (0.335)	Loss 2.20 (1.85)	mAP 38.77 (45.34)
Train: [300/2566]	Time 0.331 (0.334)	Loss 1.82 (1.83)	mAP 47.83 (45.31)
Train: [400/2566]	Time 0.331 (0.333)	Loss 1.81 (1.82)	mAP 35.72 (45.35)
Train: [500/2566]	Time 0.333 (0.333)	Loss 1.98 (1.82)	mAP 46.95 (45.23)
Train: [600/2566]	Time 0.332 (0.332)	Loss 1.58 (1.82)	mAP 44.51 (45.28)
Train: [700/2566]	Time 0.332 (0.332)	Loss 2.32 (1.82)	mAP 53.08 (45.22)
Train: [800/2566]	Time 0.332 (0.332)	Loss 1.64 (1.82)	mAP 45.91 (45.21)
Train: [900/2566]	Time 0.332 (0.332)	Loss 1.51 (1.81)	mAP 37.66 (45.24)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.73 (1.81)	mAP 34.73 (45.16)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 1.87 (1.81)	mAP 46.87 (45.18)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 1.87 (1.81)	mAP 46.64 (45.20)
Train: [1300/2566]	Time 0.330 (0.332)	Loss 1.44 (1.81)	mAP 45.62 (45.19)
Train: [1400/2566]	Time 0.331 (0.332)	Loss 2.10 (1.81)	mAP 43.88 (45.17)
Train: [1500/2566]	Time 0.332 (0.332)	Loss 1.70 (1.81)	mAP 43.50 (45.14)
Train: [1600/2566]	Time 0.330 (0.332)	Loss 1.73 (1.81)	mAP 44.29 (45.14)
Train: [1700/2566]	Time 0.332 (0.332)	Loss 1.70 (1.82)	mAP 33.12 (45.14)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.96 (1.82)	mAP 51.11 (45.19)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.46 (1.81)	mAP 45.42 (45.15)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.96 (1.81)	mAP 45.88 (45.15)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.88 (1.81)	mAP 41.88 (45.12)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.77 (1.81)	mAP 35.70 (45.12)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.73 (1.81)	mAP 46.89 (45.11)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.66 (1.81)	mAP 41.93 (45.10)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.55 (1.81)	mAP 48.79 (45.09)
Train: [22/51]	Time 0.331	Loss 1.81 	mAP 45.11
Test: [0/402]	Time 1.778 (1.778)	Precision 73.73 (73.73)	Recall 78.12 (78.12) 	 P_C 28.06 	 R_C 29.48 	 F_C 27.93 	 P_O 73.73 	 R_O 78.12 	 F_O 75.86
Test: [100/402]	Time 0.500 (0.512)	Precision 90.00 (77.47)	Recall 88.24 (83.60) 	 P_C 65.45 	 R_C 69.43 	 F_C 65.51 	 P_O 75.47 	 R_O 82.44 	 F_O 78.80
Test: [200/402]	Time 0.500 (0.506)	Precision 83.40 (78.63)	Recall 87.01 (83.75) 	 P_C 68.32 	 R_C 72.40 	 F_C 69.08 	 P_O 77.16 	 R_O 82.87 	 F_O 79.91
Test: [300/402]	Time 0.500 (0.504)	Precision 83.12 (77.82)	Recall 82.09 (84.74) 	 P_C 72.74 	 R_C 78.11 	 F_C 74.35 	 P_O 76.04 	 R_O 83.89 	 F_O 79.78
Test: [400/402]	Time 0.500 (0.503)	Precision 53.64 (76.59)	Recall 68.60 (84.72) 	 P_C 74.73 	 R_C 79.33 	 F_C 76.24 	 P_O 75.02 	 R_O 84.11 	 F_O 79.30
Test: [22/51]	  P_C 74.73 	 R_C 79.32 	 F_C 76.24 	 P_O 75.01 	 R_O 84.10 	 F_O 79.30 	 mAP 83.10
Train: [0/2566]	Time 1.003 (1.003)	Loss 1.76 (1.76)	mAP 47.24 (47.24)
Train: [100/2566]	Time 0.332 (0.338)	Loss 1.53 (1.82)	mAP 43.18 (44.73)
Train: [200/2566]	Time 0.333 (0.335)	Loss 1.71 (1.80)	mAP 43.38 (44.83)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.64 (1.80)	mAP 47.08 (44.87)
Train: [400/2566]	Time 0.332 (0.333)	Loss 2.07 (1.79)	mAP 41.35 (45.05)
Train: [500/2566]	Time 0.332 (0.332)	Loss 1.86 (1.79)	mAP 44.34 (44.96)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.68 (1.79)	mAP 37.90 (44.99)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.78 (1.79)	mAP 54.32 (45.17)
Train: [800/2566]	Time 0.331 (0.332)	Loss 2.01 (1.79)	mAP 44.17 (45.17)
Train: [900/2566]	Time 0.330 (0.332)	Loss 1.42 (1.79)	mAP 44.10 (45.10)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.24 (1.80)	mAP 39.21 (45.12)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 0.98 (1.80)	mAP 41.91 (45.15)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.79 (1.80)	mAP 40.73 (45.13)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.83 (1.80)	mAP 37.85 (45.17)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.94 (1.80)	mAP 45.04 (45.11)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.97 (1.79)	mAP 45.12 (45.13)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.91 (1.80)	mAP 47.46 (45.14)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.87 (1.80)	mAP 54.34 (45.15)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.84 (1.80)	mAP 46.49 (45.13)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.23 (1.80)	mAP 45.57 (45.11)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 2.24 (1.80)	mAP 49.45 (45.14)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 2.06 (1.80)	mAP 34.41 (45.17)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.19 (1.80)	mAP 50.50 (45.18)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.35 (1.80)	mAP 45.52 (45.16)
Train: [2400/2566]	Time 0.332 (0.331)	Loss 1.65 (1.80)	mAP 45.99 (45.13)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 2.02 (1.80)	mAP 42.84 (45.15)
Train: [23/51]	Time 0.331	Loss 1.80 	mAP 45.14
Test: [0/402]	Time 1.810 (1.810)	Precision 75.34 (75.34)	Recall 78.98 (78.98) 	 P_C 30.62 	 R_C 32.65 	 F_C 30.17 	 P_O 75.34 	 R_O 78.98 	 F_O 77.12
Test: [100/402]	Time 0.500 (0.513)	Precision 88.00 (75.54)	Recall 86.27 (84.10) 	 P_C 66.51 	 R_C 69.54 	 F_C 66.36 	 P_O 73.78 	 R_O 82.93 	 F_O 78.08
Test: [200/402]	Time 0.500 (0.506)	Precision 82.04 (76.50)	Recall 87.01 (84.09) 	 P_C 68.26 	 R_C 72.54 	 F_C 69.08 	 P_O 75.13 	 R_O 83.26 	 F_O 78.98
Test: [300/402]	Time 0.500 (0.504)	Precision 81.16 (76.85)	Recall 83.58 (84.63) 	 P_C 72.44 	 R_C 78.31 	 F_C 74.33 	 P_O 75.39 	 R_O 83.75 	 F_O 79.35
Test: [400/402]	Time 0.500 (0.503)	Precision 58.74 (75.89)	Recall 70.35 (84.67) 	 P_C 74.13 	 R_C 79.54 	 F_C 76.05 	 P_O 74.59 	 R_O 84.02 	 F_O 79.03
Test: [23/51]	  P_C 74.14 	 R_C 79.54 	 F_C 76.05 	 P_O 74.59 	 R_O 84.02 	 F_O 79.02 	 mAP 83.09
Train: [0/2566]	Time 1.035 (1.035)	Loss 1.64 (1.64)	mAP 43.52 (43.52)
Train: [100/2566]	Time 0.332 (0.338)	Loss 1.68 (1.79)	mAP 42.97 (45.88)
Train: [200/2566]	Time 0.331 (0.335)	Loss 2.36 (1.81)	mAP 45.96 (45.52)
Train: [300/2566]	Time 0.330 (0.333)	Loss 2.04 (1.82)	mAP 52.60 (45.49)
Train: [400/2566]	Time 0.332 (0.333)	Loss 1.83 (1.81)	mAP 53.83 (45.38)
Train: [500/2566]	Time 0.330 (0.333)	Loss 1.79 (1.81)	mAP 52.21 (45.26)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.60 (1.80)	mAP 44.94 (45.33)
Train: [700/2566]	Time 0.330 (0.332)	Loss 1.80 (1.80)	mAP 41.92 (45.35)
Train: [800/2566]	Time 0.333 (0.332)	Loss 1.67 (1.79)	mAP 45.06 (45.32)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.02 (1.79)	mAP 47.37 (45.18)
Train: [1000/2566]	Time 0.332 (0.332)	Loss 1.68 (1.80)	mAP 43.83 (45.25)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.46 (1.80)	mAP 46.15 (45.28)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.74 (1.79)	mAP 44.28 (45.25)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 2.13 (1.80)	mAP 50.09 (45.21)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.39 (1.80)	mAP 40.88 (45.21)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 2.32 (1.80)	mAP 44.71 (45.24)
Train: [1600/2566]	Time 0.332 (0.331)	Loss 1.39 (1.80)	mAP 40.42 (45.18)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.64 (1.79)	mAP 38.15 (45.16)
Train: [1800/2566]	Time 0.332 (0.331)	Loss 2.28 (1.79)	mAP 42.92 (45.13)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.23 (1.79)	mAP 39.34 (45.08)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.62 (1.79)	mAP 41.00 (45.09)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.55 (1.79)	mAP 43.28 (45.06)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.55 (1.79)	mAP 46.40 (45.07)
Train: [2300/2566]	Time 0.332 (0.331)	Loss 1.67 (1.79)	mAP 40.42 (45.11)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.63 (1.79)	mAP 50.53 (45.12)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.11 (1.79)	mAP 42.38 (45.15)
Train: [24/51]	Time 0.331	Loss 1.79 	mAP 45.16
Test: [0/402]	Time 1.857 (1.857)	Precision 76.57 (76.57)	Recall 79.83 (79.83) 	 P_C 30.88 	 R_C 33.80 	 F_C 30.68 	 P_O 76.57 	 R_O 79.83 	 F_O 78.16
Test: [100/402]	Time 0.500 (0.513)	Precision 87.04 (76.75)	Recall 92.16 (83.68) 	 P_C 65.64 	 R_C 68.44 	 F_C 65.52 	 P_O 75.58 	 R_O 82.18 	 F_O 78.74
Test: [200/402]	Time 0.499 (0.507)	Precision 80.40 (77.39)	Recall 87.01 (84.15) 	 P_C 68.09 	 R_C 71.59 	 F_C 69.08 	 P_O 76.58 	 R_O 82.99 	 F_O 79.66
Test: [300/402]	Time 0.499 (0.504)	Precision 80.63 (77.60)	Recall 82.84 (84.51) 	 P_C 73.22 	 R_C 77.24 	 F_C 74.53 	 P_O 76.65 	 R_O 83.33 	 F_O 79.85
Test: [400/402]	Time 0.499 (0.503)	Precision 57.92 (76.90)	Recall 68.02 (84.34) 	 P_C 75.07 	 R_C 78.60 	 F_C 76.34 	 P_O 76.13 	 R_O 83.39 	 F_O 79.59
Test: [24/51]	  P_C 75.06 	 R_C 78.60 	 F_C 76.34 	 P_O 76.13 	 R_O 83.38 	 F_O 79.59 	 mAP 83.11
Train: [0/2566]	Time 1.053 (1.053)	Loss 2.24 (2.24)	mAP 40.62 (40.62)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.64 (1.78)	mAP 40.12 (44.38)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.79 (1.78)	mAP 42.49 (44.73)
Train: [300/2566]	Time 0.332 (0.333)	Loss 2.34 (1.79)	mAP 47.04 (44.74)
Train: [400/2566]	Time 0.332 (0.333)	Loss 1.50 (1.79)	mAP 42.30 (44.98)
Train: [500/2566]	Time 0.330 (0.332)	Loss 2.28 (1.79)	mAP 49.65 (45.06)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.58 (1.79)	mAP 47.89 (45.12)
Train: [700/2566]	Time 0.330 (0.332)	Loss 1.60 (1.78)	mAP 53.34 (45.11)
Train: [800/2566]	Time 0.331 (0.332)	Loss 1.99 (1.78)	mAP 46.30 (45.05)
Train: [900/2566]	Time 0.331 (0.332)	Loss 1.47 (1.78)	mAP 39.73 (45.08)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.98 (1.78)	mAP 46.71 (45.14)
Train: [1100/2566]	Time 0.332 (0.331)	Loss 1.74 (1.78)	mAP 38.68 (45.10)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.54 (1.79)	mAP 47.59 (45.18)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 2.07 (1.79)	mAP 41.37 (45.24)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.81 (1.79)	mAP 43.90 (45.26)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.57 (1.79)	mAP 44.53 (45.24)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.40 (1.78)	mAP 44.44 (45.26)
Train: [1700/2566]	Time 0.332 (0.331)	Loss 1.92 (1.79)	mAP 51.26 (45.27)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.77 (1.79)	mAP 49.44 (45.27)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.71 (1.79)	mAP 43.54 (45.29)
Train: [2000/2566]	Time 0.332 (0.331)	Loss 1.55 (1.79)	mAP 39.96 (45.28)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.20 (1.79)	mAP 51.72 (45.27)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.74 (1.79)	mAP 39.27 (45.22)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.91 (1.79)	mAP 51.05 (45.22)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.94 (1.79)	mAP 47.11 (45.17)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 1.91 (1.79)	mAP 49.11 (45.16)
Train: [25/51]	Time 0.331	Loss 1.79 	mAP 45.16
Test: [0/402]	Time 1.820 (1.820)	Precision 75.54 (75.54)	Recall 79.83 (79.83) 	 P_C 28.76 	 R_C 31.52 	 F_C 28.74 	 P_O 75.54 	 R_O 79.83 	 F_O 77.62
Test: [100/402]	Time 0.499 (0.513)	Precision 83.85 (76.12)	Recall 88.24 (84.11) 	 P_C 66.12 	 R_C 69.81 	 F_C 65.56 	 P_O 74.25 	 R_O 82.96 	 F_O 78.36
Test: [200/402]	Time 0.500 (0.506)	Precision 81.27 (77.63)	Recall 88.31 (84.27) 	 P_C 68.24 	 R_C 72.86 	 F_C 69.39 	 P_O 76.16 	 R_O 83.39 	 F_O 79.61
Test: [300/402]	Time 0.499 (0.504)	Precision 83.92 (77.68)	Recall 83.08 (84.90) 	 P_C 72.67 	 R_C 78.49 	 F_C 74.70 	 P_O 76.19 	 R_O 83.95 	 F_O 79.88
Test: [400/402]	Time 0.500 (0.503)	Precision 53.92 (76.75)	Recall 68.02 (84.88) 	 P_C 74.33 	 R_C 79.94 	 F_C 76.30 	 P_O 75.43 	 R_O 84.17 	 F_O 79.56
Test: [25/51]	  P_C 74.33 	 R_C 79.94 	 F_C 76.30 	 P_O 75.43 	 R_O 84.17 	 F_O 79.56 	 mAP 83.40
Train: [0/2566]	Time 1.023 (1.023)	Loss 2.20 (2.20)	mAP 38.66 (38.66)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.66 (1.76)	mAP 45.22 (45.49)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.52 (1.77)	mAP 45.46 (45.54)
Train: [300/2566]	Time 0.330 (0.333)	Loss 2.37 (1.79)	mAP 40.81 (45.51)
Train: [400/2566]	Time 0.330 (0.332)	Loss 2.15 (1.79)	mAP 54.24 (45.24)
Train: [500/2566]	Time 0.331 (0.332)	Loss 2.31 (1.79)	mAP 54.54 (45.30)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.53 (1.79)	mAP 45.05 (45.21)
Train: [700/2566]	Time 0.330 (0.332)	Loss 2.69 (1.79)	mAP 43.79 (45.10)
Train: [800/2566]	Time 0.331 (0.331)	Loss 1.36 (1.79)	mAP 32.84 (45.13)
Train: [900/2566]	Time 0.332 (0.331)	Loss 2.13 (1.79)	mAP 47.28 (45.18)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.62 (1.79)	mAP 45.91 (45.23)
Train: [1100/2566]	Time 0.332 (0.331)	Loss 1.85 (1.79)	mAP 40.05 (45.20)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.91 (1.79)	mAP 42.29 (45.16)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.94 (1.78)	mAP 44.79 (45.17)
Train: [1400/2566]	Time 0.332 (0.331)	Loss 1.49 (1.78)	mAP 47.60 (45.16)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.39 (1.78)	mAP 44.30 (45.15)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 2.23 (1.78)	mAP 50.10 (45.19)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.63 (1.78)	mAP 44.39 (45.22)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.95 (1.78)	mAP 49.90 (45.23)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.20 (1.78)	mAP 38.20 (45.20)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.70 (1.78)	mAP 44.65 (45.21)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.37 (1.78)	mAP 50.33 (45.19)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.55 (1.78)	mAP 42.58 (45.17)
Train: [2300/2566]	Time 0.332 (0.331)	Loss 2.50 (1.78)	mAP 51.49 (45.18)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.91 (1.78)	mAP 36.19 (45.20)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.45 (1.78)	mAP 42.97 (45.22)
Train: [26/51]	Time 0.331	Loss 1.79 	mAP 45.22
Test: [0/402]	Time 1.771 (1.771)	Precision 74.67 (74.67)	Recall 80.40 (80.40) 	 P_C 29.77 	 R_C 33.14 	 F_C 29.95 	 P_O 74.67 	 R_O 80.40 	 F_O 77.43
Test: [100/402]	Time 0.500 (0.512)	Precision 88.61 (75.75)	Recall 91.50 (84.54) 	 P_C 66.59 	 R_C 70.08 	 F_C 66.70 	 P_O 74.28 	 R_O 83.34 	 F_O 78.55
Test: [200/402]	Time 0.500 (0.506)	Precision 80.80 (76.67)	Recall 87.45 (84.75) 	 P_C 69.23 	 R_C 72.60 	 F_C 69.81 	 P_O 75.60 	 R_O 83.84 	 F_O 79.51
Test: [300/402]	Time 0.500 (0.504)	Precision 83.59 (77.46)	Recall 82.34 (84.88) 	 P_C 73.50 	 R_C 78.05 	 F_C 74.94 	 P_O 76.27 	 R_O 83.93 	 F_O 79.91
Test: [400/402]	Time 0.500 (0.503)	Precision 56.25 (76.80)	Recall 68.02 (84.72) 	 P_C 75.09 	 R_C 79.29 	 F_C 76.61 	 P_O 75.78 	 R_O 83.96 	 F_O 79.66
Test: [26/51]	  P_C 75.10 	 R_C 79.29 	 F_C 76.61 	 P_O 75.78 	 R_O 83.95 	 F_O 79.66 	 mAP 83.43
Train: [0/2566]	Time 1.032 (1.032)	Loss 1.99 (1.99)	mAP 44.33 (44.33)
Train: [100/2566]	Time 0.330 (0.338)	Loss 1.86 (1.68)	mAP 46.73 (44.09)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.24 (1.72)	mAP 46.62 (45.03)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.28 (1.74)	mAP 46.61 (45.09)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.99 (1.76)	mAP 44.23 (45.38)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.64 (1.77)	mAP 44.20 (45.42)
Train: [600/2566]	Time 0.332 (0.332)	Loss 1.74 (1.76)	mAP 51.69 (45.31)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.15 (1.76)	mAP 44.58 (45.30)
Train: [800/2566]	Time 0.331 (0.332)	Loss 1.44 (1.76)	mAP 48.82 (45.28)
Train: [900/2566]	Time 0.332 (0.332)	Loss 2.03 (1.76)	mAP 49.28 (45.33)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.85 (1.76)	mAP 45.91 (45.37)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 1.89 (1.76)	mAP 40.84 (45.27)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 2.02 (1.76)	mAP 48.64 (45.20)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 2.17 (1.76)	mAP 50.99 (45.21)
Train: [1400/2566]	Time 0.330 (0.332)	Loss 1.89 (1.77)	mAP 50.58 (45.15)
Train: [1500/2566]	Time 0.331 (0.332)	Loss 1.65 (1.77)	mAP 40.30 (45.16)
Train: [1600/2566]	Time 0.330 (0.332)	Loss 1.65 (1.77)	mAP 38.81 (45.13)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 2.24 (1.77)	mAP 44.56 (45.15)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.91 (1.77)	mAP 48.55 (45.17)
Train: [1900/2566]	Time 0.332 (0.331)	Loss 1.79 (1.77)	mAP 37.56 (45.19)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.82 (1.77)	mAP 43.54 (45.19)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 2.44 (1.77)	mAP 46.33 (45.19)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.91 (1.77)	mAP 44.82 (45.20)
Train: [2300/2566]	Time 0.332 (0.331)	Loss 1.67 (1.77)	mAP 41.79 (45.21)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.43 (1.77)	mAP 47.62 (45.20)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.66 (1.77)	mAP 42.63 (45.23)
Train: [27/51]	Time 0.331	Loss 1.77 	mAP 45.25
Test: [0/402]	Time 1.782 (1.782)	Precision 79.82 (79.82)	Recall 77.56 (77.56) 	 P_C 27.84 	 R_C 28.47 	 F_C 27.32 	 P_O 79.82 	 R_O 77.56 	 F_O 78.67
Test: [100/402]	Time 0.500 (0.512)	Precision 87.58 (80.58)	Recall 87.58 (81.92) 	 P_C 69.08 	 R_C 66.77 	 F_C 66.55 	 P_O 79.14 	 R_O 80.51 	 F_O 79.82
Test: [200/402]	Time 0.500 (0.506)	Precision 86.34 (81.66)	Recall 84.85 (82.24) 	 P_C 71.63 	 R_C 69.76 	 F_C 70.15 	 P_O 80.54 	 R_O 81.14 	 F_O 80.84
Test: [300/402]	Time 0.501 (0.504)	Precision 84.75 (81.66)	Recall 81.59 (82.71) 	 P_C 76.20 	 R_C 75.77 	 F_C 75.45 	 P_O 80.49 	 R_O 81.58 	 F_O 81.03
Test: [400/402]	Time 0.500 (0.503)	Precision 64.29 (80.67)	Recall 62.79 (82.58) 	 P_C 77.78 	 R_C 77.13 	 F_C 77.05 	 P_O 79.63 	 R_O 81.70 	 F_O 80.65
Test: [27/51]	  P_C 77.79 	 R_C 77.13 	 F_C 77.05 	 P_O 79.63 	 R_O 81.69 	 F_O 80.65 	 mAP 83.34
Train: [0/2566]	Time 1.012 (1.012)	Loss 1.50 (1.50)	mAP 35.31 (35.31)
Train: [100/2566]	Time 0.330 (0.338)	Loss 1.76 (1.72)	mAP 46.34 (45.84)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.63 (1.75)	mAP 46.61 (45.42)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.56 (1.76)	mAP 38.82 (45.32)
Train: [400/2566]	Time 0.330 (0.333)	Loss 2.57 (1.75)	mAP 44.91 (45.13)
Train: [500/2566]	Time 0.332 (0.332)	Loss 2.11 (1.75)	mAP 46.24 (45.17)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.02 (1.76)	mAP 44.92 (45.09)
Train: [700/2566]	Time 0.330 (0.332)	Loss 1.76 (1.76)	mAP 56.64 (45.13)
Train: [800/2566]	Time 0.333 (0.332)	Loss 1.51 (1.76)	mAP 43.15 (45.24)
Train: [900/2566]	Time 0.332 (0.331)	Loss 1.45 (1.76)	mAP 40.66 (45.26)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 2.18 (1.76)	mAP 39.50 (45.27)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.69 (1.76)	mAP 44.50 (45.26)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 2.15 (1.76)	mAP 46.05 (45.23)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.77 (1.76)	mAP 42.64 (45.24)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.51 (1.77)	mAP 41.24 (45.23)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.86 (1.77)	mAP 45.46 (45.22)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 2.04 (1.77)	mAP 43.38 (45.25)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.54 (1.77)	mAP 44.26 (45.23)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.81 (1.76)	mAP 43.23 (45.24)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.40 (1.77)	mAP 45.86 (45.22)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.64 (1.77)	mAP 49.81 (45.27)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.90 (1.77)	mAP 50.42 (45.26)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.78 (1.77)	mAP 44.57 (45.28)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.16 (1.77)	mAP 44.62 (45.25)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.51 (1.77)	mAP 47.97 (45.27)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.99 (1.77)	mAP 41.00 (45.27)
Train: [28/51]	Time 0.331	Loss 1.77 	mAP 45.26
Test: [0/402]	Time 1.808 (1.808)	Precision 80.71 (80.71)	Recall 77.27 (77.27) 	 P_C 28.49 	 R_C 28.57 	 F_C 27.71 	 P_O 80.71 	 R_O 77.27 	 F_O 78.96
Test: [100/402]	Time 0.500 (0.513)	Precision 91.67 (81.51)	Recall 86.27 (81.59) 	 P_C 70.59 	 R_C 66.89 	 F_C 66.95 	 P_O 79.98 	 R_O 80.15 	 F_O 80.06
Test: [200/402]	Time 0.500 (0.506)	Precision 88.65 (82.54)	Recall 87.88 (81.89) 	 P_C 72.60 	 R_C 69.75 	 F_C 70.23 	 P_O 81.45 	 R_O 80.81 	 F_O 81.13
Test: [300/402]	Time 0.499 (0.504)	Precision 87.16 (82.20)	Recall 79.35 (82.61) 	 P_C 76.79 	 R_C 75.82 	 F_C 75.62 	 P_O 80.99 	 R_O 81.44 	 F_O 81.22
Test: [400/402]	Time 0.500 (0.503)	Precision 58.51 (81.57)	Recall 63.95 (82.33) 	 P_C 78.11 	 R_C 77.37 	 F_C 77.21 	 P_O 80.54 	 R_O 81.39 	 F_O 80.96
Test: [28/51]	  P_C 78.12 	 R_C 77.37 	 F_C 77.21 	 P_O 80.53 	 R_O 81.39 	 F_O 80.96 	 mAP 83.62
Train: [0/2566]	Time 1.035 (1.035)	Loss 1.72 (1.72)	mAP 45.50 (45.50)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.82 (1.80)	mAP 48.45 (45.59)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.66 (1.78)	mAP 43.01 (45.63)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.98 (1.77)	mAP 46.42 (45.48)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.41 (1.77)	mAP 41.11 (45.50)
Train: [500/2566]	Time 0.332 (0.332)	Loss 1.81 (1.78)	mAP 42.43 (45.35)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.73 (1.78)	mAP 46.36 (45.29)
Train: [700/2566]	Time 0.330 (0.332)	Loss 1.94 (1.77)	mAP 45.97 (45.24)
Train: [800/2566]	Time 0.330 (0.332)	Loss 1.82 (1.78)	mAP 46.67 (45.26)
Train: [900/2566]	Time 0.332 (0.332)	Loss 2.07 (1.77)	mAP 42.57 (45.27)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.89 (1.77)	mAP 46.78 (45.29)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.58 (1.77)	mAP 40.07 (45.26)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.29 (1.77)	mAP 50.84 (45.28)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 2.41 (1.77)	mAP 44.11 (45.24)
Train: [1400/2566]	Time 0.333 (0.331)	Loss 2.08 (1.77)	mAP 49.03 (45.21)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.42 (1.77)	mAP 46.25 (45.25)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.67 (1.77)	mAP 34.73 (45.26)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.86 (1.77)	mAP 41.45 (45.25)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.94 (1.77)	mAP 48.81 (45.27)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.17 (1.77)	mAP 46.51 (45.24)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 2.14 (1.77)	mAP 47.85 (45.22)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.90 (1.77)	mAP 40.04 (45.22)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.64 (1.77)	mAP 40.34 (45.23)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.96 (1.77)	mAP 41.45 (45.28)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.60 (1.77)	mAP 49.40 (45.29)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 1.42 (1.77)	mAP 50.31 (45.27)
Train: [29/51]	Time 0.331	Loss 1.77 	mAP 45.27
Test: [0/402]	Time 1.827 (1.827)	Precision 74.41 (74.41)	Recall 80.11 (80.11) 	 P_C 27.62 	 R_C 31.54 	 F_C 27.96 	 P_O 74.41 	 R_O 80.11 	 F_O 77.15
Test: [100/402]	Time 0.499 (0.513)	Precision 84.38 (75.16)	Recall 88.24 (85.14) 	 P_C 64.71 	 R_C 70.66 	 F_C 66.01 	 P_O 73.75 	 R_O 83.93 	 F_O 78.51
Test: [200/402]	Time 0.500 (0.506)	Precision 79.77 (75.63)	Recall 88.74 (85.42) 	 P_C 66.91 	 R_C 73.63 	 F_C 69.17 	 P_O 74.60 	 R_O 84.48 	 F_O 79.23
Test: [300/402]	Time 0.499 (0.504)	Precision 79.91 (76.05)	Recall 86.07 (85.74) 	 P_C 72.38 	 R_C 78.96 	 F_C 74.76 	 P_O 74.97 	 R_O 84.74 	 F_O 79.55
Test: [400/402]	Time 0.500 (0.503)	Precision 57.07 (75.15)	Recall 68.02 (85.68) 	 P_C 74.43 	 R_C 80.39 	 F_C 76.45 	 P_O 74.27 	 R_O 84.90 	 F_O 79.23
Test: [29/51]	  P_C 74.43 	 R_C 80.39 	 F_C 76.45 	 P_O 74.27 	 R_O 84.90 	 F_O 79.23 	 mAP 83.60
Train: [0/2566]	Time 1.005 (1.005)	Loss 1.39 (1.39)	mAP 36.48 (36.48)
Train: [100/2566]	Time 0.330 (0.338)	Loss 1.83 (1.76)	mAP 58.20 (45.14)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.86 (1.76)	mAP 45.33 (45.06)
Train: [300/2566]	Time 0.332 (0.333)	Loss 2.13 (1.76)	mAP 44.41 (45.07)
Train: [400/2566]	Time 0.331 (0.333)	Loss 2.23 (1.76)	mAP 45.80 (45.01)
Train: [500/2566]	Time 0.331 (0.333)	Loss 1.80 (1.77)	mAP 35.82 (45.05)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.98 (1.77)	mAP 52.66 (45.18)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.83 (1.76)	mAP 48.95 (45.15)
Train: [800/2566]	Time 0.330 (0.332)	Loss 1.68 (1.76)	mAP 42.18 (45.22)
Train: [900/2566]	Time 0.332 (0.332)	Loss 1.63 (1.76)	mAP 41.22 (45.26)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.75 (1.76)	mAP 42.23 (45.25)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 1.66 (1.76)	mAP 46.74 (45.21)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 1.76 (1.75)	mAP 44.71 (45.21)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.77 (1.76)	mAP 50.28 (45.22)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.70 (1.76)	mAP 49.86 (45.21)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.93 (1.76)	mAP 48.68 (45.23)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.68 (1.76)	mAP 44.50 (45.22)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.63 (1.76)	mAP 43.46 (45.24)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.82 (1.75)	mAP 46.20 (45.24)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.86 (1.75)	mAP 36.64 (45.29)
Train: [2000/2566]	Time 0.332 (0.331)	Loss 1.98 (1.75)	mAP 45.45 (45.30)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.84 (1.75)	mAP 49.43 (45.31)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.91 (1.75)	mAP 47.76 (45.31)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.50 (1.76)	mAP 43.58 (45.31)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.69 (1.76)	mAP 44.71 (45.32)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.74 (1.75)	mAP 52.37 (45.34)
Train: [30/51]	Time 0.331	Loss 1.75 	mAP 45.32
Test: [0/402]	Time 1.773 (1.773)	Precision 72.11 (72.11)	Recall 81.53 (81.53) 	 P_C 29.54 	 R_C 34.66 	 F_C 30.27 	 P_O 72.11 	 R_O 81.53 	 F_O 76.53
Test: [100/402]	Time 0.500 (0.512)	Precision 82.82 (74.46)	Recall 88.24 (85.74) 	 P_C 63.14 	 R_C 72.15 	 F_C 65.65 	 P_O 72.85 	 R_O 84.68 	 F_O 78.32
Test: [200/402]	Time 0.500 (0.506)	Precision 81.27 (75.25)	Recall 88.31 (86.04) 	 P_C 65.49 	 R_C 74.77 	 F_C 69.04 	 P_O 74.04 	 R_O 85.23 	 F_O 79.25
Test: [300/402]	Time 0.500 (0.504)	Precision 80.71 (75.44)	Recall 85.32 (86.53) 	 P_C 71.07 	 R_C 79.66 	 F_C 74.47 	 P_O 74.15 	 R_O 85.67 	 F_O 79.49
Test: [400/402]	Time 0.499 (0.503)	Precision 54.50 (74.60)	Recall 70.35 (86.41) 	 P_C 72.90 	 R_C 81.06 	 F_C 76.25 	 P_O 73.51 	 R_O 85.75 	 F_O 79.16
Test: [30/51]	  P_C 72.90 	 R_C 81.05 	 F_C 76.25 	 P_O 73.51 	 R_O 85.74 	 F_O 79.15 	 mAP 83.76
Train: [0/2566]	Time 1.005 (1.005)	Loss 2.33 (2.33)	mAP 45.48 (45.48)
Train: [100/2566]	Time 0.330 (0.338)	Loss 1.61 (1.81)	mAP 45.04 (45.27)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.85 (1.80)	mAP 41.22 (45.35)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.60 (1.80)	mAP 55.35 (45.28)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.93 (1.79)	mAP 42.08 (45.06)
Train: [500/2566]	Time 0.332 (0.332)	Loss 1.73 (1.78)	mAP 34.39 (45.20)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.62 (1.78)	mAP 44.33 (45.15)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.73 (1.77)	mAP 46.25 (45.16)
Train: [800/2566]	Time 0.332 (0.331)	Loss 1.88 (1.77)	mAP 51.64 (45.17)
Train: [900/2566]	Time 0.331 (0.331)	Loss 1.70 (1.77)	mAP 44.91 (45.18)
Train: [1000/2566]	Time 0.332 (0.331)	Loss 1.61 (1.76)	mAP 46.62 (45.19)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 1.83 (1.76)	mAP 47.57 (45.16)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.68 (1.76)	mAP 40.37 (45.19)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.78 (1.76)	mAP 41.29 (45.22)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.77 (1.76)	mAP 46.59 (45.31)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 2.12 (1.75)	mAP 52.50 (45.30)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.60 (1.75)	mAP 43.35 (45.28)
Train: [1700/2566]	Time 0.332 (0.331)	Loss 2.07 (1.75)	mAP 44.15 (45.27)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.14 (1.75)	mAP 51.97 (45.33)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.30 (1.75)	mAP 36.91 (45.30)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.35 (1.75)	mAP 41.66 (45.30)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.56 (1.75)	mAP 43.73 (45.30)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.15 (1.75)	mAP 46.43 (45.30)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.69 (1.75)	mAP 48.56 (45.33)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.67 (1.75)	mAP 46.94 (45.36)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.84 (1.75)	mAP 49.86 (45.37)
Train: [31/51]	Time 0.331	Loss 1.75 	mAP 45.34
Test: [0/402]	Time 1.814 (1.814)	Precision 78.49 (78.49)	Recall 79.83 (79.83) 	 P_C 31.08 	 R_C 31.96 	 F_C 30.23 	 P_O 78.49 	 R_O 79.83 	 F_O 79.15
Test: [100/402]	Time 0.500 (0.513)	Precision 89.93 (78.87)	Recall 87.58 (83.33) 	 P_C 68.60 	 R_C 69.63 	 F_C 67.17 	 P_O 76.95 	 R_O 82.21 	 F_O 79.49
Test: [200/402]	Time 0.499 (0.506)	Precision 83.82 (80.14)	Recall 87.45 (83.40) 	 P_C 70.75 	 R_C 72.32 	 F_C 70.56 	 P_O 78.72 	 R_O 82.53 	 F_O 80.58
Test: [300/402]	Time 0.500 (0.504)	Precision 83.04 (79.16)	Recall 81.59 (84.51) 	 P_C 74.43 	 R_C 78.03 	 F_C 75.34 	 P_O 77.31 	 R_O 83.69 	 F_O 80.37
Test: [400/402]	Time 0.499 (0.503)	Precision 55.81 (78.06)	Recall 69.77 (84.47) 	 P_C 77.05 	 R_C 79.50 	 F_C 76.90 	 P_O 76.34 	 R_O 83.88 	 F_O 79.93
Test: [31/51]	  P_C 77.05 	 R_C 79.50 	 F_C 76.90 	 P_O 76.34 	 R_O 83.87 	 F_O 79.93 	 mAP 83.80
Train: [0/2566]	Time 1.003 (1.003)	Loss 1.56 (1.56)	mAP 52.42 (52.42)
Train: [100/2566]	Time 0.331 (0.337)	Loss 1.78 (1.75)	mAP 43.87 (45.26)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.98 (1.74)	mAP 50.84 (45.42)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.75 (1.75)	mAP 41.38 (45.38)
Train: [400/2566]	Time 0.331 (0.333)	Loss 1.60 (1.75)	mAP 45.74 (45.22)
Train: [500/2566]	Time 0.332 (0.332)	Loss 2.02 (1.75)	mAP 45.51 (45.34)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.99 (1.74)	mAP 45.05 (45.37)
Train: [700/2566]	Time 0.332 (0.332)	Loss 1.61 (1.74)	mAP 48.47 (45.44)
Train: [800/2566]	Time 0.331 (0.332)	Loss 1.81 (1.74)	mAP 42.61 (45.47)
Train: [900/2566]	Time 0.331 (0.332)	Loss 1.33 (1.74)	mAP 43.38 (45.43)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.94 (1.74)	mAP 45.04 (45.42)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 2.36 (1.75)	mAP 47.39 (45.44)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.83 (1.75)	mAP 44.53 (45.44)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.78 (1.75)	mAP 48.17 (45.45)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.64 (1.74)	mAP 48.53 (45.39)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.81 (1.74)	mAP 45.13 (45.39)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 2.05 (1.75)	mAP 47.25 (45.37)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.65 (1.75)	mAP 46.32 (45.40)
Train: [1800/2566]	Time 0.332 (0.331)	Loss 1.70 (1.75)	mAP 50.17 (45.44)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.68 (1.75)	mAP 41.43 (45.42)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.31 (1.75)	mAP 49.21 (45.38)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 2.00 (1.75)	mAP 48.78 (45.37)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.81 (1.75)	mAP 46.15 (45.35)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.85 (1.75)	mAP 51.92 (45.34)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.00 (1.75)	mAP 40.29 (45.30)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.66 (1.75)	mAP 46.59 (45.28)
Train: [32/51]	Time 0.331	Loss 1.75 	mAP 45.28
Test: [0/402]	Time 1.771 (1.771)	Precision 76.09 (76.09)	Recall 79.55 (79.55) 	 P_C 29.76 	 R_C 32.06 	 F_C 29.68 	 P_O 76.09 	 R_O 79.55 	 F_O 77.78
Test: [100/402]	Time 0.500 (0.512)	Precision 87.66 (78.03)	Recall 88.24 (83.64) 	 P_C 66.23 	 R_C 69.32 	 F_C 66.45 	 P_O 76.73 	 R_O 82.30 	 F_O 79.41
Test: [200/402]	Time 0.500 (0.506)	Precision 82.11 (78.90)	Recall 87.45 (84.13) 	 P_C 68.74 	 R_C 72.32 	 F_C 69.83 	 P_O 77.89 	 R_O 83.10 	 F_O 80.41
Test: [300/402]	Time 0.500 (0.504)	Precision 83.95 (78.97)	Recall 84.58 (84.64) 	 P_C 73.28 	 R_C 78.21 	 F_C 75.10 	 P_O 77.89 	 R_O 83.58 	 F_O 80.63
Test: [400/402]	Time 0.500 (0.503)	Precision 58.62 (78.18)	Recall 69.19 (84.48) 	 P_C 75.01 	 R_C 79.56 	 F_C 76.91 	 P_O 77.33 	 R_O 83.61 	 F_O 80.35
Test: [32/51]	  P_C 75.02 	 R_C 79.56 	 F_C 76.91 	 P_O 77.33 	 R_O 83.60 	 F_O 80.34 	 mAP 83.97
Train: [0/2566]	Time 1.014 (1.014)	Loss 2.08 (2.08)	mAP 46.47 (46.47)
Train: [100/2566]	Time 0.331 (0.337)	Loss 1.65 (1.72)	mAP 45.36 (45.74)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.42 (1.74)	mAP 39.07 (45.83)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.90 (1.73)	mAP 43.82 (45.96)
Train: [400/2566]	Time 0.332 (0.332)	Loss 1.21 (1.74)	mAP 47.01 (45.92)
Train: [500/2566]	Time 0.330 (0.332)	Loss 2.07 (1.75)	mAP 53.73 (45.78)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.35 (1.75)	mAP 44.80 (45.70)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.47 (1.75)	mAP 38.49 (45.61)
Train: [800/2566]	Time 0.332 (0.332)	Loss 1.33 (1.75)	mAP 45.71 (45.61)
Train: [900/2566]	Time 0.330 (0.332)	Loss 1.68 (1.75)	mAP 52.71 (45.55)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.57 (1.74)	mAP 46.13 (45.49)
Train: [1100/2566]	Time 0.332 (0.331)	Loss 1.62 (1.75)	mAP 51.86 (45.44)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.76 (1.74)	mAP 44.15 (45.40)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 2.00 (1.74)	mAP 43.97 (45.44)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.23 (1.74)	mAP 45.00 (45.46)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.72 (1.74)	mAP 46.74 (45.43)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.77 (1.74)	mAP 47.82 (45.43)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.41 (1.74)	mAP 48.21 (45.39)
Train: [1800/2566]	Time 0.329 (0.331)	Loss 1.37 (1.74)	mAP 49.35 (45.34)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.21 (1.74)	mAP 47.00 (45.31)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.61 (1.74)	mAP 45.05 (45.31)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.44 (1.74)	mAP 46.65 (45.32)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.19 (1.74)	mAP 42.32 (45.34)
Train: [2300/2566]	Time 0.329 (0.331)	Loss 1.71 (1.74)	mAP 43.87 (45.34)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.24 (1.74)	mAP 47.11 (45.32)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.28 (1.74)	mAP 36.59 (45.29)
Train: [33/51]	Time 0.331	Loss 1.74 	mAP 45.27
Test: [0/402]	Time 1.794 (1.794)	Precision 76.45 (76.45)	Recall 78.41 (78.41) 	 P_C 29.00 	 R_C 30.48 	 F_C 28.64 	 P_O 76.45 	 R_O 78.41 	 F_O 77.42
Test: [100/402]	Time 0.500 (0.512)	Precision 88.82 (79.64)	Recall 88.24 (82.74) 	 P_C 68.15 	 R_C 68.63 	 F_C 66.81 	 P_O 77.97 	 R_O 81.46 	 F_O 79.68
Test: [200/402]	Time 0.501 (0.506)	Precision 83.06 (79.89)	Recall 87.01 (83.54) 	 P_C 69.76 	 R_C 71.66 	 F_C 69.76 	 P_O 78.66 	 R_O 82.55 	 F_O 80.56
Test: [300/402]	Time 0.500 (0.504)	Precision 81.49 (79.70)	Recall 84.33 (84.34) 	 P_C 74.47 	 R_C 77.92 	 F_C 75.42 	 P_O 78.16 	 R_O 83.43 	 F_O 80.71
Test: [400/402]	Time 0.500 (0.503)	Precision 58.76 (78.48)	Recall 66.28 (84.42) 	 P_C 76.17 	 R_C 79.34 	 F_C 77.09 	 P_O 77.12 	 R_O 83.72 	 F_O 80.29
Test: [33/51]	  P_C 76.17 	 R_C 79.34 	 F_C 77.09 	 P_O 77.12 	 R_O 83.72 	 F_O 80.28 	 mAP 83.91
Train: [0/2566]	Time 1.005 (1.005)	Loss 1.74 (1.74)	mAP 45.92 (45.92)
Train: [100/2566]	Time 0.330 (0.338)	Loss 1.47 (1.75)	mAP 40.18 (45.76)
Train: [200/2566]	Time 0.331 (0.334)	Loss 2.11 (1.75)	mAP 44.50 (45.51)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.27 (1.75)	mAP 41.50 (45.16)
Train: [400/2566]	Time 0.330 (0.333)	Loss 1.52 (1.74)	mAP 47.82 (45.18)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.67 (1.74)	mAP 48.59 (45.14)
Train: [600/2566]	Time 0.332 (0.332)	Loss 1.64 (1.74)	mAP 51.21 (45.20)
Train: [700/2566]	Time 0.331 (0.332)	Loss 2.24 (1.74)	mAP 39.57 (45.13)
Train: [800/2566]	Time 0.331 (0.332)	Loss 2.34 (1.74)	mAP 49.81 (45.18)
Train: [900/2566]	Time 0.330 (0.332)	Loss 1.41 (1.74)	mAP 44.81 (45.23)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.75 (1.74)	mAP 42.78 (45.27)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 1.88 (1.74)	mAP 43.22 (45.31)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 1.37 (1.74)	mAP 43.65 (45.30)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.98 (1.74)	mAP 45.48 (45.32)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.33 (1.74)	mAP 44.83 (45.25)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 2.00 (1.74)	mAP 54.87 (45.23)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.96 (1.74)	mAP 49.16 (45.23)
Train: [1700/2566]	Time 0.329 (0.331)	Loss 2.07 (1.74)	mAP 41.71 (45.24)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.76 (1.74)	mAP 47.93 (45.24)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.78 (1.74)	mAP 50.35 (45.24)
Train: [2000/2566]	Time 0.332 (0.331)	Loss 1.48 (1.74)	mAP 45.07 (45.26)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 1.63 (1.74)	mAP 38.21 (45.24)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.02 (1.74)	mAP 52.34 (45.29)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.70 (1.74)	mAP 42.33 (45.31)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.65 (1.74)	mAP 51.34 (45.29)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.86 (1.74)	mAP 39.68 (45.30)
Train: [34/51]	Time 0.331	Loss 1.74 	mAP 45.30
Test: [0/402]	Time 1.771 (1.771)	Precision 75.89 (75.89)	Recall 78.69 (78.69) 	 P_C 31.15 	 R_C 32.76 	 F_C 30.08 	 P_O 75.89 	 R_O 78.69 	 F_O 77.27
Test: [100/402]	Time 0.500 (0.512)	Precision 86.96 (77.41)	Recall 91.50 (84.06) 	 P_C 65.32 	 R_C 70.27 	 F_C 66.46 	 P_O 76.62 	 R_O 82.47 	 F_O 79.44
Test: [200/402]	Time 0.499 (0.506)	Precision 79.30 (78.03)	Recall 87.88 (84.58) 	 P_C 67.44 	 R_C 73.05 	 F_C 69.63 	 P_O 77.42 	 R_O 83.43 	 F_O 80.31
Test: [300/402]	Time 0.499 (0.504)	Precision 82.43 (77.65)	Recall 82.84 (85.36) 	 P_C 72.36 	 R_C 78.89 	 F_C 75.06 	 P_O 76.85 	 R_O 84.20 	 F_O 80.36
Test: [400/402]	Time 0.500 (0.503)	Precision 57.00 (76.86)	Recall 66.28 (85.24) 	 P_C 74.50 	 R_C 80.34 	 F_C 76.66 	 P_O 76.15 	 R_O 84.34 	 F_O 80.03
Test: [34/51]	  P_C 74.50 	 R_C 80.34 	 F_C 76.66 	 P_O 76.14 	 R_O 84.33 	 F_O 80.03 	 mAP 84.02
Train: [0/2566]	Time 1.021 (1.021)	Loss 1.78 (1.78)	mAP 41.44 (41.44)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.93 (1.72)	mAP 32.88 (44.89)
Train: [200/2566]	Time 0.332 (0.335)	Loss 1.56 (1.74)	mAP 42.58 (45.11)
Train: [300/2566]	Time 0.332 (0.333)	Loss 2.30 (1.75)	mAP 45.72 (45.19)
Train: [400/2566]	Time 0.330 (0.333)	Loss 1.71 (1.74)	mAP 45.48 (45.07)
Train: [500/2566]	Time 0.330 (0.333)	Loss 1.41 (1.73)	mAP 39.66 (45.09)
Train: [600/2566]	Time 0.333 (0.332)	Loss 1.92 (1.73)	mAP 48.59 (45.03)
Train: [700/2566]	Time 0.332 (0.332)	Loss 1.29 (1.72)	mAP 45.33 (45.05)
Train: [800/2566]	Time 0.330 (0.332)	Loss 1.60 (1.72)	mAP 47.43 (45.15)
Train: [900/2566]	Time 0.331 (0.332)	Loss 1.51 (1.72)	mAP 42.80 (45.17)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.78 (1.72)	mAP 39.66 (45.21)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 1.83 (1.72)	mAP 39.86 (45.25)
Train: [1200/2566]	Time 0.330 (0.332)	Loss 1.92 (1.72)	mAP 41.73 (45.27)
Train: [1300/2566]	Time 0.330 (0.332)	Loss 1.58 (1.72)	mAP 45.74 (45.32)
Train: [1400/2566]	Time 0.331 (0.332)	Loss 1.83 (1.72)	mAP 45.58 (45.35)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.63 (1.72)	mAP 37.05 (45.39)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 2.14 (1.72)	mAP 45.23 (45.41)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.45 (1.72)	mAP 40.06 (45.42)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.52 (1.72)	mAP 46.78 (45.43)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.83 (1.72)	mAP 44.40 (45.43)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.52 (1.72)	mAP 50.63 (45.44)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.39 (1.73)	mAP 43.14 (45.46)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 2.13 (1.73)	mAP 48.40 (45.44)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.25 (1.73)	mAP 45.01 (45.44)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.21 (1.73)	mAP 42.64 (45.43)
Train: [2500/2566]	Time 0.335 (0.331)	Loss 1.64 (1.73)	mAP 44.21 (45.46)
Train: [35/51]	Time 0.331	Loss 1.73 	mAP 45.46
Test: [0/402]	Time 1.764 (1.764)	Precision 77.53 (77.53)	Recall 78.41 (78.41) 	 P_C 32.61 	 R_C 31.86 	 F_C 30.74 	 P_O 77.53 	 R_O 78.41 	 F_O 77.97
Test: [100/402]	Time 0.500 (0.512)	Precision 88.82 (78.73)	Recall 88.24 (83.75) 	 P_C 67.92 	 R_C 70.22 	 F_C 66.97 	 P_O 76.24 	 R_O 82.85 	 F_O 79.41
Test: [200/402]	Time 0.499 (0.506)	Precision 83.95 (80.07)	Recall 88.31 (83.92) 	 P_C 70.17 	 R_C 73.09 	 F_C 70.48 	 P_O 78.24 	 R_O 83.20 	 F_O 80.64
Test: [300/402]	Time 0.500 (0.504)	Precision 83.29 (80.16)	Recall 83.08 (84.41) 	 P_C 74.86 	 R_C 78.81 	 F_C 75.94 	 P_O 78.38 	 R_O 83.57 	 F_O 80.89
Test: [400/402]	Time 0.500 (0.503)	Precision 60.61 (79.06)	Recall 69.77 (84.44) 	 P_C 76.80 	 R_C 80.15 	 F_C 77.67 	 P_O 77.51 	 R_O 83.81 	 F_O 80.54
Test: [35/51]	  P_C 76.80 	 R_C 80.15 	 F_C 77.67 	 P_O 77.51 	 R_O 83.80 	 F_O 80.53 	 mAP 84.01
Train: [0/2566]	Time 0.991 (0.991)	Loss 1.50 (1.50)	mAP 44.25 (44.25)
Train: [100/2566]	Time 0.331 (0.337)	Loss 2.04 (1.68)	mAP 46.38 (46.76)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.53 (1.70)	mAP 42.53 (46.03)
Train: [300/2566]	Time 0.332 (0.333)	Loss 2.26 (1.72)	mAP 41.37 (45.88)
Train: [400/2566]	Time 0.332 (0.333)	Loss 1.83 (1.71)	mAP 57.59 (45.88)
Train: [500/2566]	Time 0.331 (0.332)	Loss 2.15 (1.72)	mAP 44.03 (45.90)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.72 (1.72)	mAP 39.95 (45.90)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.57 (1.72)	mAP 46.15 (45.94)
Train: [800/2566]	Time 0.330 (0.332)	Loss 2.10 (1.72)	mAP 45.76 (45.82)
Train: [900/2566]	Time 0.330 (0.332)	Loss 1.72 (1.72)	mAP 43.96 (45.80)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 1.80 (1.72)	mAP 42.02 (45.80)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 2.26 (1.72)	mAP 47.01 (45.81)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.84 (1.72)	mAP 41.93 (45.78)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 2.16 (1.72)	mAP 45.59 (45.71)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.27 (1.73)	mAP 45.99 (45.71)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.71 (1.73)	mAP 37.98 (45.68)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.62 (1.73)	mAP 43.82 (45.67)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.92 (1.73)	mAP 48.57 (45.66)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.40 (1.72)	mAP 49.93 (45.65)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.95 (1.72)	mAP 44.50 (45.62)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.40 (1.72)	mAP 34.49 (45.62)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.58 (1.72)	mAP 48.88 (45.64)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.55 (1.72)	mAP 49.54 (45.62)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.70 (1.72)	mAP 47.04 (45.62)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.34 (1.72)	mAP 47.64 (45.61)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.72 (1.72)	mAP 44.96 (45.59)
Train: [36/51]	Time 0.331	Loss 1.72 	mAP 45.59
Test: [0/402]	Time 1.875 (1.875)	Precision 73.82 (73.82)	Recall 80.11 (80.11) 	 P_C 30.00 	 R_C 32.52 	 F_C 29.76 	 P_O 73.82 	 R_O 80.11 	 F_O 76.84
Test: [100/402]	Time 0.499 (0.513)	Precision 85.45 (76.24)	Recall 92.16 (84.92) 	 P_C 63.97 	 R_C 70.80 	 F_C 65.50 	 P_O 74.62 	 R_O 83.81 	 F_O 78.95
Test: [200/402]	Time 0.500 (0.507)	Precision 80.31 (76.79)	Recall 88.31 (85.16) 	 P_C 66.21 	 R_C 73.90 	 F_C 68.85 	 P_O 75.62 	 R_O 84.31 	 F_O 79.73
Test: [300/402]	Time 0.500 (0.504)	Precision 82.17 (76.63)	Recall 84.83 (85.95) 	 P_C 71.44 	 R_C 79.79 	 F_C 74.69 	 P_O 75.49 	 R_O 85.03 	 F_O 79.98
Test: [400/402]	Time 0.500 (0.503)	Precision 58.74 (75.81)	Recall 70.35 (85.92) 	 P_C 73.29 	 R_C 81.14 	 F_C 76.55 	 P_O 74.87 	 R_O 85.21 	 F_O 79.71
Test: [36/51]	  P_C 73.29 	 R_C 81.14 	 F_C 76.54 	 P_O 74.87 	 R_O 85.21 	 F_O 79.70 	 mAP 84.11
Train: [0/2566]	Time 1.084 (1.084)	Loss 2.29 (2.29)	mAP 46.32 (46.32)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.81 (1.69)	mAP 47.27 (45.03)
Train: [200/2566]	Time 0.332 (0.334)	Loss 2.38 (1.70)	mAP 46.92 (45.57)
Train: [300/2566]	Time 0.332 (0.333)	Loss 1.47 (1.70)	mAP 43.51 (45.76)
Train: [400/2566]	Time 0.332 (0.333)	Loss 1.66 (1.71)	mAP 49.22 (45.66)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.77 (1.71)	mAP 41.64 (45.59)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.26 (1.72)	mAP 52.28 (45.56)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.82 (1.72)	mAP 42.14 (45.48)
Train: [800/2566]	Time 0.331 (0.332)	Loss 1.47 (1.72)	mAP 52.11 (45.45)
Train: [900/2566]	Time 0.331 (0.332)	Loss 1.61 (1.72)	mAP 46.29 (45.41)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 2.01 (1.72)	mAP 55.55 (45.44)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.94 (1.72)	mAP 51.94 (45.49)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.51 (1.72)	mAP 39.71 (45.50)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.74 (1.72)	mAP 47.95 (45.50)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.60 (1.72)	mAP 41.95 (45.48)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.62 (1.72)	mAP 43.79 (45.48)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.73 (1.72)	mAP 48.38 (45.45)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.21 (1.72)	mAP 43.19 (45.45)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.47 (1.72)	mAP 49.62 (45.46)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.92 (1.72)	mAP 43.47 (45.48)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.73 (1.72)	mAP 41.79 (45.51)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.94 (1.72)	mAP 45.29 (45.48)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.77 (1.72)	mAP 38.93 (45.51)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 2.04 (1.72)	mAP 44.37 (45.50)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.82 (1.72)	mAP 40.59 (45.50)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.88 (1.72)	mAP 50.25 (45.50)
Train: [37/51]	Time 0.331	Loss 1.72 	mAP 45.51
Test: [0/402]	Time 1.814 (1.814)	Precision 73.83 (73.83)	Recall 80.97 (80.97) 	 P_C 30.90 	 R_C 33.47 	 F_C 30.61 	 P_O 73.83 	 R_O 80.97 	 F_O 77.24
Test: [100/402]	Time 0.500 (0.513)	Precision 86.96 (76.44)	Recall 91.50 (84.98) 	 P_C 65.45 	 R_C 71.75 	 F_C 66.98 	 P_O 74.97 	 R_O 83.86 	 F_O 79.17
Test: [200/402]	Time 0.499 (0.506)	Precision 80.88 (77.01)	Recall 87.88 (85.49) 	 P_C 67.64 	 R_C 74.74 	 F_C 70.22 	 P_O 75.99 	 R_O 84.63 	 F_O 80.07
Test: [300/402]	Time 0.499 (0.504)	Precision 81.77 (76.87)	Recall 84.83 (86.17) 	 P_C 72.24 	 R_C 80.07 	 F_C 75.33 	 P_O 75.57 	 R_O 85.29 	 F_O 80.14
Test: [400/402]	Time 0.499 (0.503)	Precision 56.62 (75.88)	Recall 72.09 (86.16) 	 P_C 73.94 	 R_C 81.32 	 F_C 76.88 	 P_O 74.71 	 R_O 85.49 	 F_O 79.74
Test: [37/51]	  P_C 73.95 	 R_C 81.32 	 F_C 76.88 	 P_O 74.71 	 R_O 85.48 	 F_O 79.73 	 mAP 84.23
Train: [0/2566]	Time 0.989 (0.989)	Loss 2.03 (2.03)	mAP 41.30 (41.30)
Train: [100/2566]	Time 0.330 (0.337)	Loss 2.11 (1.68)	mAP 42.89 (45.34)
Train: [200/2566]	Time 0.330 (0.334)	Loss 2.03 (1.69)	mAP 42.65 (45.13)
Train: [300/2566]	Time 0.332 (0.333)	Loss 2.38 (1.71)	mAP 46.39 (45.02)
Train: [400/2566]	Time 0.331 (0.333)	Loss 1.70 (1.69)	mAP 39.84 (45.09)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.67 (1.69)	mAP 55.48 (45.32)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.91 (1.70)	mAP 38.45 (45.15)
Train: [700/2566]	Time 0.329 (0.332)	Loss 1.64 (1.70)	mAP 45.20 (45.22)
Train: [800/2566]	Time 0.330 (0.332)	Loss 1.62 (1.70)	mAP 53.14 (45.34)
Train: [900/2566]	Time 0.331 (0.332)	Loss 1.55 (1.70)	mAP 42.96 (45.40)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.74 (1.70)	mAP 50.12 (45.34)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.89 (1.70)	mAP 44.04 (45.35)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.66 (1.70)	mAP 45.06 (45.36)
Train: [1300/2566]	Time 0.332 (0.331)	Loss 1.48 (1.70)	mAP 43.28 (45.37)
Train: [1400/2566]	Time 0.332 (0.331)	Loss 1.52 (1.70)	mAP 48.31 (45.38)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.83 (1.70)	mAP 43.07 (45.38)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.84 (1.70)	mAP 50.33 (45.35)
Train: [1700/2566]	Time 0.332 (0.331)	Loss 1.51 (1.71)	mAP 41.55 (45.34)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.36 (1.71)	mAP 48.05 (45.36)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.42 (1.71)	mAP 39.74 (45.33)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 0.95 (1.71)	mAP 38.95 (45.37)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.93 (1.71)	mAP 44.05 (45.37)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.49 (1.71)	mAP 37.54 (45.35)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.60 (1.71)	mAP 36.63 (45.36)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.03 (1.71)	mAP 38.89 (45.38)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 1.92 (1.71)	mAP 43.81 (45.36)
Train: [38/51]	Time 0.331	Loss 1.71 	mAP 45.39
Test: [0/402]	Time 1.800 (1.800)	Precision 72.45 (72.45)	Recall 80.68 (80.68) 	 P_C 29.74 	 R_C 34.16 	 F_C 30.29 	 P_O 72.45 	 R_O 80.68 	 F_O 76.34
Test: [100/402]	Time 0.500 (0.512)	Precision 82.35 (74.81)	Recall 91.50 (85.55) 	 P_C 63.28 	 R_C 72.17 	 F_C 65.70 	 P_O 73.25 	 R_O 84.58 	 F_O 78.51
Test: [200/402]	Time 0.499 (0.506)	Precision 81.85 (76.40)	Recall 87.88 (85.52) 	 P_C 65.83 	 R_C 75.19 	 F_C 69.31 	 P_O 75.13 	 R_O 84.77 	 F_O 79.66
Test: [300/402]	Time 0.500 (0.504)	Precision 81.36 (76.51)	Recall 83.58 (86.16) 	 P_C 71.39 	 R_C 80.37 	 F_C 74.88 	 P_O 75.14 	 R_O 85.35 	 F_O 79.92
Test: [400/402]	Time 0.500 (0.503)	Precision 55.30 (75.67)	Recall 69.77 (86.07) 	 P_C 73.24 	 R_C 81.59 	 F_C 76.58 	 P_O 74.50 	 R_O 85.46 	 F_O 79.60
Test: [38/51]	  P_C 73.23 	 R_C 81.59 	 F_C 76.58 	 P_O 74.49 	 R_O 85.46 	 F_O 79.60 	 mAP 84.09
Train: [0/2566]	Time 1.045 (1.045)	Loss 2.01 (2.01)	mAP 45.37 (45.37)
Train: [100/2566]	Time 0.332 (0.338)	Loss 2.27 (1.68)	mAP 45.48 (46.41)
Train: [200/2566]	Time 0.332 (0.335)	Loss 2.06 (1.70)	mAP 45.14 (45.94)
Train: [300/2566]	Time 0.332 (0.334)	Loss 1.69 (1.72)	mAP 47.26 (45.87)
Train: [400/2566]	Time 0.330 (0.333)	Loss 1.58 (1.72)	mAP 43.24 (45.75)
Train: [500/2566]	Time 0.334 (0.333)	Loss 1.65 (1.72)	mAP 44.94 (45.65)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.56 (1.71)	mAP 41.08 (45.68)
Train: [700/2566]	Time 0.330 (0.332)	Loss 1.31 (1.71)	mAP 47.94 (45.67)
Train: [800/2566]	Time 0.331 (0.332)	Loss 1.23 (1.70)	mAP 39.76 (45.69)
Train: [900/2566]	Time 0.330 (0.332)	Loss 1.72 (1.70)	mAP 51.05 (45.72)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 1.55 (1.70)	mAP 39.56 (45.67)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 1.73 (1.70)	mAP 48.57 (45.66)
Train: [1200/2566]	Time 0.332 (0.332)	Loss 1.92 (1.70)	mAP 40.61 (45.67)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 1.45 (1.70)	mAP 44.35 (45.63)
Train: [1400/2566]	Time 0.330 (0.332)	Loss 1.59 (1.70)	mAP 52.99 (45.64)
Train: [1500/2566]	Time 0.330 (0.332)	Loss 1.66 (1.70)	mAP 47.04 (45.63)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 2.12 (1.70)	mAP 54.23 (45.64)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.81 (1.71)	mAP 42.79 (45.64)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 2.29 (1.71)	mAP 48.17 (45.64)
Train: [1900/2566]	Time 0.332 (0.331)	Loss 1.91 (1.71)	mAP 40.20 (45.62)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.31 (1.71)	mAP 41.85 (45.64)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.35 (1.71)	mAP 48.41 (45.61)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.43 (1.71)	mAP 40.37 (45.56)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.09 (1.71)	mAP 46.13 (45.57)
Train: [2400/2566]	Time 0.332 (0.331)	Loss 1.43 (1.71)	mAP 46.76 (45.56)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.67 (1.71)	mAP 45.27 (45.53)
Train: [39/51]	Time 0.331	Loss 1.71 	mAP 45.54
Test: [0/402]	Time 1.795 (1.795)	Precision 75.89 (75.89)	Recall 78.69 (78.69) 	 P_C 30.26 	 R_C 32.16 	 F_C 29.84 	 P_O 75.89 	 R_O 78.69 	 F_O 77.27
Test: [100/402]	Time 0.500 (0.512)	Precision 88.08 (79.50)	Recall 86.93 (83.37) 	 P_C 69.23 	 R_C 68.58 	 F_C 67.51 	 P_O 78.24 	 R_O 81.96 	 F_O 80.06
Test: [200/402]	Time 0.499 (0.506)	Precision 81.53 (80.32)	Recall 87.88 (83.83) 	 P_C 71.45 	 R_C 71.31 	 F_C 70.79 	 P_O 79.30 	 R_O 82.78 	 F_O 81.00
Test: [300/402]	Time 0.500 (0.504)	Precision 82.59 (80.44)	Recall 82.59 (84.29) 	 P_C 75.72 	 R_C 77.14 	 F_C 75.98 	 P_O 79.33 	 R_O 83.21 	 F_O 81.22
Test: [400/402]	Time 0.500 (0.503)	Precision 62.16 (79.49)	Recall 66.86 (84.24) 	 P_C 78.54 	 R_C 78.72 	 F_C 77.72 	 P_O 78.52 	 R_O 83.42 	 F_O 80.89
Test: [39/51]	  P_C 78.55 	 R_C 78.72 	 F_C 77.72 	 P_O 78.52 	 R_O 83.41 	 F_O 80.89 	 mAP 84.30
Train: [0/2566]	Time 1.049 (1.049)	Loss 1.19 (1.19)	mAP 42.74 (42.74)
Train: [100/2566]	Time 0.332 (0.338)	Loss 1.84 (1.76)	mAP 47.10 (45.88)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.81 (1.73)	mAP 45.08 (45.49)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.93 (1.72)	mAP 43.44 (45.58)
Train: [400/2566]	Time 0.330 (0.333)	Loss 1.39 (1.71)	mAP 40.76 (45.23)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.78 (1.72)	mAP 43.64 (45.23)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.24 (1.72)	mAP 47.99 (45.37)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.34 (1.72)	mAP 48.77 (45.34)
Train: [800/2566]	Time 0.331 (0.332)	Loss 1.29 (1.71)	mAP 43.12 (45.30)
Train: [900/2566]	Time 0.331 (0.332)	Loss 1.63 (1.71)	mAP 51.38 (45.37)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 2.14 (1.71)	mAP 41.84 (45.38)
Train: [1100/2566]	Time 0.333 (0.332)	Loss 1.61 (1.71)	mAP 41.22 (45.46)
Train: [1200/2566]	Time 0.330 (0.332)	Loss 1.54 (1.70)	mAP 40.76 (45.47)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 1.75 (1.71)	mAP 45.92 (45.51)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.75 (1.71)	mAP 43.44 (45.50)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.38 (1.71)	mAP 53.39 (45.55)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.80 (1.71)	mAP 43.03 (45.56)
Train: [1700/2566]	Time 0.332 (0.331)	Loss 1.82 (1.71)	mAP 49.70 (45.55)
Train: [1800/2566]	Time 0.332 (0.331)	Loss 1.61 (1.71)	mAP 51.62 (45.55)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.78 (1.71)	mAP 43.84 (45.55)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 2.11 (1.70)	mAP 51.21 (45.57)
Train: [2100/2566]	Time 0.333 (0.331)	Loss 2.11 (1.71)	mAP 47.22 (45.61)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.68 (1.71)	mAP 48.65 (45.62)
Train: [2300/2566]	Time 0.332 (0.331)	Loss 1.47 (1.71)	mAP 46.68 (45.60)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.58 (1.71)	mAP 40.15 (45.57)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 1.24 (1.71)	mAP 38.36 (45.57)
Train: [40/51]	Time 0.331	Loss 1.71 	mAP 45.56
Test: [0/402]	Time 1.768 (1.768)	Precision 75.81 (75.81)	Recall 80.11 (80.11) 	 P_C 31.41 	 R_C 33.32 	 F_C 30.71 	 P_O 75.81 	 R_O 80.11 	 F_O 77.90
Test: [100/402]	Time 0.500 (0.512)	Precision 87.42 (76.45)	Recall 90.85 (84.97) 	 P_C 65.76 	 R_C 71.18 	 F_C 66.78 	 P_O 74.87 	 R_O 83.82 	 F_O 79.09
Test: [200/402]	Time 0.499 (0.506)	Precision 80.88 (77.63)	Recall 87.88 (85.16) 	 P_C 68.06 	 R_C 74.13 	 F_C 70.06 	 P_O 76.40 	 R_O 84.26 	 F_O 80.14
Test: [300/402]	Time 0.499 (0.504)	Precision 81.84 (77.81)	Recall 81.84 (85.74) 	 P_C 73.33 	 R_C 79.71 	 F_C 75.62 	 P_O 76.53 	 R_O 84.79 	 F_O 80.45
Test: [400/402]	Time 0.500 (0.503)	Precision 57.42 (76.95)	Recall 69.77 (85.69) 	 P_C 74.98 	 R_C 81.16 	 F_C 77.18 	 P_O 75.83 	 R_O 84.94 	 F_O 80.13
Test: [40/51]	  P_C 74.98 	 R_C 81.15 	 F_C 77.18 	 P_O 75.83 	 R_O 84.94 	 F_O 80.13 	 mAP 84.28
Train: [0/2566]	Time 1.057 (1.057)	Loss 2.24 (2.24)	mAP 44.68 (44.68)
Train: [100/2566]	Time 0.330 (0.338)	Loss 1.61 (1.62)	mAP 48.53 (45.97)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.97 (1.63)	mAP 42.82 (45.57)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.66 (1.66)	mAP 42.53 (45.65)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.59 (1.67)	mAP 44.49 (45.45)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.53 (1.67)	mAP 47.06 (45.47)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.48 (1.68)	mAP 41.73 (45.63)
Train: [700/2566]	Time 0.330 (0.332)	Loss 1.86 (1.68)	mAP 51.00 (45.62)
Train: [800/2566]	Time 0.331 (0.332)	Loss 1.22 (1.68)	mAP 48.11 (45.58)
Train: [900/2566]	Time 0.331 (0.332)	Loss 1.53 (1.68)	mAP 48.55 (45.56)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 1.88 (1.69)	mAP 50.34 (45.62)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 1.39 (1.69)	mAP 45.12 (45.66)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.68 (1.69)	mAP 39.28 (45.64)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 2.09 (1.69)	mAP 46.79 (45.67)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.66 (1.69)	mAP 39.37 (45.65)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.22 (1.69)	mAP 48.07 (45.64)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.64 (1.69)	mAP 45.82 (45.65)
Train: [1700/2566]	Time 0.332 (0.331)	Loss 2.12 (1.69)	mAP 58.25 (45.64)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.58 (1.69)	mAP 44.50 (45.63)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.59 (1.69)	mAP 47.96 (45.63)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.62 (1.70)	mAP 45.75 (45.62)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.94 (1.70)	mAP 48.83 (45.58)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.75 (1.70)	mAP 41.15 (45.56)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.58 (1.70)	mAP 44.03 (45.57)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.61 (1.70)	mAP 41.02 (45.59)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.57 (1.70)	mAP 45.30 (45.58)
Train: [41/51]	Time 0.331	Loss 1.70 	mAP 45.58
Test: [0/402]	Time 1.765 (1.765)	Precision 75.95 (75.95)	Recall 79.83 (79.83) 	 P_C 29.72 	 R_C 32.13 	 F_C 29.65 	 P_O 75.95 	 R_O 79.83 	 F_O 77.84
Test: [100/402]	Time 0.500 (0.512)	Precision 88.82 (78.42)	Recall 88.24 (83.94) 	 P_C 67.60 	 R_C 69.54 	 F_C 66.83 	 P_O 77.00 	 R_O 82.66 	 F_O 79.73
Test: [200/402]	Time 0.500 (0.506)	Precision 79.07 (79.30)	Recall 88.31 (84.41) 	 P_C 69.91 	 R_C 72.50 	 F_C 70.24 	 P_O 78.20 	 R_O 83.40 	 F_O 80.72
Test: [300/402]	Time 0.499 (0.504)	Precision 82.88 (79.53)	Recall 83.08 (84.88) 	 P_C 74.92 	 R_C 78.30 	 F_C 75.86 	 P_O 78.45 	 R_O 83.79 	 F_O 81.04
Test: [400/402]	Time 0.500 (0.503)	Precision 62.11 (78.77)	Recall 68.60 (84.81) 	 P_C 76.73 	 R_C 79.81 	 F_C 77.57 	 P_O 77.85 	 R_O 83.95 	 F_O 80.79
Test: [41/51]	  P_C 76.73 	 R_C 79.81 	 F_C 77.57 	 P_O 77.85 	 R_O 83.95 	 F_O 80.78 	 mAP 84.35
Train: [0/2566]	Time 1.062 (1.062)	Loss 2.42 (2.42)	mAP 44.56 (44.56)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.86 (1.63)	mAP 50.51 (45.33)
Train: [200/2566]	Time 0.330 (0.335)	Loss 1.65 (1.67)	mAP 44.59 (45.71)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.73 (1.69)	mAP 38.55 (45.54)
Train: [400/2566]	Time 0.333 (0.333)	Loss 1.47 (1.70)	mAP 40.01 (45.61)
Train: [500/2566]	Time 0.331 (0.333)	Loss 1.75 (1.69)	mAP 41.59 (45.58)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.79 (1.69)	mAP 52.83 (45.50)
Train: [700/2566]	Time 0.330 (0.332)	Loss 1.19 (1.70)	mAP 41.49 (45.49)
Train: [800/2566]	Time 0.331 (0.332)	Loss 1.86 (1.70)	mAP 47.22 (45.52)
Train: [900/2566]	Time 0.330 (0.332)	Loss 2.30 (1.70)	mAP 54.39 (45.45)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 1.74 (1.70)	mAP 43.83 (45.44)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.53 (1.69)	mAP 40.09 (45.39)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.81 (1.70)	mAP 39.59 (45.35)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.54 (1.70)	mAP 51.45 (45.33)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.65 (1.69)	mAP 43.95 (45.34)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.04 (1.69)	mAP 46.86 (45.35)
Train: [1600/2566]	Time 0.332 (0.331)	Loss 1.51 (1.69)	mAP 56.13 (45.39)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.71 (1.70)	mAP 48.83 (45.46)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.99 (1.70)	mAP 46.65 (45.43)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.41 (1.70)	mAP 33.36 (45.44)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.75 (1.70)	mAP 46.80 (45.47)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.85 (1.70)	mAP 45.90 (45.47)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.81 (1.70)	mAP 48.95 (45.46)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.79 (1.70)	mAP 46.78 (45.50)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.36 (1.70)	mAP 40.52 (45.51)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 1.28 (1.70)	mAP 40.30 (45.51)
Train: [42/51]	Time 0.331	Loss 1.70 	mAP 45.53
Test: [0/402]	Time 1.770 (1.770)	Precision 76.90 (76.90)	Recall 80.40 (80.40) 	 P_C 31.68 	 R_C 33.04 	 F_C 30.68 	 P_O 76.90 	 R_O 80.40 	 F_O 78.61
Test: [100/402]	Time 0.499 (0.512)	Precision 88.68 (78.72)	Recall 92.16 (84.27) 	 P_C 67.16 	 R_C 70.56 	 F_C 67.27 	 P_O 76.97 	 R_O 83.14 	 F_O 79.94
Test: [200/402]	Time 0.500 (0.506)	Precision 82.86 (79.75)	Recall 87.88 (84.50) 	 P_C 69.57 	 R_C 73.43 	 F_C 70.62 	 P_O 78.49 	 R_O 83.62 	 F_O 80.97
Test: [300/402]	Time 0.500 (0.504)	Precision 83.17 (79.45)	Recall 83.58 (85.23) 	 P_C 74.52 	 R_C 79.07 	 F_C 76.00 	 P_O 78.08 	 R_O 84.29 	 F_O 81.07
Test: [400/402]	Time 0.500 (0.503)	Precision 59.22 (78.40)	Recall 70.93 (85.23) 	 P_C 76.40 	 R_C 80.49 	 F_C 77.65 	 P_O 77.22 	 R_O 84.50 	 F_O 80.69
Test: [42/51]	  P_C 76.40 	 R_C 80.48 	 F_C 77.65 	 P_O 77.21 	 R_O 84.50 	 F_O 80.69 	 mAP 84.43
Train: [0/2566]	Time 0.962 (0.962)	Loss 1.76 (1.76)	mAP 46.12 (46.12)
Train: [100/2566]	Time 0.330 (0.336)	Loss 1.71 (1.68)	mAP 38.90 (45.10)
Train: [200/2566]	Time 0.330 (0.333)	Loss 2.09 (1.68)	mAP 50.26 (45.28)
Train: [300/2566]	Time 0.330 (0.332)	Loss 2.14 (1.68)	mAP 44.08 (45.37)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.53 (1.68)	mAP 44.88 (45.48)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.69 (1.68)	mAP 48.79 (45.53)
Train: [600/2566]	Time 0.330 (0.331)	Loss 1.49 (1.68)	mAP 49.11 (45.50)
Train: [700/2566]	Time 0.332 (0.331)	Loss 1.80 (1.68)	mAP 55.58 (45.57)
Train: [800/2566]	Time 0.331 (0.331)	Loss 1.34 (1.67)	mAP 47.93 (45.53)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.74 (1.67)	mAP 47.84 (45.49)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.80 (1.67)	mAP 46.93 (45.50)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.55 (1.68)	mAP 42.91 (45.56)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.92 (1.68)	mAP 45.52 (45.61)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.89 (1.68)	mAP 61.06 (45.57)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.86 (1.69)	mAP 40.38 (45.60)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.49 (1.68)	mAP 46.14 (45.61)
Train: [1600/2566]	Time 0.332 (0.331)	Loss 1.50 (1.69)	mAP 44.77 (45.62)
Train: [1700/2566]	Time 0.332 (0.331)	Loss 1.46 (1.69)	mAP 45.38 (45.61)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 2.15 (1.69)	mAP 44.61 (45.66)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.79 (1.69)	mAP 46.02 (45.65)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.63 (1.69)	mAP 48.04 (45.66)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.40 (1.69)	mAP 41.75 (45.67)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.64 (1.69)	mAP 47.81 (45.67)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.70 (1.69)	mAP 44.98 (45.69)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.41 (1.69)	mAP 45.74 (45.67)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.94 (1.69)	mAP 39.77 (45.69)
Train: [43/51]	Time 0.331	Loss 1.69 	mAP 45.69
Test: [0/402]	Time 1.845 (1.845)	Precision 75.60 (75.60)	Recall 80.11 (80.11) 	 P_C 30.90 	 R_C 33.19 	 F_C 30.45 	 P_O 75.60 	 R_O 80.11 	 F_O 77.79
Test: [100/402]	Time 0.500 (0.513)	Precision 88.31 (77.47)	Recall 88.89 (84.73) 	 P_C 66.08 	 R_C 71.24 	 F_C 66.91 	 P_O 75.82 	 R_O 83.58 	 F_O 79.51
Test: [200/402]	Time 0.500 (0.506)	Precision 81.93 (78.68)	Recall 88.31 (84.94) 	 P_C 68.72 	 R_C 74.27 	 F_C 70.41 	 P_O 77.44 	 R_O 84.06 	 F_O 80.61
Test: [300/402]	Time 0.499 (0.504)	Precision 83.50 (78.59)	Recall 83.08 (85.56) 	 P_C 73.90 	 R_C 79.61 	 F_C 75.83 	 P_O 77.34 	 R_O 84.60 	 F_O 80.81
Test: [400/402]	Time 0.500 (0.503)	Precision 57.35 (77.71)	Recall 68.02 (85.52) 	 P_C 75.70 	 R_C 81.04 	 F_C 77.53 	 P_O 76.62 	 R_O 84.77 	 F_O 80.49
Test: [43/51]	  P_C 75.69 	 R_C 81.04 	 F_C 77.53 	 P_O 76.62 	 R_O 84.77 	 F_O 80.49 	 mAP 84.41
Train: [0/2566]	Time 1.000 (1.000)	Loss 1.46 (1.46)	mAP 43.34 (43.34)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.27 (1.68)	mAP 49.34 (45.36)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.93 (1.69)	mAP 50.83 (45.12)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.65 (1.68)	mAP 46.48 (45.31)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.88 (1.68)	mAP 43.44 (45.35)
Train: [500/2566]	Time 0.332 (0.332)	Loss 1.57 (1.67)	mAP 44.99 (45.39)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.61 (1.68)	mAP 47.24 (45.50)
Train: [700/2566]	Time 0.332 (0.332)	Loss 1.78 (1.67)	mAP 48.90 (45.46)
Train: [800/2566]	Time 0.330 (0.332)	Loss 1.59 (1.67)	mAP 39.38 (45.53)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.55 (1.67)	mAP 48.99 (45.51)
Train: [1000/2566]	Time 0.332 (0.331)	Loss 1.82 (1.68)	mAP 50.77 (45.54)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 2.18 (1.68)	mAP 36.96 (45.57)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.86 (1.68)	mAP 42.33 (45.55)
Train: [1300/2566]	Time 0.332 (0.331)	Loss 1.14 (1.68)	mAP 44.27 (45.56)
Train: [1400/2566]	Time 0.332 (0.331)	Loss 1.78 (1.68)	mAP 49.76 (45.54)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.91 (1.68)	mAP 44.06 (45.55)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.25 (1.68)	mAP 36.92 (45.59)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 2.35 (1.68)	mAP 48.67 (45.60)
Train: [1800/2566]	Time 0.332 (0.331)	Loss 1.60 (1.68)	mAP 51.71 (45.58)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.64 (1.68)	mAP 50.41 (45.55)
Train: [2000/2566]	Time 0.332 (0.331)	Loss 1.73 (1.68)	mAP 47.01 (45.59)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.89 (1.68)	mAP 48.50 (45.60)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.64 (1.68)	mAP 44.55 (45.62)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.06 (1.68)	mAP 42.53 (45.61)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.80 (1.69)	mAP 42.38 (45.59)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 1.58 (1.69)	mAP 46.06 (45.58)
Train: [44/51]	Time 0.331	Loss 1.69 	mAP 45.58
Test: [0/402]	Time 1.859 (1.859)	Precision 75.41 (75.41)	Recall 79.26 (79.26) 	 P_C 30.27 	 R_C 32.48 	 F_C 30.23 	 P_O 75.41 	 R_O 79.26 	 F_O 77.29
Test: [100/402]	Time 0.500 (0.513)	Precision 88.05 (79.40)	Recall 91.50 (83.60) 	 P_C 67.70 	 R_C 69.23 	 F_C 66.90 	 P_O 78.20 	 R_O 82.18 	 F_O 80.14
Test: [200/402]	Time 0.500 (0.506)	Precision 81.85 (79.99)	Recall 87.88 (84.22) 	 P_C 70.30 	 R_C 72.65 	 F_C 70.67 	 P_O 79.08 	 R_O 83.14 	 F_O 81.06
Test: [300/402]	Time 0.500 (0.504)	Precision 84.63 (79.80)	Recall 83.58 (84.96) 	 P_C 75.03 	 R_C 78.54 	 F_C 76.04 	 P_O 78.80 	 R_O 83.84 	 F_O 81.24
Test: [400/402]	Time 0.500 (0.503)	Precision 62.63 (78.93)	Recall 69.19 (84.85) 	 P_C 76.85 	 R_C 80.06 	 F_C 77.77 	 P_O 78.11 	 R_O 83.96 	 F_O 80.93
Test: [44/51]	  P_C 76.85 	 R_C 80.06 	 F_C 77.77 	 P_O 78.11 	 R_O 83.96 	 F_O 80.93 	 mAP 84.44
Train: [0/2566]	Time 1.001 (1.001)	Loss 1.67 (1.67)	mAP 56.30 (56.30)
Train: [100/2566]	Time 0.332 (0.338)	Loss 1.34 (1.65)	mAP 45.59 (45.96)
Train: [200/2566]	Time 0.331 (0.335)	Loss 1.48 (1.64)	mAP 47.35 (45.33)
Train: [300/2566]	Time 0.332 (0.334)	Loss 1.55 (1.65)	mAP 49.40 (45.55)
Train: [400/2566]	Time 0.332 (0.333)	Loss 1.59 (1.66)	mAP 45.55 (45.53)
Train: [500/2566]	Time 0.332 (0.333)	Loss 1.64 (1.66)	mAP 48.21 (45.48)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.82 (1.66)	mAP 40.52 (45.44)
Train: [700/2566]	Time 0.331 (0.332)	Loss 2.53 (1.67)	mAP 47.94 (45.54)
Train: [800/2566]	Time 0.330 (0.332)	Loss 1.80 (1.67)	mAP 47.26 (45.50)
Train: [900/2566]	Time 0.333 (0.332)	Loss 1.86 (1.66)	mAP 49.36 (45.44)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.34 (1.67)	mAP 47.24 (45.42)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 1.94 (1.67)	mAP 44.61 (45.38)
Train: [1200/2566]	Time 0.332 (0.332)	Loss 1.57 (1.67)	mAP 46.67 (45.43)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 1.06 (1.67)	mAP 39.34 (45.45)
Train: [1400/2566]	Time 0.331 (0.332)	Loss 2.09 (1.68)	mAP 45.67 (45.46)
Train: [1500/2566]	Time 0.332 (0.332)	Loss 1.68 (1.68)	mAP 44.99 (45.47)
Train: [1600/2566]	Time 0.330 (0.332)	Loss 2.10 (1.68)	mAP 42.55 (45.51)
Train: [1700/2566]	Time 0.331 (0.332)	Loss 1.99 (1.68)	mAP 48.46 (45.51)
Train: [1800/2566]	Time 0.331 (0.332)	Loss 1.61 (1.68)	mAP 46.79 (45.53)
Train: [1900/2566]	Time 0.330 (0.332)	Loss 1.70 (1.68)	mAP 40.96 (45.60)
Train: [2000/2566]	Time 0.331 (0.332)	Loss 1.64 (1.68)	mAP 41.69 (45.58)
Train: [2100/2566]	Time 0.330 (0.332)	Loss 1.94 (1.68)	mAP 46.88 (45.60)
Train: [2200/2566]	Time 0.330 (0.332)	Loss 1.29 (1.69)	mAP 47.19 (45.60)
Train: [2300/2566]	Time 0.330 (0.332)	Loss 1.46 (1.69)	mAP 50.64 (45.59)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.88 (1.69)	mAP 45.13 (45.59)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.30 (1.69)	mAP 45.78 (45.56)
Train: [45/51]	Time 0.331	Loss 1.69 	mAP 45.60
Test: [0/402]	Time 1.803 (1.803)	Precision 75.40 (75.40)	Recall 80.97 (80.97) 	 P_C 29.98 	 R_C 33.47 	 F_C 30.13 	 P_O 75.40 	 R_O 80.97 	 F_O 78.08
Test: [100/402]	Time 0.500 (0.513)	Precision 88.68 (77.79)	Recall 92.16 (84.83) 	 P_C 65.85 	 R_C 71.25 	 F_C 66.99 	 P_O 75.92 	 R_O 83.77 	 F_O 79.65
Test: [200/402]	Time 0.500 (0.506)	Precision 81.60 (78.78)	Recall 88.31 (85.00) 	 P_C 68.24 	 R_C 74.46 	 F_C 70.41 	 P_O 77.47 	 R_O 84.18 	 F_O 80.69
Test: [300/402]	Time 0.499 (0.504)	Precision 82.67 (78.63)	Recall 83.08 (85.65) 	 P_C 73.47 	 R_C 79.70 	 F_C 75.74 	 P_O 77.22 	 R_O 84.77 	 F_O 80.82
Test: [400/402]	Time 0.500 (0.503)	Precision 58.37 (77.64)	Recall 70.93 (85.64) 	 P_C 75.45 	 R_C 81.04 	 F_C 77.48 	 P_O 76.40 	 R_O 84.97 	 F_O 80.46
Test: [45/51]	  P_C 75.45 	 R_C 81.03 	 F_C 77.48 	 P_O 76.39 	 R_O 84.97 	 F_O 80.45 	 mAP 84.48
Train: [0/2566]	Time 0.996 (0.996)	Loss 1.29 (1.29)	mAP 46.68 (46.68)
Train: [100/2566]	Time 0.331 (0.338)	Loss 2.14 (1.71)	mAP 46.94 (44.94)
Train: [200/2566]	Time 0.332 (0.335)	Loss 1.84 (1.71)	mAP 49.99 (45.27)
Train: [300/2566]	Time 0.332 (0.333)	Loss 1.36 (1.69)	mAP 50.02 (45.59)
Train: [400/2566]	Time 0.331 (0.333)	Loss 1.70 (1.68)	mAP 43.97 (45.61)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.70 (1.68)	mAP 42.31 (45.59)
Train: [600/2566]	Time 0.332 (0.332)	Loss 1.35 (1.67)	mAP 52.43 (45.71)
Train: [700/2566]	Time 0.332 (0.332)	Loss 1.30 (1.68)	mAP 46.57 (45.72)
Train: [800/2566]	Time 0.330 (0.332)	Loss 1.68 (1.68)	mAP 41.02 (45.77)
Train: [900/2566]	Time 0.332 (0.332)	Loss 1.53 (1.68)	mAP 31.46 (45.75)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.04 (1.68)	mAP 44.68 (45.74)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 1.69 (1.68)	mAP 46.19 (45.68)
Train: [1200/2566]	Time 0.330 (0.332)	Loss 1.84 (1.68)	mAP 49.44 (45.59)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 1.10 (1.67)	mAP 44.71 (45.57)
Train: [1400/2566]	Time 0.332 (0.332)	Loss 1.84 (1.68)	mAP 51.66 (45.61)
Train: [1500/2566]	Time 0.331 (0.332)	Loss 1.35 (1.68)	mAP 45.46 (45.65)
Train: [1600/2566]	Time 0.330 (0.332)	Loss 1.86 (1.68)	mAP 43.83 (45.65)
Train: [1700/2566]	Time 0.331 (0.332)	Loss 2.28 (1.68)	mAP 43.19 (45.63)
Train: [1800/2566]	Time 0.333 (0.332)	Loss 1.89 (1.68)	mAP 45.74 (45.61)
Train: [1900/2566]	Time 0.332 (0.332)	Loss 1.79 (1.68)	mAP 37.78 (45.58)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.48 (1.68)	mAP 52.70 (45.57)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 1.52 (1.68)	mAP 41.11 (45.58)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.03 (1.68)	mAP 46.14 (45.60)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.74 (1.68)	mAP 46.88 (45.64)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.53 (1.68)	mAP 46.52 (45.65)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.50 (1.68)	mAP 49.16 (45.66)
Train: [46/51]	Time 0.331	Loss 1.68 	mAP 45.65
Test: [0/402]	Time 1.767 (1.767)	Precision 77.78 (77.78)	Recall 79.55 (79.55) 	 P_C 31.18 	 R_C 32.29 	 F_C 30.35 	 P_O 77.78 	 R_O 79.55 	 F_O 78.65
Test: [100/402]	Time 0.500 (0.512)	Precision 89.10 (79.32)	Recall 90.85 (83.85) 	 P_C 67.38 	 R_C 70.04 	 F_C 67.45 	 P_O 77.78 	 R_O 82.63 	 F_O 80.13
Test: [200/402]	Time 0.500 (0.506)	Precision 82.79 (80.36)	Recall 87.45 (84.12) 	 P_C 69.64 	 R_C 72.92 	 F_C 70.77 	 P_O 79.23 	 R_O 83.18 	 F_O 81.16
Test: [300/402]	Time 0.500 (0.504)	Precision 83.25 (80.42)	Recall 82.84 (84.71) 	 P_C 74.65 	 R_C 78.45 	 F_C 76.13 	 P_O 79.21 	 R_O 83.71 	 F_O 81.40
Test: [400/402]	Time 0.500 (0.503)	Precision 60.82 (79.53)	Recall 68.60 (84.65) 	 P_C 77.40 	 R_C 79.94 	 F_C 77.81 	 P_O 78.48 	 R_O 83.87 	 F_O 81.09
Test: [46/51]	  P_C 77.41 	 R_C 79.93 	 F_C 77.81 	 P_O 78.48 	 R_O 83.86 	 F_O 81.08 	 mAP 84.48
Train: [0/2566]	Time 1.016 (1.016)	Loss 1.79 (1.79)	mAP 45.71 (45.71)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.83 (1.65)	mAP 42.16 (45.37)
Train: [200/2566]	Time 0.332 (0.335)	Loss 1.40 (1.63)	mAP 44.78 (45.37)
Train: [300/2566]	Time 0.331 (0.334)	Loss 1.69 (1.64)	mAP 41.77 (45.49)
Train: [400/2566]	Time 0.331 (0.333)	Loss 1.56 (1.65)	mAP 37.91 (45.45)
Train: [500/2566]	Time 0.331 (0.333)	Loss 1.66 (1.66)	mAP 41.97 (45.44)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.25 (1.67)	mAP 44.27 (45.41)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.58 (1.67)	mAP 53.84 (45.53)
Train: [800/2566]	Time 0.333 (0.332)	Loss 2.27 (1.67)	mAP 47.27 (45.62)
Train: [900/2566]	Time 0.332 (0.332)	Loss 2.01 (1.67)	mAP 48.01 (45.69)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.19 (1.67)	mAP 38.41 (45.67)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 1.35 (1.67)	mAP 35.25 (45.58)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 1.75 (1.67)	mAP 40.72 (45.59)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 1.51 (1.67)	mAP 39.83 (45.51)
Train: [1400/2566]	Time 0.331 (0.332)	Loss 1.65 (1.67)	mAP 50.39 (45.54)
Train: [1500/2566]	Time 0.330 (0.332)	Loss 2.04 (1.67)	mAP 41.38 (45.59)
Train: [1600/2566]	Time 0.331 (0.332)	Loss 1.41 (1.68)	mAP 48.04 (45.60)
Train: [1700/2566]	Time 0.336 (0.332)	Loss 2.01 (1.68)	mAP 42.01 (45.59)
Train: [1800/2566]	Time 0.332 (0.332)	Loss 1.51 (1.68)	mAP 43.22 (45.61)
Train: [1900/2566]	Time 0.330 (0.332)	Loss 1.43 (1.68)	mAP 45.94 (45.64)
Train: [2000/2566]	Time 0.330 (0.332)	Loss 1.48 (1.68)	mAP 36.11 (45.63)
Train: [2100/2566]	Time 0.330 (0.332)	Loss 1.61 (1.68)	mAP 46.14 (45.65)
Train: [2200/2566]	Time 0.331 (0.332)	Loss 1.86 (1.68)	mAP 42.77 (45.59)
Train: [2300/2566]	Time 0.331 (0.332)	Loss 1.55 (1.68)	mAP 48.49 (45.63)
Train: [2400/2566]	Time 0.330 (0.332)	Loss 1.38 (1.68)	mAP 47.06 (45.64)
Train: [2500/2566]	Time 0.331 (0.332)	Loss 1.82 (1.68)	mAP 41.10 (45.65)
Train: [47/51]	Time 0.331	Loss 1.68 	mAP 45.65
Test: [0/402]	Time 1.755 (1.755)	Precision 75.95 (75.95)	Recall 79.83 (79.83) 	 P_C 30.43 	 R_C 32.71 	 F_C 30.21 	 P_O 75.95 	 R_O 79.83 	 F_O 77.84
Test: [100/402]	Time 0.500 (0.512)	Precision 87.97 (77.84)	Recall 90.85 (84.44) 	 P_C 66.98 	 R_C 70.24 	 F_C 67.00 	 P_O 76.38 	 R_O 83.26 	 F_O 79.68
Test: [200/402]	Time 0.499 (0.506)	Precision 82.26 (78.74)	Recall 88.31 (84.87) 	 P_C 69.39 	 R_C 73.42 	 F_C 70.62 	 P_O 77.64 	 R_O 83.97 	 F_O 80.68
Test: [300/402]	Time 0.499 (0.505)	Precision 82.40 (78.81)	Recall 83.83 (85.47) 	 P_C 73.97 	 R_C 79.27 	 F_C 75.90 	 P_O 77.63 	 R_O 84.53 	 F_O 80.93
Test: [400/402]	Time 0.500 (0.504)	Precision 59.80 (77.85)	Recall 69.19 (85.47) 	 P_C 75.68 	 R_C 80.75 	 F_C 77.53 	 P_O 76.85 	 R_O 84.74 	 F_O 80.60
Test: [47/51]	  P_C 75.69 	 R_C 80.74 	 F_C 77.53 	 P_O 76.85 	 R_O 84.73 	 F_O 80.60 	 mAP 84.49
Train: [0/2566]	Time 1.054 (1.054)	Loss 1.68 (1.68)	mAP 49.92 (49.92)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.82 (1.67)	mAP 46.17 (44.94)
Train: [200/2566]	Time 0.332 (0.334)	Loss 1.68 (1.69)	mAP 43.17 (45.07)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.92 (1.68)	mAP 46.91 (45.28)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.77 (1.68)	mAP 39.08 (45.41)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.26 (1.66)	mAP 45.92 (45.32)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.88 (1.66)	mAP 48.60 (45.34)
Train: [700/2566]	Time 0.332 (0.331)	Loss 2.15 (1.67)	mAP 48.71 (45.41)
Train: [800/2566]	Time 0.331 (0.331)	Loss 1.88 (1.67)	mAP 47.07 (45.54)
Train: [900/2566]	Time 0.331 (0.331)	Loss 1.50 (1.67)	mAP 42.38 (45.45)
Train: [1000/2566]	Time 0.333 (0.331)	Loss 1.62 (1.67)	mAP 42.82 (45.46)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 1.44 (1.67)	mAP 48.05 (45.44)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.57 (1.67)	mAP 40.43 (45.45)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.87 (1.67)	mAP 45.67 (45.54)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.48 (1.67)	mAP 44.06 (45.53)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.47 (1.68)	mAP 39.17 (45.55)
Train: [1600/2566]	Time 0.332 (0.331)	Loss 1.56 (1.67)	mAP 42.94 (45.55)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.95 (1.68)	mAP 42.36 (45.58)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.65 (1.68)	mAP 39.80 (45.57)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.01 (1.67)	mAP 49.08 (45.59)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.59 (1.67)	mAP 46.07 (45.59)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.81 (1.67)	mAP 52.73 (45.56)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.22 (1.67)	mAP 48.09 (45.55)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.72 (1.67)	mAP 44.20 (45.60)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.23 (1.67)	mAP 43.83 (45.59)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 1.47 (1.67)	mAP 43.39 (45.62)
Train: [48/51]	Time 0.331	Loss 1.67 	mAP 45.61
Test: [0/402]	Time 1.802 (1.802)	Precision 76.29 (76.29)	Recall 79.55 (79.55) 	 P_C 29.04 	 R_C 32.15 	 F_C 29.28 	 P_O 76.29 	 R_O 79.55 	 F_O 77.89
Test: [100/402]	Time 0.500 (0.513)	Precision 87.42 (78.39)	Recall 90.85 (84.31) 	 P_C 66.51 	 R_C 70.23 	 F_C 66.95 	 P_O 76.90 	 R_O 83.14 	 F_O 79.90
Test: [200/402]	Time 0.500 (0.507)	Precision 82.59 (79.39)	Recall 88.31 (84.65) 	 P_C 69.20 	 R_C 73.38 	 F_C 70.61 	 P_O 78.27 	 R_O 83.76 	 F_O 80.92
Test: [300/402]	Time 0.500 (0.504)	Precision 82.84 (79.31)	Recall 82.84 (85.33) 	 P_C 74.03 	 R_C 79.19 	 F_C 75.98 	 P_O 78.13 	 R_O 84.38 	 F_O 81.13
Test: [400/402]	Time 0.500 (0.503)	Precision 58.62 (78.39)	Recall 69.19 (85.28) 	 P_C 76.02 	 R_C 80.64 	 F_C 77.68 	 P_O 77.37 	 R_O 84.55 	 F_O 80.80
Test: [48/51]	  P_C 76.02 	 R_C 80.63 	 F_C 77.68 	 P_O 77.36 	 R_O 84.54 	 F_O 80.79 	 mAP 84.51
Train: [0/2566]	Time 0.995 (0.995)	Loss 1.53 (1.53)	mAP 45.74 (45.74)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.49 (1.65)	mAP 47.50 (46.44)
Train: [200/2566]	Time 0.330 (0.333)	Loss 1.94 (1.68)	mAP 45.15 (46.36)
Train: [300/2566]	Time 0.329 (0.332)	Loss 1.40 (1.68)	mAP 48.11 (46.22)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.71 (1.68)	mAP 43.46 (45.90)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.42 (1.67)	mAP 43.01 (45.74)
Train: [600/2566]	Time 0.332 (0.332)	Loss 1.98 (1.67)	mAP 47.98 (45.78)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.70 (1.66)	mAP 41.45 (45.65)
Train: [800/2566]	Time 0.332 (0.332)	Loss 2.28 (1.67)	mAP 45.80 (45.59)
Train: [900/2566]	Time 0.331 (0.331)	Loss 1.60 (1.66)	mAP 54.72 (45.59)
Train: [1000/2566]	Time 0.338 (0.332)	Loss 0.97 (1.66)	mAP 42.30 (45.58)
Train: [1100/2566]	Time 0.334 (0.332)	Loss 1.29 (1.66)	mAP 52.58 (45.60)
Train: [1200/2566]	Time 0.330 (0.332)	Loss 1.77 (1.67)	mAP 44.28 (45.61)
Train: [1300/2566]	Time 0.330 (0.332)	Loss 1.79 (1.67)	mAP 43.38 (45.69)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.90 (1.68)	mAP 52.40 (45.69)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.56 (1.68)	mAP 43.45 (45.71)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.53 (1.68)	mAP 54.35 (45.71)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.91 (1.67)	mAP 41.08 (45.72)
Train: [1800/2566]	Time 0.367 (0.332)	Loss 1.43 (1.68)	mAP 49.08 (45.69)
Train: [1900/2566]	Time 0.331 (0.332)	Loss 1.49 (1.68)	mAP 38.72 (45.69)
Train: [2000/2566]	Time 0.332 (0.332)	Loss 1.73 (1.68)	mAP 40.62 (45.69)
Train: [2100/2566]	Time 0.331 (0.332)	Loss 1.60 (1.68)	mAP 51.60 (45.69)
Train: [2200/2566]	Time 0.331 (0.332)	Loss 1.47 (1.68)	mAP 42.65 (45.65)
Train: [2300/2566]	Time 0.330 (0.332)	Loss 1.87 (1.68)	mAP 46.46 (45.63)
Train: [2400/2566]	Time 0.331 (0.332)	Loss 1.44 (1.67)	mAP 46.35 (45.62)
Train: [2500/2566]	Time 0.331 (0.332)	Loss 1.94 (1.67)	mAP 36.83 (45.67)
Train: [49/51]	Time 0.332	Loss 1.67 	mAP 45.64
Test: [0/402]	Time 1.754 (1.754)	Precision 76.50 (76.50)	Recall 79.55 (79.55) 	 P_C 29.82 	 R_C 32.08 	 F_C 29.55 	 P_O 76.50 	 R_O 79.55 	 F_O 77.99
Test: [100/402]	Time 0.500 (0.512)	Precision 87.34 (78.12)	Recall 90.20 (84.44) 	 P_C 66.86 	 R_C 70.16 	 F_C 67.06 	 P_O 76.69 	 R_O 83.22 	 F_O 79.82
Test: [200/402]	Time 0.500 (0.506)	Precision 82.86 (79.05)	Recall 87.88 (84.82) 	 P_C 69.44 	 R_C 73.17 	 F_C 70.70 	 P_O 77.96 	 R_O 83.90 	 F_O 80.82
Test: [300/402]	Time 0.500 (0.504)	Precision 82.72 (79.12)	Recall 83.33 (85.42) 	 P_C 73.97 	 R_C 78.96 	 F_C 75.98 	 P_O 77.94 	 R_O 84.45 	 F_O 81.06
Test: [400/402]	Time 0.500 (0.503)	Precision 60.31 (78.19)	Recall 68.02 (85.37) 	 P_C 76.35 	 R_C 80.41 	 F_C 77.68 	 P_O 77.18 	 R_O 84.62 	 F_O 80.73
Test: [49/51]	  P_C 76.35 	 R_C 80.41 	 F_C 77.68 	 P_O 77.18 	 R_O 84.61 	 F_O 80.72 	 mAP 84.51
Train: [0/2566]	Time 0.998 (0.998)	Loss 1.31 (1.31)	mAP 45.98 (45.98)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.58 (1.67)	mAP 46.38 (45.61)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.93 (1.68)	mAP 52.49 (45.59)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.55 (1.68)	mAP 46.01 (45.59)
Train: [400/2566]	Time 0.330 (0.333)	Loss 1.65 (1.68)	mAP 52.12 (45.74)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.47 (1.68)	mAP 47.07 (45.63)
Train: [600/2566]	Time 0.329 (0.332)	Loss 1.65 (1.68)	mAP 49.42 (45.59)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.79 (1.68)	mAP 43.72 (45.60)
Train: [800/2566]	Time 0.331 (0.332)	Loss 1.16 (1.68)	mAP 47.64 (45.68)
Train: [900/2566]	Time 0.331 (0.332)	Loss 1.14 (1.67)	mAP 50.68 (45.71)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.50 (1.68)	mAP 51.47 (45.69)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 1.76 (1.68)	mAP 42.01 (45.66)
Train: [1200/2566]	Time 0.332 (0.332)	Loss 1.69 (1.68)	mAP 40.80 (45.68)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.73 (1.68)	mAP 48.22 (45.68)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.39 (1.68)	mAP 40.61 (45.67)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.53 (1.68)	mAP 41.95 (45.62)
Train: [1600/2566]	Time 0.332 (0.331)	Loss 1.47 (1.68)	mAP 38.80 (45.60)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.60 (1.68)	mAP 46.17 (45.60)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.65 (1.68)	mAP 42.23 (45.60)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.41 (1.68)	mAP 42.90 (45.60)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.60 (1.68)	mAP 54.25 (45.59)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.47 (1.68)	mAP 50.88 (45.60)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 1.64 (1.68)	mAP 50.48 (45.56)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.13 (1.68)	mAP 39.66 (45.56)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.24 (1.68)	mAP 45.99 (45.58)
Train: [2500/2566]	Time 0.335 (0.331)	Loss 1.91 (1.68)	mAP 46.19 (45.61)
Train: [50/51]	Time 0.331	Loss 1.68 	mAP 45.61
Test: [0/402]	Time 1.920 (1.920)	Precision 76.78 (76.78)	Recall 79.83 (79.83) 	 P_C 30.66 	 R_C 32.78 	 F_C 30.37 	 P_O 76.78 	 R_O 79.83 	 F_O 78.27
Test: [100/402]	Time 0.500 (0.516)	Precision 87.50 (78.28)	Recall 91.50 (84.39) 	 P_C 66.77 	 R_C 70.51 	 F_C 67.09 	 P_O 76.72 	 R_O 83.19 	 F_O 79.82
Test: [200/402]	Time 0.500 (0.508)	Precision 82.79 (79.29)	Recall 87.45 (84.69) 	 P_C 69.45 	 R_C 73.62 	 F_C 70.75 	 P_O 78.13 	 R_O 83.79 	 F_O 80.86
Test: [300/402]	Time 0.500 (0.505)	Precision 83.08 (79.23)	Recall 83.08 (85.38) 	 P_C 74.14 	 R_C 79.36 	 F_C 76.05 	 P_O 77.99 	 R_O 84.44 	 F_O 81.09
Test: [400/402]	Time 0.500 (0.504)	Precision 60.61 (78.24)	Recall 69.77 (85.37) 	 P_C 75.90 	 R_C 80.80 	 F_C 77.67 	 P_O 77.16 	 R_O 84.64 	 F_O 80.73
Test: [50/51]	  P_C 75.90 	 R_C 80.80 	 F_C 77.67 	 P_O 77.16 	 R_O 84.64 	 F_O 80.72 	 mAP 84.52
Train: [0/2566]	Time 1.060 (1.060)	Loss 1.96 (1.96)	mAP 43.82 (43.82)
Train: [100/2566]	Time 0.330 (0.338)	Loss 1.67 (1.69)	mAP 50.82 (45.59)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.73 (1.67)	mAP 44.45 (45.71)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.71 (1.65)	mAP 46.12 (45.80)
Train: [400/2566]	Time 0.332 (0.333)	Loss 1.07 (1.67)	mAP 36.98 (45.50)
Train: [500/2566]	Time 0.332 (0.332)	Loss 1.73 (1.67)	mAP 49.85 (45.52)
Train: [600/2566]	Time 0.332 (0.332)	Loss 1.54 (1.66)	mAP 47.68 (45.35)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.15 (1.66)	mAP 43.58 (45.32)
Train: [800/2566]	Time 0.331 (0.332)	Loss 1.30 (1.66)	mAP 43.26 (45.23)
Train: [900/2566]	Time 0.330 (0.332)	Loss 1.80 (1.66)	mAP 42.58 (45.24)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.76 (1.66)	mAP 49.02 (45.28)
Train: [1100/2566]	Time 0.330 (0.332)	Loss 1.44 (1.66)	mAP 40.86 (45.29)
Train: [1200/2566]	Time 0.332 (0.332)	Loss 0.99 (1.66)	mAP 48.67 (45.30)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.68 (1.66)	mAP 43.80 (45.32)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.49 (1.66)	mAP 46.44 (45.30)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.48 (1.66)	mAP 42.35 (45.34)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 2.23 (1.67)	mAP 54.50 (45.39)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.52 (1.67)	mAP 39.90 (45.42)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.85 (1.67)	mAP 39.63 (45.44)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.76 (1.67)	mAP 44.71 (45.48)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.44 (1.67)	mAP 35.53 (45.48)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.42 (1.67)	mAP 50.69 (45.47)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.61 (1.67)	mAP 44.16 (45.48)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.86 (1.67)	mAP 46.85 (45.50)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.80 (1.67)	mAP 44.76 (45.51)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.18 (1.67)	mAP 38.46 (45.48)
Train: [51/51]	Time 0.331	Loss 1.67 	mAP 45.48
Test: [0/402]	Time 1.835 (1.835)	Precision 76.15 (76.15)	Recall 79.83 (79.83) 	 P_C 30.45 	 R_C 32.78 	 F_C 30.21 	 P_O 76.15 	 R_O 79.83 	 F_O 77.95
Test: [100/402]	Time 0.500 (0.513)	Precision 87.42 (78.30)	Recall 90.85 (84.41) 	 P_C 66.84 	 R_C 70.52 	 F_C 67.12 	 P_O 76.76 	 R_O 83.22 	 F_O 79.86
Test: [200/402]	Time 0.500 (0.506)	Precision 82.52 (79.25)	Recall 87.88 (84.75) 	 P_C 69.49 	 R_C 73.61 	 F_C 70.78 	 P_O 78.11 	 R_O 83.84 	 F_O 80.87
Test: [300/402]	Time 0.499 (0.504)	Precision 82.92 (79.21)	Recall 83.33 (85.38) 	 P_C 74.26 	 R_C 79.28 	 F_C 76.07 	 P_O 77.99 	 R_O 84.44 	 F_O 81.09
Test: [400/402]	Time 0.499 (0.503)	Precision 60.80 (78.23)	Recall 70.35 (85.38) 	 P_C 76.07 	 R_C 80.69 	 F_C 77.71 	 P_O 77.17 	 R_O 84.64 	 F_O 80.73
Test: [51/51]	  P_C 76.08 	 R_C 80.68 	 F_C 77.71 	 P_O 77.17 	 R_O 84.64 	 F_O 80.73 	 mAP 84.52
Evaluating the best model
Evaluate with threshold 0.50
... loading pretrained weights from ./output/coco-DualCoop-RN101-cosine-bs32-e51/model_best.pth.tar
Test: [0/402]	Time 1.751 (1.751)	Precision 76.15 (76.15)	Recall 79.83 (79.83) 	 P_C 30.45 	 R_C 32.78 	 F_C 30.21 	 P_O 76.15 	 R_O 79.83 	 F_O 77.95
Test: [100/402]	Time 0.500 (0.511)	Precision 87.42 (78.30)	Recall 90.85 (84.41) 	 P_C 66.84 	 R_C 70.52 	 F_C 67.12 	 P_O 76.76 	 R_O 83.22 	 F_O 79.86
Test: [200/402]	Time 0.499 (0.506)	Precision 82.52 (79.25)	Recall 87.88 (84.75) 	 P_C 69.49 	 R_C 73.61 	 F_C 70.78 	 P_O 78.11 	 R_O 83.84 	 F_O 80.87
Test: [300/402]	Time 0.500 (0.504)	Precision 82.92 (79.21)	Recall 83.33 (85.38) 	 P_C 74.26 	 R_C 79.28 	 F_C 76.07 	 P_O 77.99 	 R_O 84.44 	 F_O 81.09
Test: [400/402]	Time 0.501 (0.503)	Precision 60.80 (78.23)	Recall 70.35 (85.38) 	 P_C 76.07 	 R_C 80.69 	 F_C 77.71 	 P_O 77.17 	 R_O 84.64 	 F_O 80.73
Test: [51/51]	  P_C 76.08 	 R_C 80.68 	 F_C 77.71 	 P_O 77.17 	 R_O 84.64 	 F_O 80.73 	 mAP 84.52
