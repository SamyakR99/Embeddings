nohup: ignoring input
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/multilabel/data/coco14
data_split = train2014
PARTIAL_PORTION= 1.000000
INPUT.SIZE = 448
loading annotations into memory...
Done (t=6.91s)
creating index...
index created!
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/multilabel/data/coco14
data_split = val2014
PARTIAL_PORTION= 1.000000
INPUT.SIZE = 448
loading annotations into memory...
Done (t=3.01s)
creating index...
index created!
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/multilabel/data/coco14
data_split = val2014
PARTIAL_PORTION= 1.000000
INPUT.SIZE = 448
loading annotations into memory...
Done (t=3.22s)
creating index...
index created!
Loading CLIP (backbone: RN101)
Building dualcoop
Freeze the backbone weights
Freeze the attn weights
image_encoder.attnpool.positional_embedding
image_encoder.attnpool.k_proj.weight
image_encoder.attnpool.k_proj.bias
image_encoder.attnpool.q_proj.weight
image_encoder.attnpool.q_proj.bias
image_encoder.attnpool.v_proj.weight
image_encoder.attnpool.v_proj.bias
image_encoder.attnpool.c_proj.weight
image_encoder.attnpool.c_proj.bias
train.py --config_file configs/models/rn101_ep50.yaml --datadir /home/samyakr2/multilabel/data/coco14 --dataset_config_file /home/samyakr2/Redundancy/DualCoOp/configs/datasets/coco.yaml --input_size 448 --lr 0.002 --loss_w 0.05 -pp 1.0 --csc --max_epochs 51
Namespace(prefix='', resume=None, pretrained=None, auto_resume=False, datadir='/home/samyakr2/multilabel/data/coco14', input_size=448, train_input_size=None, num_train_cls=100, test_input_size=None, thre=0.5, single_prompt='pos', output_dir='', print_freq=100, val_freq_in_epoch=-1, evaluate=False, config_file='configs/models/rn101_ep50.yaml', dataset_config_file='/home/samyakr2/Redundancy/DualCoOp/configs/datasets/coco.yaml', positive_prompt=None, negative_prompt=None, n_ctx_pos=None, n_ctx_neg=None, lr=0.002, loss_w=0.05, csc=True, logit_scale=100.0, gamma_neg=2.0, gamma_pos=1.0, portion=1.0, partial_portion=1.0, mask_file=None, train_batch_size=None, stop_epochs=None, max_epochs=51, finetune=False, finetune_backbone=False, finetune_attn=False, finetune_text=False, base_lr_mult=None, backbone_lr_mult=None, text_lr_mult=None, attn_lr_mult=None, val_every_n_epochs=1, warmup_epochs=1, top_k=3)
DualCoop(
  (image_encoder): ModifiedResNet_conv_proj(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (attnpool): AttentionConv(
      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (c_proj): Linear(in_features=2048, out_features=512, bias=True)
    )
  )
  (text_encoder): TextEncoder(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (clip_model): CLIP_conv_proj(
    (visual): ModifiedResNet_conv_proj(
      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
      (relu): ReLU(inplace=True)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (attnpool): AttentionConv(
        (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (c_proj): Linear(in_features=2048, out_features=512, bias=True)
      )
    )
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (text_projector): Sequential(
    (0): Linear(in_features=512, out_features=384, bias=False)
    (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=384, out_features=256, bias=False)
  )
  (image_projector): Sequential(
    (0): Linear(in_features=512, out_features=256, bias=False)
  )
  (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 3
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
    SHUFFLE: False
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    PARTIAL_PORTION: 1.0
    PORTION: 1.0
    SAMPLER: RandomSampler
    SHUFFLE: True
  VAL:
    BATCH_SIZE: 100
    SHUFFLE: False
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  MASK_FILE: None
  NAME: coco
  NUM_LABELED: -1
  NUM_SHOTS: -1
  ROOT: /home/samyakr2/multilabel/data/coco14
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  TARGET_DOMAINS: ()
  TEST_GZSL_SPLIT: val2014
  TEST_SPLIT: val2014
  TRAIN_SPLIT: train2014
  VAL_GZSL_SPLIT: val2014
  VAL_PERCENT: 0.1
  VAL_SPLIT: val2014
  ZS_TEST: instances_val2014_gzsi_48_17.json
  ZS_TEST_UNSEEN: instances_val2014_unseen_48_17.json
  ZS_TRAIN: instances_train2014_seen_48_17.json
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (448, 448)
  TEST:
    SIZE: (448, 448)
  TRAIN:
    SIZE: (448, 448)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MLCCLIP:
  FLOAT: False
  NEGATIVE_PROMPT: 
  POSITIVE_PROMPT: 
MODEL:
  BACKBONE:
    NAME: RN101
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  ATTN_LR_MULT: 0.1
  BACKBONE_LR_MULT: 0.1
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 51
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: ./output
RESUME: 
SEED: -1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 100
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COOP_MLC:
    ASL_GAMMA_NEG: 2.0
    ASL_GAMMA_POS: 1.0
    CSC: True
    LS: 100.0
    NEGATIVE_PROMPT_INIT: 
    N_CTX_NEG: 16
    N_CTX_POS: 16
    POSITIVE_PROMPT_INIT: 
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FINETUNE: False
  FINETUNE_ATTN: False
  FINETUNE_BACKBONE: False
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: 
  RESNET_IMAGENET:
    DEPTH: 50
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
/home/samyakr2/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:836: UserWarning: non-inplace resize is deprecated
  warnings.warn("non-inplace resize is deprecated")
Train: [0/2566]	Time 1.891 (1.891)	Loss 46.86 (46.86)	mAP 12.87 (12.87)
Train: [100/2566]	Time 0.325 (0.337)	Loss 5.49 (8.66)	mAP 10.10 (10.89)
Train: [200/2566]	Time 0.327 (0.332)	Loss 6.48 (7.37)	mAP 12.73 (11.93)
Train: [300/2566]	Time 0.329 (0.331)	Loss 5.32 (6.82)	mAP 19.13 (12.82)
Train: [400/2566]	Time 0.329 (0.330)	Loss 5.40 (6.49)	mAP 17.29 (13.72)
Train: [500/2566]	Time 0.330 (0.330)	Loss 5.01 (6.26)	mAP 16.98 (14.55)
Train: [600/2566]	Time 0.331 (0.330)	Loss 5.41 (6.09)	mAP 20.84 (15.46)
Train: [700/2566]	Time 0.330 (0.330)	Loss 5.41 (5.95)	mAP 20.23 (16.32)
Train: [800/2566]	Time 0.330 (0.330)	Loss 5.07 (5.82)	mAP 25.73 (17.11)
Train: [900/2566]	Time 0.331 (0.330)	Loss 4.99 (5.71)	mAP 27.74 (17.88)
Train: [1000/2566]	Time 0.330 (0.330)	Loss 4.99 (5.61)	mAP 27.26 (18.57)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 4.10 (5.52)	mAP 25.13 (19.28)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 4.02 (5.43)	mAP 25.07 (19.96)
Train: [1300/2566]	Time 0.332 (0.331)	Loss 4.90 (5.36)	mAP 26.60 (20.61)
Train: [1400/2566]	Time 0.332 (0.331)	Loss 4.13 (5.28)	mAP 27.95 (21.21)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 4.19 (5.21)	mAP 29.33 (21.76)
Train: [1600/2566]	Time 0.333 (0.331)	Loss 3.86 (5.15)	mAP 27.67 (22.30)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 4.47 (5.09)	mAP 30.51 (22.81)
Train: [1800/2566]	Time 0.332 (0.331)	Loss 4.64 (5.04)	mAP 31.56 (23.28)
Train: [1900/2566]	Time 0.332 (0.331)	Loss 3.64 (4.99)	mAP 34.49 (23.75)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 4.27 (4.94)	mAP 38.56 (24.18)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 3.95 (4.89)	mAP 35.82 (24.63)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 4.00 (4.85)	mAP 36.83 (25.07)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 3.68 (4.80)	mAP 35.54 (25.46)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 3.68 (4.76)	mAP 30.47 (25.83)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 3.82 (4.72)	mAP 34.61 (26.16)
Train: [1/51]	Time 0.331	Loss 4.70 	mAP 26.40
Test: [0/402]	Time 1.844 (1.844)	Precision 53.67 (53.67)	Recall 47.73 (47.73) 	 P_C 17.90 	 R_C 15.80 	 F_C 15.13 	 P_O 53.67 	 R_O 47.73 	 F_O 50.53
Test: [100/402]	Time 0.500 (0.513)	Precision 51.13 (53.11)	Recall 44.44 (52.15) 	 P_C 36.66 	 R_C 33.98 	 F_C 32.59 	 P_O 53.57 	 R_O 51.76 	 F_O 52.65
Test: [200/402]	Time 0.500 (0.507)	Precision 71.36 (51.94)	Recall 63.64 (50.37) 	 P_C 38.31 	 R_C 34.41 	 F_C 33.77 	 P_O 52.76 	 R_O 50.39 	 F_O 51.55
Test: [300/402]	Time 0.500 (0.504)	Precision 67.43 (54.26)	Recall 51.00 (53.65) 	 P_C 42.88 	 R_C 40.51 	 F_C 39.64 	 P_O 54.22 	 R_O 53.22 	 F_O 53.71
Test: [400/402]	Time 0.499 (0.503)	Precision 32.49 (53.72)	Recall 37.21 (53.78) 	 P_C 42.71 	 R_C 41.78 	 F_C 40.63 	 P_O 53.67 	 R_O 53.94 	 F_O 53.80
Test: [1/51]	  P_C 42.71 	 R_C 41.77 	 F_C 40.62 	 P_O 53.65 	 R_O 53.93 	 F_O 53.79 	 mAP 40.82
Train: [0/2566]	Time 0.956 (0.956)	Loss 4.28 (4.28)	mAP 37.10 (37.10)
Train: [100/2566]	Time 0.331 (0.337)	Loss 6.31 (9.69)	mAP 8.11 (9.80)
Train: [200/2566]	Time 0.331 (0.334)	Loss 5.19 (7.75)	mAP 8.43 (10.09)
Train: [300/2566]	Time 0.332 (0.333)	Loss 5.82 (7.06)	mAP 9.55 (10.39)
Train: [400/2566]	Time 0.331 (0.333)	Loss 5.73 (6.74)	mAP 10.09 (10.65)
Train: [500/2566]	Time 0.330 (0.333)	Loss 6.11 (6.51)	mAP 12.31 (10.83)
Train: [600/2566]	Time 0.332 (0.332)	Loss 5.37 (6.35)	mAP 9.93 (10.98)
Train: [700/2566]	Time 0.329 (0.332)	Loss 5.42 (6.25)	mAP 14.14 (11.12)
Train: [800/2566]	Time 0.331 (0.332)	Loss 5.05 (6.16)	mAP 9.75 (11.20)
Train: [900/2566]	Time 0.330 (0.332)	Loss 6.05 (6.10)	mAP 14.79 (11.35)
Train: [1000/2566]	Time 0.333 (0.332)	Loss 5.55 (6.05)	mAP 8.76 (11.47)
Train: [1100/2566]	Time 0.330 (0.332)	Loss 5.40 (6.01)	mAP 13.24 (11.59)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 5.49 (5.97)	mAP 12.90 (11.72)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 6.01 (5.92)	mAP 12.03 (11.78)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 6.52 (5.90)	mAP 15.24 (11.90)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 5.48 (5.87)	mAP 13.55 (12.00)
Train: [1600/2566]	Time 0.332 (0.331)	Loss 5.23 (5.85)	mAP 12.20 (12.07)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 5.17 (5.83)	mAP 8.58 (12.15)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 5.61 (5.80)	mAP 13.68 (12.21)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 5.34 (5.79)	mAP 16.41 (12.30)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 5.46 (5.77)	mAP 9.12 (12.36)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 5.47 (5.75)	mAP 12.14 (12.41)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 5.03 (5.74)	mAP 11.27 (12.48)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 5.57 (5.72)	mAP 13.52 (12.54)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 4.89 (5.71)	mAP 15.37 (12.59)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 5.93 (5.70)	mAP 13.42 (12.64)
Train: [2/51]	Time 0.331	Loss 5.69 	mAP 12.67
Test: [0/402]	Time 1.719 (1.719)	Precision 60.78 (60.78)	Recall 17.61 (17.61) 	 P_C 0.78 	 R_C 1.25 	 F_C 0.96 	 P_O 60.78 	 R_O 17.61 	 F_O 27.31
Test: [100/402]	Time 0.498 (0.510)	Precision 14.63 (40.85)	Recall 7.84 (14.35) 	 P_C 2.07 	 R_C 1.54 	 F_C 1.17 	 P_O 41.09 	 R_O 14.68 	 F_O 21.63
Test: [200/402]	Time 0.498 (0.504)	Precision 69.70 (44.44)	Recall 29.87 (16.56) 	 P_C 2.13 	 R_C 1.43 	 F_C 1.08 	 P_O 44.68 	 R_O 16.96 	 F_O 24.59
Test: [300/402]	Time 0.498 (0.502)	Precision 97.03 (51.13)	Recall 24.38 (18.69) 	 P_C 2.36 	 R_C 1.43 	 F_C 1.16 	 P_O 51.45 	 R_O 18.61 	 F_O 27.33
Test: [400/402]	Time 0.497 (0.501)	Precision 22.77 (54.06)	Recall 13.37 (19.36) 	 P_C 2.74 	 R_C 1.46 	 F_C 1.24 	 P_O 54.34 	 R_O 19.17 	 F_O 28.35
Test: [2/51]	  P_C 2.74 	 R_C 1.46 	 F_C 1.24 	 P_O 54.30 	 R_O 19.17 	 F_O 28.33 	 mAP 8.36
Train: [0/2566]	Time 0.968 (0.968)	Loss 5.73 (5.73)	mAP 16.46 (16.46)
Train: [100/2566]	Time 0.332 (0.337)	Loss 5.22 (5.42)	mAP 18.11 (14.01)
Train: [200/2566]	Time 0.331 (0.334)	Loss 5.39 (5.42)	mAP 13.61 (13.86)
Train: [300/2566]	Time 0.331 (0.333)	Loss 5.35 (5.39)	mAP 11.08 (13.75)
Train: [400/2566]	Time 0.333 (0.333)	Loss 5.58 (5.38)	mAP 17.52 (13.88)
Train: [500/2566]	Time 0.330 (0.332)	Loss 4.93 (5.36)	mAP 14.42 (13.88)
Train: [600/2566]	Time 0.332 (0.332)	Loss 6.10 (5.36)	mAP 15.40 (13.95)
Train: [700/2566]	Time 0.332 (0.332)	Loss 4.79 (5.36)	mAP 12.87 (14.04)
Train: [800/2566]	Time 0.332 (0.332)	Loss 5.43 (5.36)	mAP 14.92 (14.10)
Train: [900/2566]	Time 0.330 (0.332)	Loss 4.94 (5.36)	mAP 14.71 (14.15)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 5.19 (5.35)	mAP 14.53 (14.20)
Train: [1100/2566]	Time 0.330 (0.332)	Loss 5.26 (5.35)	mAP 17.70 (14.27)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 6.47 (5.34)	mAP 15.88 (14.31)
Train: [1300/2566]	Time 0.330 (0.332)	Loss 4.72 (5.34)	mAP 14.41 (14.35)
Train: [1400/2566]	Time 0.331 (0.332)	Loss 5.13 (5.34)	mAP 14.20 (14.39)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 5.55 (5.34)	mAP 19.03 (14.49)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 4.34 (5.33)	mAP 13.66 (14.54)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 5.28 (5.32)	mAP 16.86 (14.61)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 4.87 (5.32)	mAP 12.54 (14.67)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 5.54 (5.31)	mAP 15.19 (14.73)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 5.42 (5.31)	mAP 16.36 (14.78)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 5.39 (5.30)	mAP 15.17 (14.84)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 5.29 (5.30)	mAP 15.82 (14.89)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 5.04 (5.29)	mAP 16.39 (14.94)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 4.60 (5.29)	mAP 16.67 (14.97)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 5.57 (5.28)	mAP 18.45 (15.03)
Train: [3/51]	Time 0.331	Loss 5.28 	mAP 15.07
Test: [0/402]	Time 1.754 (1.754)	Precision 66.67 (66.67)	Recall 18.75 (18.75) 	 P_C 3.99 	 R_C 2.12 	 F_C 2.34 	 P_O 66.67 	 R_O 18.75 	 F_O 29.27
Test: [100/402]	Time 0.497 (0.510)	Precision 64.71 (55.53)	Recall 7.19 (14.50) 	 P_C 8.06 	 R_C 2.43 	 F_C 2.55 	 P_O 53.60 	 R_O 15.73 	 F_O 24.33
Test: [200/402]	Time 0.499 (0.504)	Precision 100.00 (60.30)	Recall 24.68 (16.01) 	 P_C 8.17 	 R_C 2.10 	 F_C 2.31 	 P_O 60.03 	 R_O 17.09 	 F_O 26.61
Test: [300/402]	Time 0.498 (0.502)	Precision 93.69 (63.40)	Recall 25.87 (18.74) 	 P_C 8.60 	 R_C 2.15 	 F_C 2.38 	 P_O 64.13 	 R_O 19.50 	 F_O 29.91
Test: [400/402]	Time 0.499 (0.501)	Precision 30.14 (63.53)	Recall 12.79 (20.02) 	 P_C 8.10 	 R_C 2.32 	 F_C 2.59 	 P_O 64.26 	 R_O 20.80 	 F_O 31.43
Test: [3/51]	  P_C 8.10 	 R_C 2.32 	 F_C 2.59 	 P_O 64.23 	 R_O 20.80 	 F_O 31.42 	 mAP 9.96
Train: [0/2566]	Time 1.003 (1.003)	Loss 4.98 (4.98)	mAP 15.84 (15.84)
Train: [100/2566]	Time 0.330 (0.338)	Loss 4.81 (5.15)	mAP 16.12 (16.65)
Train: [200/2566]	Time 0.331 (0.335)	Loss 4.96 (5.12)	mAP 16.33 (16.74)
Train: [300/2566]	Time 0.331 (0.334)	Loss 5.28 (5.12)	mAP 22.07 (16.66)
Train: [400/2566]	Time 0.331 (0.333)	Loss 4.99 (5.10)	mAP 14.53 (16.65)
Train: [500/2566]	Time 0.332 (0.333)	Loss 4.27 (5.09)	mAP 15.48 (16.74)
Train: [600/2566]	Time 0.330 (0.332)	Loss 4.63 (5.09)	mAP 16.13 (16.76)
Train: [700/2566]	Time 0.332 (0.332)	Loss 4.80 (5.08)	mAP 15.05 (16.84)
Train: [800/2566]	Time 0.332 (0.332)	Loss 5.01 (5.09)	mAP 15.12 (16.84)
Train: [900/2566]	Time 0.331 (0.332)	Loss 4.98 (5.08)	mAP 15.30 (16.91)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 4.79 (5.07)	mAP 15.57 (16.97)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 4.42 (5.07)	mAP 18.15 (17.08)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 5.31 (5.06)	mAP 19.30 (17.21)
Train: [1300/2566]	Time 0.332 (0.332)	Loss 5.11 (5.05)	mAP 17.17 (17.36)
Train: [1400/2566]	Time 0.332 (0.332)	Loss 4.57 (5.03)	mAP 19.87 (17.54)
Train: [1500/2566]	Time 0.330 (0.332)	Loss 4.90 (5.02)	mAP 21.81 (17.77)
Train: [1600/2566]	Time 0.330 (0.332)	Loss 4.40 (5.01)	mAP 20.66 (17.98)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 4.83 (4.99)	mAP 19.86 (18.21)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 5.03 (4.97)	mAP 25.76 (18.41)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 4.43 (4.95)	mAP 29.67 (18.62)
Train: [2000/2566]	Time 0.332 (0.331)	Loss 5.64 (4.93)	mAP 24.70 (18.83)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 5.15 (4.92)	mAP 28.72 (19.03)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 3.88 (4.90)	mAP 20.10 (19.22)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 4.23 (4.88)	mAP 25.34 (19.42)
Train: [2400/2566]	Time 0.332 (0.331)	Loss 4.07 (4.86)	mAP 26.81 (19.60)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 4.35 (4.85)	mAP 23.73 (19.78)
Train: [4/51]	Time 0.331	Loss 4.84 	mAP 19.88
Test: [0/402]	Time 1.757 (1.757)	Precision 53.42 (53.42)	Recall 22.16 (22.16) 	 P_C 10.98 	 R_C 4.57 	 F_C 5.69 	 P_O 53.42 	 R_O 22.16 	 F_O 31.33
Test: [100/402]	Time 0.498 (0.511)	Precision 50.00 (50.30)	Recall 7.84 (20.39) 	 P_C 15.56 	 R_C 6.44 	 F_C 6.90 	 P_O 52.86 	 R_O 23.11 	 F_O 32.16
Test: [200/402]	Time 0.499 (0.505)	Precision 91.67 (54.06)	Recall 28.57 (20.58) 	 P_C 17.07 	 R_C 5.75 	 F_C 6.32 	 P_O 56.85 	 R_O 22.85 	 F_O 32.59
Test: [300/402]	Time 0.499 (0.503)	Precision 90.08 (58.76)	Recall 29.35 (23.14) 	 P_C 21.07 	 R_C 5.96 	 F_C 7.03 	 P_O 61.17 	 R_O 24.78 	 F_O 35.27
Test: [400/402]	Time 0.500 (0.502)	Precision 39.71 (60.06)	Recall 15.70 (24.40) 	 P_C 22.23 	 R_C 6.17 	 F_C 7.58 	 P_O 62.70 	 R_O 25.99 	 F_O 36.74
Test: [4/51]	  P_C 22.23 	 R_C 6.16 	 F_C 7.58 	 P_O 62.68 	 R_O 25.98 	 F_O 36.73 	 mAP 16.63
Train: [0/2566]	Time 1.021 (1.021)	Loss 4.84 (4.84)	mAP 25.91 (25.91)
Train: [100/2566]	Time 0.331 (0.338)	Loss 4.89 (4.46)	mAP 25.32 (24.80)
Train: [200/2566]	Time 0.332 (0.334)	Loss 4.54 (4.44)	mAP 23.28 (24.61)
Train: [300/2566]	Time 0.331 (0.333)	Loss 4.88 (4.44)	mAP 23.09 (24.63)
Train: [400/2566]	Time 0.330 (0.333)	Loss 4.52 (4.43)	mAP 28.66 (24.57)
Train: [500/2566]	Time 0.332 (0.332)	Loss 4.48 (4.43)	mAP 25.44 (24.65)
Train: [600/2566]	Time 0.332 (0.332)	Loss 4.11 (4.43)	mAP 21.77 (24.73)
Train: [700/2566]	Time 0.330 (0.332)	Loss 4.93 (4.43)	mAP 24.91 (24.83)
Train: [800/2566]	Time 0.330 (0.332)	Loss 4.37 (4.42)	mAP 19.34 (24.92)
Train: [900/2566]	Time 0.332 (0.332)	Loss 4.36 (4.42)	mAP 21.24 (25.06)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 4.23 (4.41)	mAP 31.85 (25.15)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 4.59 (4.40)	mAP 29.04 (25.20)
Train: [1200/2566]	Time 0.330 (0.332)	Loss 4.41 (4.39)	mAP 23.77 (25.29)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 4.41 (4.39)	mAP 25.00 (25.32)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 3.82 (4.38)	mAP 22.92 (25.36)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 5.05 (4.38)	mAP 26.00 (25.43)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 4.52 (4.38)	mAP 25.70 (25.52)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 4.16 (4.37)	mAP 25.27 (25.55)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 4.30 (4.37)	mAP 34.34 (25.64)
Train: [1900/2566]	Time 0.332 (0.331)	Loss 4.11 (4.36)	mAP 32.30 (25.74)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 3.97 (4.36)	mAP 29.37 (25.81)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 3.79 (4.35)	mAP 23.88 (25.87)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 3.82 (4.35)	mAP 28.32 (25.93)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 4.03 (4.34)	mAP 27.08 (25.99)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 4.23 (4.34)	mAP 29.10 (26.06)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 3.98 (4.33)	mAP 29.75 (26.14)
Train: [5/51]	Time 0.331	Loss 4.33 	mAP 26.16
Test: [0/402]	Time 1.769 (1.769)	Precision 45.11 (45.11)	Recall 30.11 (30.11) 	 P_C 12.76 	 R_C 9.96 	 F_C 10.01 	 P_O 45.11 	 R_O 30.11 	 F_O 36.12
Test: [100/402]	Time 0.499 (0.511)	Precision 43.33 (48.99)	Recall 8.50 (33.70) 	 P_C 19.02 	 R_C 17.07 	 F_C 15.36 	 P_O 48.16 	 R_O 37.65 	 F_O 42.26
Test: [200/402]	Time 0.499 (0.505)	Precision 72.38 (48.52)	Recall 32.90 (30.50) 	 P_C 20.69 	 R_C 16.07 	 F_C 14.76 	 P_O 48.93 	 R_O 34.18 	 F_O 40.24
Test: [300/402]	Time 0.499 (0.503)	Precision 65.86 (49.68)	Recall 47.51 (34.34) 	 P_C 24.75 	 R_C 18.89 	 F_C 17.77 	 P_O 49.80 	 R_O 37.47 	 F_O 42.76
Test: [400/402]	Time 0.499 (0.502)	Precision 36.46 (49.70)	Recall 20.35 (36.26) 	 P_C 25.33 	 R_C 19.13 	 F_C 17.99 	 P_O 50.52 	 R_O 39.33 	 F_O 44.23
Test: [5/51]	  P_C 25.33 	 R_C 19.13 	 F_C 17.99 	 P_O 50.52 	 R_O 39.32 	 F_O 44.22 	 mAP 21.54
Train: [0/2566]	Time 1.017 (1.017)	Loss 4.36 (4.36)	mAP 26.30 (26.30)
Train: [100/2566]	Time 0.331 (0.338)	Loss 4.46 (4.18)	mAP 27.48 (28.66)
Train: [200/2566]	Time 0.330 (0.334)	Loss 4.37 (4.20)	mAP 29.50 (28.37)
Train: [300/2566]	Time 0.331 (0.333)	Loss 4.43 (4.19)	mAP 26.16 (28.40)
Train: [400/2566]	Time 0.331 (0.332)	Loss 4.19 (4.19)	mAP 31.28 (28.32)
Train: [500/2566]	Time 0.330 (0.332)	Loss 3.96 (4.19)	mAP 25.66 (28.24)
Train: [600/2566]	Time 0.331 (0.332)	Loss 4.42 (4.18)	mAP 22.66 (28.24)
Train: [700/2566]	Time 0.331 (0.332)	Loss 3.46 (4.18)	mAP 30.15 (28.35)
Train: [800/2566]	Time 0.330 (0.332)	Loss 4.54 (4.18)	mAP 33.06 (28.45)
Train: [900/2566]	Time 0.330 (0.331)	Loss 4.01 (4.17)	mAP 30.91 (28.54)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 4.27 (4.17)	mAP 27.15 (28.60)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 3.20 (4.17)	mAP 33.66 (28.72)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 4.21 (4.17)	mAP 33.22 (28.77)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 3.96 (4.16)	mAP 31.00 (28.86)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 4.01 (4.15)	mAP 31.29 (28.92)
Train: [1500/2566]	Time 0.332 (0.331)	Loss 4.50 (4.14)	mAP 31.93 (28.99)
Train: [1600/2566]	Time 0.333 (0.331)	Loss 3.85 (4.14)	mAP 28.07 (29.05)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 4.69 (4.14)	mAP 35.85 (29.09)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 3.81 (4.13)	mAP 33.31 (29.15)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 4.19 (4.13)	mAP 29.44 (29.20)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 4.63 (4.13)	mAP 29.45 (29.21)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 3.10 (4.12)	mAP 28.95 (29.25)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 4.02 (4.11)	mAP 32.57 (29.31)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 4.11 (4.11)	mAP 32.15 (29.36)
Train: [2400/2566]	Time 0.332 (0.331)	Loss 4.37 (4.11)	mAP 28.73 (29.43)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 4.15 (4.10)	mAP 30.42 (29.49)
Train: [6/51]	Time 0.331	Loss 4.10 	mAP 29.53
Test: [0/402]	Time 1.711 (1.711)	Precision 39.94 (39.94)	Recall 36.08 (36.08) 	 P_C 11.65 	 R_C 12.86 	 F_C 10.77 	 P_O 39.94 	 R_O 36.08 	 F_O 37.91
Test: [100/402]	Time 0.500 (0.511)	Precision 27.87 (42.93)	Recall 11.11 (46.14) 	 P_C 21.76 	 R_C 28.78 	 F_C 21.66 	 P_O 42.18 	 R_O 50.01 	 F_O 45.76
Test: [200/402]	Time 0.499 (0.506)	Precision 52.80 (41.46)	Recall 36.80 (39.73) 	 P_C 22.61 	 R_C 27.06 	 F_C 20.75 	 P_O 42.32 	 R_O 43.54 	 F_O 42.92
Test: [300/402]	Time 0.500 (0.504)	Precision 57.51 (44.28)	Recall 49.50 (43.99) 	 P_C 25.34 	 R_C 30.79 	 F_C 24.79 	 P_O 43.73 	 R_O 47.25 	 F_O 45.42
Test: [400/402]	Time 0.499 (0.503)	Precision 25.32 (44.35)	Recall 22.67 (45.89) 	 P_C 25.40 	 R_C 30.84 	 F_C 25.04 	 P_O 43.94 	 R_O 49.33 	 F_O 46.48
Test: [6/51]	  P_C 25.39 	 R_C 30.83 	 F_C 25.03 	 P_O 43.93 	 R_O 49.31 	 F_O 46.46 	 mAP 26.13
Train: [0/2566]	Time 0.982 (0.982)	Loss 4.13 (4.13)	mAP 32.68 (32.68)
Train: [100/2566]	Time 0.330 (0.338)	Loss 3.92 (4.00)	mAP 30.83 (30.96)
Train: [200/2566]	Time 0.331 (0.334)	Loss 3.79 (4.01)	mAP 30.05 (31.13)
Train: [300/2566]	Time 0.331 (0.333)	Loss 3.83 (4.00)	mAP 28.71 (31.20)
Train: [400/2566]	Time 0.331 (0.333)	Loss 4.16 (3.99)	mAP 35.56 (31.10)
Train: [500/2566]	Time 0.330 (0.332)	Loss 3.95 (3.98)	mAP 33.83 (31.15)
Train: [600/2566]	Time 0.331 (0.332)	Loss 3.19 (3.97)	mAP 27.97 (31.17)
Train: [700/2566]	Time 0.331 (0.332)	Loss 3.55 (3.97)	mAP 31.87 (31.19)
Train: [800/2566]	Time 0.330 (0.332)	Loss 4.26 (3.98)	mAP 35.72 (31.24)
Train: [900/2566]	Time 0.331 (0.332)	Loss 3.54 (3.98)	mAP 31.40 (31.30)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 3.80 (3.97)	mAP 30.41 (31.42)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 3.96 (3.97)	mAP 33.40 (31.41)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 3.47 (3.97)	mAP 32.39 (31.50)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 3.17 (3.97)	mAP 25.60 (31.56)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 4.12 (3.96)	mAP 36.03 (31.57)
Train: [1500/2566]	Time 0.332 (0.331)	Loss 3.92 (3.96)	mAP 31.49 (31.59)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 3.77 (3.95)	mAP 33.12 (31.61)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 4.11 (3.95)	mAP 30.38 (31.67)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 3.68 (3.95)	mAP 32.03 (31.71)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 3.93 (3.94)	mAP 29.15 (31.77)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 4.08 (3.94)	mAP 35.83 (31.80)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 4.56 (3.93)	mAP 29.21 (31.86)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 3.54 (3.92)	mAP 33.71 (31.91)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 4.16 (3.92)	mAP 34.57 (31.97)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 4.38 (3.92)	mAP 31.94 (32.02)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 3.60 (3.91)	mAP 34.17 (32.07)
Train: [7/51]	Time 0.331	Loss 3.91 	mAP 32.11
Test: [0/402]	Time 1.721 (1.721)	Precision 50.74 (50.74)	Recall 29.26 (29.26) 	 P_C 12.50 	 R_C 11.11 	 F_C 10.73 	 P_O 50.74 	 R_O 29.26 	 F_O 37.12
Test: [100/402]	Time 0.499 (0.511)	Precision 26.19 (54.97)	Recall 7.19 (39.70) 	 P_C 32.07 	 R_C 23.54 	 F_C 23.08 	 P_O 55.67 	 R_O 42.58 	 F_O 48.26
Test: [200/402]	Time 0.500 (0.505)	Precision 74.51 (54.43)	Recall 32.90 (33.81) 	 P_C 38.28 	 R_C 22.50 	 F_C 22.53 	 P_O 56.61 	 R_O 36.76 	 F_O 44.58
Test: [300/402]	Time 0.500 (0.504)	Precision 83.41 (57.37)	Recall 42.54 (38.45) 	 P_C 39.05 	 R_C 25.77 	 F_C 26.02 	 P_O 57.77 	 R_O 40.61 	 F_O 47.70
Test: [400/402]	Time 0.500 (0.503)	Precision 33.04 (57.92)	Recall 21.51 (40.47) 	 P_C 39.87 	 R_C 26.02 	 F_C 26.69 	 P_O 58.19 	 R_O 43.00 	 F_O 49.45
Test: [7/51]	  P_C 39.87 	 R_C 26.01 	 F_C 26.68 	 P_O 58.18 	 R_O 42.98 	 F_O 49.44 	 mAP 32.82
Train: [0/2566]	Time 0.973 (0.973)	Loss 3.86 (3.86)	mAP 31.46 (31.46)
Train: [100/2566]	Time 0.331 (0.337)	Loss 3.74 (3.73)	mAP 29.60 (33.36)
Train: [200/2566]	Time 0.330 (0.334)	Loss 3.69 (3.77)	mAP 32.88 (33.64)
Train: [300/2566]	Time 0.331 (0.333)	Loss 3.47 (3.76)	mAP 36.74 (33.88)
Train: [400/2566]	Time 0.331 (0.332)	Loss 3.94 (3.76)	mAP 32.55 (33.99)
Train: [500/2566]	Time 0.331 (0.332)	Loss 3.83 (3.76)	mAP 40.22 (34.19)
Train: [600/2566]	Time 0.330 (0.332)	Loss 3.91 (3.76)	mAP 29.55 (34.24)
Train: [700/2566]	Time 0.331 (0.332)	Loss 3.96 (3.74)	mAP 32.74 (34.25)
Train: [800/2566]	Time 0.330 (0.332)	Loss 4.04 (3.74)	mAP 36.66 (34.28)
Train: [900/2566]	Time 0.330 (0.331)	Loss 3.34 (3.73)	mAP 38.84 (34.32)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 3.60 (3.73)	mAP 33.08 (34.45)
Train: [1100/2566]	Time 0.332 (0.331)	Loss 3.45 (3.73)	mAP 33.20 (34.54)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 3.29 (3.73)	mAP 30.51 (34.69)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 3.73 (3.72)	mAP 43.47 (34.79)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 3.41 (3.71)	mAP 37.19 (34.82)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 3.57 (3.70)	mAP 37.82 (34.90)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 3.58 (3.70)	mAP 32.99 (34.99)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 3.38 (3.69)	mAP 36.17 (35.07)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 3.36 (3.68)	mAP 35.74 (35.14)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 3.69 (3.68)	mAP 38.89 (35.27)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 3.22 (3.67)	mAP 35.90 (35.36)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 3.27 (3.67)	mAP 33.22 (35.45)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 3.75 (3.66)	mAP 36.26 (35.56)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 4.21 (3.66)	mAP 39.90 (35.65)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 3.21 (3.65)	mAP 41.74 (35.72)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 4.01 (3.65)	mAP 36.06 (35.81)
Train: [8/51]	Time 0.331	Loss 3.64 	mAP 35.86
Test: [0/402]	Time 1.735 (1.735)	Precision 48.64 (48.64)	Recall 35.51 (35.51) 	 P_C 14.67 	 R_C 9.24 	 F_C 8.80 	 P_O 48.64 	 R_O 35.51 	 F_O 41.05
Test: [100/402]	Time 0.500 (0.512)	Precision 40.30 (52.23)	Recall 17.65 (50.63) 	 P_C 42.79 	 R_C 29.86 	 F_C 28.99 	 P_O 51.71 	 R_O 52.07 	 F_O 51.89
Test: [200/402]	Time 0.500 (0.506)	Precision 64.04 (51.14)	Recall 63.20 (48.94) 	 P_C 44.79 	 R_C 29.86 	 F_C 29.86 	 P_O 50.67 	 R_O 50.29 	 F_O 50.48
Test: [300/402]	Time 0.499 (0.504)	Precision 81.50 (55.35)	Recall 46.02 (49.68) 	 P_C 47.83 	 R_C 33.49 	 F_C 34.09 	 P_O 55.06 	 R_O 49.62 	 F_O 52.20
Test: [400/402]	Time 0.500 (0.503)	Precision 41.76 (57.16)	Recall 22.09 (49.60) 	 P_C 48.66 	 R_C 34.29 	 F_C 35.41 	 P_O 57.48 	 R_O 49.98 	 F_O 53.47
Test: [8/51]	  P_C 48.65 	 R_C 34.28 	 F_C 35.41 	 P_O 57.48 	 R_O 49.96 	 F_O 53.46 	 mAP 42.94
Train: [0/2566]	Time 1.000 (1.000)	Loss 3.55 (3.55)	mAP 45.67 (45.67)
Train: [100/2566]	Time 0.331 (0.337)	Loss 4.28 (3.47)	mAP 37.21 (37.46)
Train: [200/2566]	Time 0.330 (0.334)	Loss 3.63 (3.49)	mAP 39.17 (38.06)
Train: [300/2566]	Time 0.332 (0.333)	Loss 3.41 (3.48)	mAP 34.74 (37.95)
Train: [400/2566]	Time 0.331 (0.333)	Loss 3.12 (3.49)	mAP 40.55 (38.13)
Train: [500/2566]	Time 0.331 (0.332)	Loss 3.63 (3.47)	mAP 32.49 (38.25)
Train: [600/2566]	Time 0.331 (0.332)	Loss 3.25 (3.46)	mAP 30.85 (38.19)
Train: [700/2566]	Time 0.331 (0.332)	Loss 3.27 (3.45)	mAP 30.03 (38.22)
Train: [800/2566]	Time 0.330 (0.332)	Loss 3.30 (3.45)	mAP 37.57 (38.29)
Train: [900/2566]	Time 0.331 (0.332)	Loss 3.26 (3.44)	mAP 41.44 (38.39)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 3.24 (3.44)	mAP 36.89 (38.41)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 3.72 (3.43)	mAP 38.87 (38.48)
Train: [1200/2566]	Time 0.332 (0.332)	Loss 2.72 (3.43)	mAP 41.04 (38.55)
Train: [1300/2566]	Time 0.332 (0.332)	Loss 2.71 (3.42)	mAP 52.65 (38.59)
Train: [1400/2566]	Time 0.330 (0.332)	Loss 3.32 (3.42)	mAP 41.32 (38.64)
Train: [1500/2566]	Time 0.331 (0.332)	Loss 3.31 (3.41)	mAP 38.68 (38.68)
Train: [1600/2566]	Time 0.332 (0.332)	Loss 3.73 (3.41)	mAP 34.33 (38.77)
Train: [1700/2566]	Time 0.331 (0.332)	Loss 3.49 (3.41)	mAP 47.96 (38.81)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 3.02 (3.40)	mAP 39.03 (38.89)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 3.96 (3.40)	mAP 39.28 (38.98)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 3.58 (3.39)	mAP 33.07 (39.07)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 3.47 (3.39)	mAP 42.54 (39.14)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 3.93 (3.38)	mAP 39.93 (39.17)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 3.09 (3.38)	mAP 35.43 (39.20)
Train: [2400/2566]	Time 0.333 (0.331)	Loss 3.65 (3.37)	mAP 37.85 (39.24)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 3.51 (3.37)	mAP 42.03 (39.30)
Train: [9/51]	Time 0.331	Loss 3.37 	mAP 39.33
Test: [0/402]	Time 1.805 (1.805)	Precision 55.70 (55.70)	Recall 47.16 (47.16) 	 P_C 17.54 	 R_C 15.38 	 F_C 14.80 	 P_O 55.70 	 R_O 47.16 	 F_O 51.08
Test: [100/402]	Time 0.500 (0.512)	Precision 68.75 (56.92)	Recall 28.76 (60.82) 	 P_C 44.27 	 R_C 45.02 	 F_C 39.47 	 P_O 54.47 	 R_O 62.02 	 F_O 58.00
Test: [200/402]	Time 0.500 (0.506)	Precision 82.07 (57.85)	Recall 65.37 (57.89) 	 P_C 46.65 	 R_C 45.85 	 F_C 40.76 	 P_O 55.82 	 R_O 59.54 	 F_O 57.62
Test: [300/402]	Time 0.502 (0.504)	Precision 65.11 (57.74)	Recall 69.15 (62.52) 	 P_C 50.12 	 R_C 52.82 	 F_C 46.18 	 P_O 53.92 	 R_O 63.92 	 F_O 58.49
Test: [400/402]	Time 0.500 (0.503)	Precision 33.85 (56.50)	Recall 50.58 (64.26) 	 P_C 52.72 	 R_C 53.71 	 F_C 47.93 	 P_O 52.79 	 R_O 66.04 	 F_O 58.68
Test: [9/51]	  P_C 52.73 	 R_C 53.70 	 F_C 47.92 	 P_O 52.77 	 R_O 66.03 	 F_O 58.66 	 mAP 51.80
Train: [0/2566]	Time 0.989 (0.989)	Loss 3.46 (3.46)	mAP 43.55 (43.55)
Train: [100/2566]	Time 0.332 (0.338)	Loss 2.69 (3.24)	mAP 35.78 (40.59)
Train: [200/2566]	Time 0.331 (0.335)	Loss 3.19 (3.25)	mAP 39.16 (40.73)
Train: [300/2566]	Time 0.330 (0.334)	Loss 3.50 (3.23)	mAP 40.23 (40.82)
Train: [400/2566]	Time 0.330 (0.333)	Loss 3.51 (3.25)	mAP 39.83 (40.71)
Train: [500/2566]	Time 0.331 (0.333)	Loss 2.75 (3.24)	mAP 33.87 (40.85)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.57 (3.23)	mAP 41.44 (40.91)
Train: [700/2566]	Time 0.331 (0.332)	Loss 3.12 (3.23)	mAP 43.48 (41.01)
Train: [800/2566]	Time 0.331 (0.332)	Loss 3.16 (3.22)	mAP 41.95 (41.08)
Train: [900/2566]	Time 0.332 (0.332)	Loss 3.26 (3.21)	mAP 43.07 (41.08)
Train: [1000/2566]	Time 0.332 (0.332)	Loss 3.71 (3.20)	mAP 42.95 (41.03)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 3.34 (3.20)	mAP 39.02 (41.12)
Train: [1200/2566]	Time 0.332 (0.332)	Loss 3.26 (3.19)	mAP 40.91 (41.07)
Train: [1300/2566]	Time 0.332 (0.332)	Loss 2.51 (3.19)	mAP 37.76 (41.13)
Train: [1400/2566]	Time 0.332 (0.332)	Loss 3.29 (3.19)	mAP 36.56 (41.15)
Train: [1500/2566]	Time 0.331 (0.332)	Loss 3.20 (3.19)	mAP 37.66 (41.23)
Train: [1600/2566]	Time 0.331 (0.332)	Loss 3.00 (3.18)	mAP 36.06 (41.27)
Train: [1700/2566]	Time 0.331 (0.332)	Loss 3.48 (3.18)	mAP 43.95 (41.30)
Train: [1800/2566]	Time 0.331 (0.332)	Loss 3.11 (3.18)	mAP 42.57 (41.34)
Train: [1900/2566]	Time 0.330 (0.332)	Loss 3.16 (3.18)	mAP 45.39 (41.33)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 2.66 (3.18)	mAP 45.50 (41.39)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 2.88 (3.17)	mAP 41.47 (41.43)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.82 (3.17)	mAP 41.99 (41.46)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 3.19 (3.16)	mAP 41.32 (41.52)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.71 (3.16)	mAP 43.50 (41.57)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 3.46 (3.16)	mAP 35.88 (41.60)
Train: [10/51]	Time 0.331	Loss 3.16 	mAP 41.61
Test: [0/402]	Time 1.717 (1.717)	Precision 50.12 (50.12)	Recall 58.81 (58.81) 	 P_C 17.65 	 R_C 20.25 	 F_C 17.07 	 P_O 50.12 	 R_O 58.81 	 F_O 54.12
Test: [100/402]	Time 0.500 (0.512)	Precision 56.74 (51.77)	Recall 66.01 (73.60) 	 P_C 40.51 	 R_C 59.94 	 F_C 44.71 	 P_O 50.79 	 R_O 73.22 	 F_O 59.98
Test: [200/402]	Time 0.500 (0.506)	Precision 80.91 (53.05)	Recall 77.06 (72.12) 	 P_C 41.43 	 R_C 60.03 	 F_C 46.76 	 P_O 51.68 	 R_O 71.91 	 F_O 60.14
Test: [300/402]	Time 0.500 (0.504)	Precision 69.10 (54.02)	Recall 72.89 (73.90) 	 P_C 45.42 	 R_C 67.57 	 F_C 52.62 	 P_O 52.05 	 R_O 73.50 	 F_O 60.94
Test: [400/402]	Time 0.501 (0.503)	Precision 32.68 (53.76)	Recall 58.14 (74.57) 	 P_C 46.79 	 R_C 69.06 	 F_C 54.43 	 P_O 51.87 	 R_O 74.45 	 F_O 61.14
Test: [10/51]	  P_C 46.79 	 R_C 69.07 	 F_C 54.44 	 P_O 51.86 	 R_O 74.45 	 F_O 61.14 	 mAP 58.17
Train: [0/2566]	Time 0.925 (0.925)	Loss 3.55 (3.55)	mAP 46.17 (46.17)
Train: [100/2566]	Time 0.331 (0.337)	Loss 2.88 (3.04)	mAP 37.72 (42.92)
Train: [200/2566]	Time 0.331 (0.334)	Loss 3.24 (3.04)	mAP 42.58 (42.68)
Train: [300/2566]	Time 0.332 (0.333)	Loss 3.03 (3.04)	mAP 42.85 (42.74)
Train: [400/2566]	Time 0.331 (0.333)	Loss 3.34 (3.04)	mAP 48.71 (42.65)
Train: [500/2566]	Time 0.330 (0.332)	Loss 3.66 (3.03)	mAP 33.49 (42.63)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.96 (3.04)	mAP 44.42 (42.67)
Train: [700/2566]	Time 0.332 (0.332)	Loss 3.03 (3.02)	mAP 44.97 (42.74)
Train: [800/2566]	Time 0.330 (0.332)	Loss 2.82 (3.03)	mAP 48.45 (42.74)
Train: [900/2566]	Time 0.330 (0.332)	Loss 2.98 (3.03)	mAP 39.98 (42.75)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 2.68 (3.02)	mAP 48.26 (42.66)
Train: [1100/2566]	Time 0.330 (0.332)	Loss 3.07 (3.02)	mAP 42.57 (42.65)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 3.38 (3.03)	mAP 40.83 (42.67)
Train: [1300/2566]	Time 0.332 (0.331)	Loss 2.51 (3.02)	mAP 33.87 (42.65)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 3.98 (3.03)	mAP 42.37 (42.62)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.77 (3.02)	mAP 43.49 (42.61)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 2.76 (3.02)	mAP 50.38 (42.66)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 3.00 (3.02)	mAP 49.53 (42.67)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 3.00 (3.02)	mAP 37.85 (42.68)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.72 (3.02)	mAP 44.23 (42.67)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 2.87 (3.02)	mAP 38.94 (42.66)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 3.13 (3.02)	mAP 44.11 (42.71)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 3.25 (3.01)	mAP 50.64 (42.76)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 2.97 (3.01)	mAP 38.60 (42.75)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.72 (3.01)	mAP 46.21 (42.76)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 3.53 (3.01)	mAP 38.97 (42.77)
Train: [11/51]	Time 0.331	Loss 3.01 	mAP 42.79
Test: [0/402]	Time 1.768 (1.768)	Precision 57.51 (57.51)	Recall 56.53 (56.53) 	 P_C 18.45 	 R_C 17.09 	 F_C 16.23 	 P_O 57.51 	 R_O 56.53 	 F_O 57.02
Test: [100/402]	Time 0.499 (0.512)	Precision 80.53 (61.84)	Recall 59.48 (69.56) 	 P_C 51.63 	 R_C 48.90 	 F_C 46.63 	 P_O 59.49 	 R_O 68.75 	 F_O 63.78
Test: [200/402]	Time 0.500 (0.506)	Precision 82.43 (61.63)	Recall 79.22 (69.39) 	 P_C 54.26 	 R_C 49.63 	 F_C 48.90 	 P_O 59.57 	 R_O 69.08 	 F_O 63.97
Test: [300/402]	Time 0.499 (0.504)	Precision 73.77 (62.79)	Recall 70.65 (69.80) 	 P_C 59.45 	 R_C 57.07 	 F_C 55.35 	 P_O 60.58 	 R_O 69.29 	 F_O 64.64
Test: [400/402]	Time 0.500 (0.503)	Precision 49.27 (62.21)	Recall 58.72 (70.53) 	 P_C 61.22 	 R_C 58.73 	 F_C 57.00 	 P_O 60.21 	 R_O 70.27 	 F_O 64.85
Test: [11/51]	  P_C 61.22 	 R_C 58.73 	 F_C 56.99 	 P_O 60.20 	 R_O 70.26 	 F_O 64.84 	 mAP 62.06
Train: [0/2566]	Time 1.017 (1.017)	Loss 3.12 (3.12)	mAP 38.10 (38.10)
Train: [100/2566]	Time 0.332 (0.338)	Loss 2.65 (3.01)	mAP 40.65 (43.66)
Train: [200/2566]	Time 0.330 (0.334)	Loss 2.84 (2.95)	mAP 41.73 (43.74)
Train: [300/2566]	Time 0.331 (0.333)	Loss 2.59 (2.93)	mAP 50.14 (43.71)
Train: [400/2566]	Time 0.331 (0.333)	Loss 3.01 (2.92)	mAP 40.01 (43.60)
Train: [500/2566]	Time 0.332 (0.333)	Loss 3.17 (2.93)	mAP 41.05 (43.45)
Train: [600/2566]	Time 0.332 (0.332)	Loss 3.86 (2.94)	mAP 36.06 (43.46)
Train: [700/2566]	Time 0.332 (0.332)	Loss 3.46 (2.92)	mAP 46.69 (43.50)
Train: [800/2566]	Time 0.332 (0.332)	Loss 2.51 (2.92)	mAP 40.69 (43.49)
Train: [900/2566]	Time 0.332 (0.332)	Loss 3.42 (2.92)	mAP 46.47 (43.45)
Train: [1000/2566]	Time 0.332 (0.332)	Loss 2.59 (2.92)	mAP 48.30 (43.50)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 2.92 (2.91)	mAP 40.05 (43.49)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 2.63 (2.91)	mAP 38.32 (43.48)
Train: [1300/2566]	Time 0.332 (0.332)	Loss 2.90 (2.91)	mAP 44.94 (43.55)
Train: [1400/2566]	Time 0.330 (0.332)	Loss 2.33 (2.91)	mAP 39.31 (43.60)
Train: [1500/2566]	Time 0.332 (0.332)	Loss 2.75 (2.91)	mAP 43.01 (43.63)
Train: [1600/2566]	Time 0.332 (0.332)	Loss 3.08 (2.91)	mAP 49.73 (43.61)
Train: [1700/2566]	Time 0.331 (0.332)	Loss 2.35 (2.91)	mAP 44.59 (43.63)
Train: [1800/2566]	Time 0.330 (0.332)	Loss 3.16 (2.91)	mAP 38.55 (43.65)
Train: [1900/2566]	Time 0.332 (0.332)	Loss 3.06 (2.91)	mAP 39.51 (43.63)
Train: [2000/2566]	Time 0.330 (0.332)	Loss 2.54 (2.91)	mAP 49.61 (43.64)
Train: [2100/2566]	Time 0.330 (0.332)	Loss 2.92 (2.90)	mAP 49.97 (43.65)
Train: [2200/2566]	Time 0.331 (0.332)	Loss 3.24 (2.91)	mAP 40.19 (43.65)
Train: [2300/2566]	Time 0.331 (0.332)	Loss 3.57 (2.90)	mAP 41.23 (43.66)
Train: [2400/2566]	Time 0.330 (0.332)	Loss 3.13 (2.90)	mAP 40.42 (43.68)
Train: [2500/2566]	Time 0.331 (0.332)	Loss 3.37 (2.90)	mAP 39.59 (43.72)
Train: [12/51]	Time 0.331	Loss 2.90 	mAP 43.73
Test: [0/402]	Time 1.777 (1.777)	Precision 68.70 (68.70)	Recall 51.14 (51.14) 	 P_C 20.03 	 R_C 13.57 	 F_C 15.36 	 P_O 68.70 	 R_O 51.14 	 F_O 58.63
Test: [100/402]	Time 0.500 (0.512)	Precision 90.65 (71.70)	Recall 63.40 (66.11) 	 P_C 57.56 	 R_C 48.53 	 F_C 48.73 	 P_O 68.09 	 R_O 65.21 	 F_O 66.62
Test: [200/402]	Time 0.500 (0.506)	Precision 89.27 (73.22)	Recall 68.40 (65.12) 	 P_C 59.33 	 R_C 49.81 	 F_C 51.24 	 P_O 70.37 	 R_O 64.33 	 F_O 67.21
Test: [300/402]	Time 0.500 (0.504)	Precision 78.63 (71.83)	Recall 68.66 (67.17) 	 P_C 62.55 	 R_C 57.90 	 F_C 57.35 	 P_O 67.79 	 R_O 66.55 	 F_O 67.16
Test: [400/402]	Time 0.500 (0.503)	Precision 51.38 (70.11)	Recall 54.07 (68.69) 	 P_C 65.07 	 R_C 59.90 	 F_C 59.61 	 P_O 66.19 	 R_O 68.48 	 F_O 67.31
Test: [12/51]	  P_C 65.07 	 R_C 59.89 	 F_C 59.60 	 P_O 66.18 	 R_O 68.48 	 F_O 67.31 	 mAP 64.79
Train: [0/2566]	Time 1.040 (1.040)	Loss 2.94 (2.94)	mAP 43.36 (43.36)
Train: [100/2566]	Time 0.330 (0.338)	Loss 2.62 (2.86)	mAP 43.34 (43.95)
Train: [200/2566]	Time 0.331 (0.334)	Loss 3.13 (2.85)	mAP 43.09 (44.22)
Train: [300/2566]	Time 0.332 (0.333)	Loss 2.82 (2.85)	mAP 45.05 (43.94)
Train: [400/2566]	Time 0.331 (0.333)	Loss 3.18 (2.85)	mAP 33.95 (43.93)
Train: [500/2566]	Time 0.332 (0.333)	Loss 2.67 (2.85)	mAP 38.43 (43.78)
Train: [600/2566]	Time 0.330 (0.332)	Loss 3.40 (2.86)	mAP 45.91 (43.90)
Train: [700/2566]	Time 0.331 (0.332)	Loss 2.74 (2.85)	mAP 46.19 (43.92)
Train: [800/2566]	Time 0.331 (0.332)	Loss 3.22 (2.85)	mAP 45.78 (44.01)
Train: [900/2566]	Time 0.331 (0.332)	Loss 3.21 (2.85)	mAP 42.71 (44.00)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 2.56 (2.85)	mAP 48.00 (44.07)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 2.73 (2.84)	mAP 43.90 (44.08)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 2.80 (2.84)	mAP 43.74 (44.13)
Train: [1300/2566]	Time 0.332 (0.332)	Loss 2.71 (2.84)	mAP 40.76 (44.19)
Train: [1400/2566]	Time 0.332 (0.332)	Loss 3.07 (2.84)	mAP 46.74 (44.20)
Train: [1500/2566]	Time 0.332 (0.332)	Loss 2.67 (2.83)	mAP 43.82 (44.26)
Train: [1600/2566]	Time 0.332 (0.332)	Loss 2.86 (2.83)	mAP 40.24 (44.27)
Train: [1700/2566]	Time 0.330 (0.332)	Loss 2.62 (2.82)	mAP 40.03 (44.24)
Train: [1800/2566]	Time 0.330 (0.332)	Loss 2.67 (2.82)	mAP 43.94 (44.25)
Train: [1900/2566]	Time 0.331 (0.332)	Loss 2.72 (2.82)	mAP 47.99 (44.25)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 2.85 (2.82)	mAP 42.92 (44.27)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.64 (2.82)	mAP 44.51 (44.31)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.86 (2.82)	mAP 46.58 (44.36)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.98 (2.82)	mAP 45.24 (44.39)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.89 (2.82)	mAP 42.39 (44.38)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 3.16 (2.82)	mAP 42.72 (44.40)
Train: [13/51]	Time 0.331	Loss 2.82 	mAP 44.39
Test: [0/402]	Time 1.779 (1.779)	Precision 64.67 (64.67)	Recall 58.24 (58.24) 	 P_C 19.94 	 R_C 17.83 	 F_C 17.74 	 P_O 64.67 	 R_O 58.24 	 F_O 61.29
Test: [100/402]	Time 0.500 (0.512)	Precision 90.27 (66.55)	Recall 66.67 (71.42) 	 P_C 56.39 	 R_C 51.65 	 F_C 51.22 	 P_O 63.36 	 R_O 71.03 	 F_O 66.98
Test: [200/402]	Time 0.500 (0.506)	Precision 84.69 (68.74)	Recall 71.86 (70.00) 	 P_C 59.96 	 R_C 52.97 	 F_C 53.37 	 P_O 65.69 	 R_O 70.06 	 F_O 67.80
Test: [300/402]	Time 0.500 (0.504)	Precision 73.18 (69.21)	Recall 72.64 (70.84) 	 P_C 63.39 	 R_C 60.66 	 F_C 60.15 	 P_O 65.73 	 R_O 70.60 	 F_O 68.08
Test: [400/402]	Time 0.501 (0.503)	Precision 48.15 (68.10)	Recall 60.47 (71.87) 	 P_C 65.47 	 R_C 62.42 	 F_C 62.13 	 P_O 65.11 	 R_O 71.78 	 F_O 68.28
Test: [13/51]	  P_C 65.47 	 R_C 62.41 	 F_C 62.12 	 P_O 65.09 	 R_O 71.77 	 F_O 68.27 	 mAP 66.72
Train: [0/2566]	Time 0.964 (0.964)	Loss 2.26 (2.26)	mAP 45.75 (45.75)
Train: [100/2566]	Time 0.331 (0.337)	Loss 2.54 (2.75)	mAP 34.78 (44.61)
Train: [200/2566]	Time 0.331 (0.334)	Loss 2.69 (2.73)	mAP 36.03 (44.35)
Train: [300/2566]	Time 0.330 (0.333)	Loss 2.49 (2.74)	mAP 39.76 (44.62)
Train: [400/2566]	Time 0.331 (0.332)	Loss 2.77 (2.76)	mAP 54.06 (44.69)
Train: [500/2566]	Time 0.331 (0.332)	Loss 2.24 (2.76)	mAP 41.89 (44.68)
Train: [600/2566]	Time 0.330 (0.332)	Loss 3.01 (2.76)	mAP 46.48 (44.54)
Train: [700/2566]	Time 0.331 (0.332)	Loss 3.16 (2.76)	mAP 46.44 (44.65)
Train: [800/2566]	Time 0.331 (0.332)	Loss 2.94 (2.76)	mAP 42.87 (44.67)
Train: [900/2566]	Time 0.332 (0.332)	Loss 3.05 (2.77)	mAP 46.31 (44.70)
Train: [1000/2566]	Time 0.332 (0.332)	Loss 2.99 (2.76)	mAP 46.87 (44.74)
Train: [1100/2566]	Time 0.330 (0.332)	Loss 2.95 (2.77)	mAP 41.61 (44.75)
Train: [1200/2566]	Time 0.332 (0.331)	Loss 3.21 (2.77)	mAP 50.31 (44.70)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 2.91 (2.76)	mAP 40.78 (44.73)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.60 (2.76)	mAP 46.13 (44.72)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.85 (2.77)	mAP 45.88 (44.72)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 2.33 (2.76)	mAP 44.01 (44.73)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 2.49 (2.76)	mAP 50.12 (44.78)
Train: [1800/2566]	Time 0.332 (0.331)	Loss 2.69 (2.76)	mAP 49.43 (44.82)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.41 (2.76)	mAP 44.80 (44.83)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 2.42 (2.76)	mAP 44.07 (44.83)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 2.93 (2.76)	mAP 47.06 (44.87)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.51 (2.76)	mAP 45.41 (44.88)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.73 (2.76)	mAP 40.35 (44.91)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.56 (2.76)	mAP 47.83 (44.89)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 2.75 (2.76)	mAP 45.27 (44.88)
Train: [14/51]	Time 0.331	Loss 2.76 	mAP 44.89
Test: [0/402]	Time 1.740 (1.740)	Precision 69.61 (69.61)	Recall 60.51 (60.51) 	 P_C 22.47 	 R_C 17.30 	 F_C 18.00 	 P_O 69.61 	 R_O 60.51 	 F_O 64.74
Test: [100/402]	Time 0.500 (0.512)	Precision 88.33 (74.74)	Recall 69.28 (69.00) 	 P_C 59.97 	 R_C 51.94 	 F_C 53.25 	 P_O 72.02 	 R_O 67.31 	 F_O 69.59
Test: [200/402]	Time 0.500 (0.506)	Precision 85.57 (74.38)	Recall 74.46 (70.23) 	 P_C 61.67 	 R_C 53.12 	 F_C 55.33 	 P_O 72.26 	 R_O 68.87 	 F_O 70.52
Test: [300/402]	Time 0.499 (0.504)	Precision 80.90 (74.06)	Recall 71.64 (70.80) 	 P_C 65.79 	 R_C 60.97 	 F_C 61.85 	 P_O 71.93 	 R_O 69.22 	 F_O 70.55
Test: [400/402]	Time 0.500 (0.503)	Precision 54.82 (73.26)	Recall 52.91 (71.17) 	 P_C 68.02 	 R_C 62.74 	 F_C 64.07 	 P_O 71.47 	 R_O 69.94 	 F_O 70.70
Test: [14/51]	  P_C 68.02 	 R_C 62.73 	 F_C 64.06 	 P_O 71.46 	 R_O 69.94 	 F_O 70.69 	 mAP 68.65
Train: [0/2566]	Time 0.975 (0.975)	Loss 2.46 (2.46)	mAP 56.46 (56.46)
Train: [100/2566]	Time 0.331 (0.338)	Loss 2.78 (2.66)	mAP 44.43 (44.72)
Train: [200/2566]	Time 0.330 (0.334)	Loss 2.40 (2.64)	mAP 46.76 (44.95)
Train: [300/2566]	Time 0.330 (0.333)	Loss 3.08 (2.65)	mAP 50.99 (45.18)
Train: [400/2566]	Time 0.330 (0.333)	Loss 2.58 (2.68)	mAP 40.14 (45.24)
Train: [500/2566]	Time 0.331 (0.332)	Loss 2.52 (2.69)	mAP 37.59 (45.22)
Train: [600/2566]	Time 0.332 (0.332)	Loss 2.68 (2.70)	mAP 45.40 (45.29)
Train: [700/2566]	Time 0.330 (0.332)	Loss 2.39 (2.69)	mAP 40.61 (45.21)
Train: [800/2566]	Time 0.331 (0.332)	Loss 2.44 (2.69)	mAP 46.08 (45.22)
Train: [900/2566]	Time 0.331 (0.332)	Loss 3.25 (2.69)	mAP 45.65 (45.30)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 2.79 (2.68)	mAP 59.31 (45.37)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 2.45 (2.68)	mAP 40.65 (45.40)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 2.67 (2.68)	mAP 39.25 (45.39)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 2.46 (2.68)	mAP 43.30 (45.44)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.60 (2.68)	mAP 43.69 (45.43)
Train: [1500/2566]	Time 0.332 (0.331)	Loss 2.65 (2.68)	mAP 39.86 (45.40)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 2.40 (2.68)	mAP 53.24 (45.44)
Train: [1700/2566]	Time 0.332 (0.331)	Loss 3.04 (2.69)	mAP 53.06 (45.41)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.50 (2.69)	mAP 38.28 (45.41)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.74 (2.69)	mAP 47.24 (45.39)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 3.16 (2.69)	mAP 41.72 (45.37)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.64 (2.69)	mAP 48.01 (45.37)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 3.04 (2.69)	mAP 47.18 (45.38)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.41 (2.69)	mAP 39.76 (45.41)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.17 (2.69)	mAP 44.75 (45.43)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.85 (2.69)	mAP 45.17 (45.41)
Train: [15/51]	Time 0.331	Loss 2.69 	mAP 45.41
Test: [0/402]	Time 1.785 (1.785)	Precision 62.67 (62.67)	Recall 65.34 (65.34) 	 P_C 23.56 	 R_C 23.44 	 F_C 22.43 	 P_O 62.67 	 R_O 65.34 	 F_O 63.98
Test: [100/402]	Time 0.499 (0.512)	Precision 80.42 (67.57)	Recall 75.16 (75.08) 	 P_C 53.17 	 R_C 58.83 	 F_C 52.96 	 P_O 65.73 	 R_O 73.81 	 F_O 69.54
Test: [200/402]	Time 0.500 (0.506)	Precision 81.22 (68.66)	Recall 80.52 (75.45) 	 P_C 56.57 	 R_C 60.59 	 F_C 56.28 	 P_O 66.94 	 R_O 74.30 	 F_O 70.42
Test: [300/402]	Time 0.500 (0.504)	Precision 74.44 (67.85)	Recall 74.63 (76.96) 	 P_C 61.06 	 R_C 68.37 	 F_C 62.50 	 P_O 65.71 	 R_O 75.78 	 F_O 70.39
Test: [400/402]	Time 0.499 (0.503)	Precision 47.75 (67.18)	Recall 61.63 (77.14) 	 P_C 63.00 	 R_C 69.93 	 F_C 64.59 	 P_O 65.27 	 R_O 76.29 	 F_O 70.35
Test: [15/51]	  P_C 63.00 	 R_C 69.93 	 F_C 64.59 	 P_O 65.26 	 R_O 76.29 	 F_O 70.35 	 mAP 70.24
Train: [0/2566]	Time 0.993 (0.993)	Loss 2.63 (2.63)	mAP 46.05 (46.05)
Train: [100/2566]	Time 0.331 (0.337)	Loss 2.69 (2.64)	mAP 54.55 (44.68)
Train: [200/2566]	Time 0.330 (0.334)	Loss 2.28 (2.63)	mAP 38.73 (45.08)
Train: [300/2566]	Time 0.332 (0.333)	Loss 3.02 (2.61)	mAP 52.36 (45.51)
Train: [400/2566]	Time 0.331 (0.332)	Loss 2.24 (2.62)	mAP 44.23 (45.60)
Train: [500/2566]	Time 0.331 (0.332)	Loss 2.26 (2.63)	mAP 50.07 (45.63)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.63 (2.64)	mAP 44.57 (45.66)
Train: [700/2566]	Time 0.331 (0.331)	Loss 2.60 (2.63)	mAP 48.42 (45.63)
Train: [800/2566]	Time 0.331 (0.331)	Loss 2.92 (2.64)	mAP 50.29 (45.65)
Train: [900/2566]	Time 0.331 (0.331)	Loss 2.47 (2.64)	mAP 51.08 (45.71)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 3.24 (2.63)	mAP 47.94 (45.69)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 2.30 (2.64)	mAP 42.48 (45.71)
Train: [1200/2566]	Time 0.332 (0.331)	Loss 2.69 (2.64)	mAP 44.46 (45.72)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 2.84 (2.64)	mAP 48.69 (45.69)
Train: [1400/2566]	Time 0.332 (0.331)	Loss 2.46 (2.64)	mAP 47.11 (45.76)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.16 (2.63)	mAP 39.17 (45.73)
Train: [1600/2566]	Time 0.332 (0.331)	Loss 2.26 (2.63)	mAP 47.31 (45.77)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 2.59 (2.63)	mAP 46.56 (45.80)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 2.19 (2.63)	mAP 44.76 (45.82)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.48 (2.63)	mAP 52.24 (45.82)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 2.62 (2.63)	mAP 39.22 (45.81)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.64 (2.63)	mAP 47.80 (45.82)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.32 (2.63)	mAP 45.76 (45.83)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.86 (2.63)	mAP 43.45 (45.82)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.53 (2.63)	mAP 44.73 (45.84)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.32 (2.63)	mAP 45.77 (45.84)
Train: [16/51]	Time 0.331	Loss 2.63 	mAP 45.84
Test: [0/402]	Time 1.724 (1.724)	Precision 61.88 (61.88)	Recall 71.02 (71.02) 	 P_C 21.08 	 R_C 21.58 	 F_C 19.84 	 P_O 61.88 	 R_O 71.02 	 F_O 66.14
Test: [100/402]	Time 0.500 (0.512)	Precision 81.25 (64.21)	Recall 76.47 (77.22) 	 P_C 61.00 	 R_C 57.27 	 F_C 55.22 	 P_O 62.41 	 R_O 76.12 	 F_O 68.59
Test: [200/402]	Time 0.500 (0.506)	Precision 76.21 (65.00)	Recall 81.82 (77.49) 	 P_C 61.88 	 R_C 59.09 	 F_C 56.93 	 P_O 63.21 	 R_O 76.76 	 F_O 69.33
Test: [300/402]	Time 0.500 (0.504)	Precision 72.47 (66.09)	Recall 76.62 (77.41) 	 P_C 64.99 	 R_C 65.97 	 F_C 63.27 	 P_O 63.69 	 R_O 76.63 	 F_O 69.56
Test: [400/402]	Time 0.500 (0.503)	Precision 48.39 (65.10)	Recall 61.05 (77.93) 	 P_C 67.58 	 R_C 67.54 	 F_C 65.38 	 P_O 62.99 	 R_O 77.38 	 F_O 69.45
Test: [16/51]	  P_C 67.58 	 R_C 67.53 	 F_C 65.38 	 P_O 62.98 	 R_O 77.37 	 F_O 69.44 	 mAP 71.30
Train: [0/2566]	Time 0.975 (0.975)	Loss 2.26 (2.26)	mAP 50.73 (50.73)
Train: [100/2566]	Time 0.330 (0.337)	Loss 2.63 (2.63)	mAP 43.76 (45.72)
Train: [200/2566]	Time 0.331 (0.334)	Loss 2.69 (2.60)	mAP 45.80 (46.13)
Train: [300/2566]	Time 0.331 (0.333)	Loss 2.70 (2.59)	mAP 43.15 (46.19)
Train: [400/2566]	Time 0.330 (0.332)	Loss 3.77 (2.61)	mAP 43.55 (46.44)
Train: [500/2566]	Time 0.332 (0.332)	Loss 1.95 (2.62)	mAP 44.52 (46.32)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.48 (2.61)	mAP 46.17 (46.29)
Train: [700/2566]	Time 0.330 (0.332)	Loss 2.46 (2.61)	mAP 53.17 (46.28)
Train: [800/2566]	Time 0.330 (0.332)	Loss 2.55 (2.61)	mAP 45.01 (46.27)
Train: [900/2566]	Time 0.332 (0.332)	Loss 3.22 (2.61)	mAP 47.00 (46.27)
Train: [1000/2566]	Time 0.332 (0.331)	Loss 3.07 (2.61)	mAP 47.52 (46.28)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 2.02 (2.60)	mAP 40.62 (46.27)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 2.24 (2.60)	mAP 43.42 (46.28)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 2.71 (2.60)	mAP 49.04 (46.34)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.67 (2.60)	mAP 48.69 (46.34)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 3.14 (2.60)	mAP 46.26 (46.34)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 2.21 (2.60)	mAP 43.45 (46.33)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 2.94 (2.60)	mAP 46.39 (46.37)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.37 (2.60)	mAP 45.90 (46.38)
Train: [1900/2566]	Time 0.332 (0.331)	Loss 3.09 (2.59)	mAP 44.24 (46.36)
Train: [2000/2566]	Time 0.332 (0.331)	Loss 2.36 (2.59)	mAP 44.27 (46.34)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 2.50 (2.59)	mAP 46.47 (46.29)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.60 (2.59)	mAP 42.40 (46.28)
Train: [2300/2566]	Time 0.332 (0.331)	Loss 2.58 (2.59)	mAP 44.45 (46.26)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.06 (2.59)	mAP 55.68 (46.25)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 2.58 (2.59)	mAP 45.79 (46.27)
Train: [17/51]	Time 0.331	Loss 2.59 	mAP 46.27
Test: [0/402]	Time 1.775 (1.775)	Precision 66.48 (66.48)	Recall 67.05 (67.05) 	 P_C 21.27 	 R_C 21.53 	 F_C 20.11 	 P_O 66.48 	 R_O 67.05 	 F_O 66.76
Test: [100/402]	Time 0.500 (0.512)	Precision 84.17 (70.23)	Recall 76.47 (76.59) 	 P_C 63.60 	 R_C 57.84 	 F_C 57.02 	 P_O 68.45 	 R_O 75.23 	 F_O 71.68
Test: [200/402]	Time 0.499 (0.506)	Precision 84.82 (71.86)	Recall 82.25 (76.43) 	 P_C 64.98 	 R_C 59.56 	 F_C 58.48 	 P_O 70.26 	 R_O 75.39 	 F_O 72.73
Test: [300/402]	Time 0.500 (0.504)	Precision 79.41 (72.11)	Recall 73.88 (76.30) 	 P_C 68.96 	 R_C 65.94 	 F_C 64.71 	 P_O 70.44 	 R_O 75.11 	 F_O 72.70
Test: [400/402]	Time 0.500 (0.503)	Precision 56.45 (71.42)	Recall 61.05 (76.59) 	 P_C 70.95 	 R_C 67.55 	 F_C 66.76 	 P_O 69.80 	 R_O 75.73 	 F_O 72.65
Test: [17/51]	  P_C 70.95 	 R_C 67.54 	 F_C 66.75 	 P_O 69.79 	 R_O 75.73 	 F_O 72.64 	 mAP 72.55
Train: [0/2566]	Time 0.949 (0.949)	Loss 2.39 (2.39)	mAP 43.06 (43.06)
Train: [100/2566]	Time 0.330 (0.337)	Loss 2.89 (2.60)	mAP 46.78 (45.78)
Train: [200/2566]	Time 0.330 (0.334)	Loss 2.35 (2.59)	mAP 50.66 (45.86)
Train: [300/2566]	Time 0.330 (0.333)	Loss 3.00 (2.58)	mAP 47.46 (46.28)
Train: [400/2566]	Time 0.330 (0.332)	Loss 2.47 (2.57)	mAP 45.96 (46.15)
Train: [500/2566]	Time 0.332 (0.332)	Loss 2.71 (2.56)	mAP 47.52 (46.27)
Train: [600/2566]	Time 0.332 (0.332)	Loss 2.32 (2.56)	mAP 45.23 (46.27)
Train: [700/2566]	Time 0.331 (0.332)	Loss 2.65 (2.57)	mAP 45.58 (46.32)
Train: [800/2566]	Time 0.330 (0.332)	Loss 2.40 (2.57)	mAP 45.96 (46.25)
Train: [900/2566]	Time 0.332 (0.332)	Loss 2.88 (2.57)	mAP 44.38 (46.24)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 2.33 (2.57)	mAP 49.91 (46.21)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 2.87 (2.57)	mAP 47.23 (46.20)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 2.06 (2.57)	mAP 49.17 (46.27)
Train: [1300/2566]	Time 0.330 (0.332)	Loss 2.08 (2.56)	mAP 45.40 (46.32)
Train: [1400/2566]	Time 0.330 (0.332)	Loss 2.19 (2.56)	mAP 43.61 (46.35)
Train: [1500/2566]	Time 0.332 (0.332)	Loss 2.22 (2.56)	mAP 45.58 (46.39)
Train: [1600/2566]	Time 0.332 (0.332)	Loss 2.46 (2.55)	mAP 46.00 (46.39)
Train: [1700/2566]	Time 0.332 (0.332)	Loss 2.53 (2.55)	mAP 43.76 (46.32)
Train: [1800/2566]	Time 0.331 (0.332)	Loss 2.17 (2.55)	mAP 42.15 (46.36)
Train: [1900/2566]	Time 0.331 (0.332)	Loss 2.44 (2.55)	mAP 48.19 (46.37)
Train: [2000/2566]	Time 0.330 (0.332)	Loss 2.64 (2.55)	mAP 46.63 (46.37)
Train: [2100/2566]	Time 0.331 (0.332)	Loss 2.45 (2.56)	mAP 50.08 (46.40)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.76 (2.56)	mAP 46.23 (46.40)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.59 (2.56)	mAP 39.44 (46.39)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.50 (2.56)	mAP 49.92 (46.42)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.59 (2.56)	mAP 46.66 (46.42)
Train: [18/51]	Time 0.331	Loss 2.56 	mAP 46.42
Test: [0/402]	Time 1.770 (1.770)	Precision 63.42 (63.42)	Recall 68.47 (68.47) 	 P_C 21.91 	 R_C 20.38 	 F_C 18.89 	 P_O 63.42 	 R_O 68.47 	 F_O 65.85
Test: [100/402]	Time 0.500 (0.512)	Precision 81.29 (69.03)	Recall 73.86 (74.19) 	 P_C 65.68 	 R_C 53.96 	 F_C 55.95 	 P_O 67.79 	 R_O 72.20 	 F_O 69.92
Test: [200/402]	Time 0.500 (0.506)	Precision 75.10 (69.51)	Recall 82.25 (75.66) 	 P_C 65.40 	 R_C 56.25 	 F_C 57.65 	 P_O 68.28 	 R_O 73.96 	 F_O 71.01
Test: [300/402]	Time 0.500 (0.504)	Precision 81.84 (70.58)	Recall 72.89 (75.24) 	 P_C 70.29 	 R_C 63.72 	 F_C 64.79 	 P_O 69.61 	 R_O 73.12 	 F_O 71.32
Test: [400/402]	Time 0.501 (0.503)	Precision 55.56 (70.89)	Recall 58.14 (74.49) 	 P_C 72.38 	 R_C 65.37 	 F_C 66.98 	 P_O 70.27 	 R_O 72.60 	 F_O 71.42
Test: [18/51]	  P_C 72.38 	 R_C 65.36 	 F_C 66.98 	 P_O 70.26 	 R_O 72.60 	 F_O 71.41 	 mAP 73.21
Train: [0/2566]	Time 1.023 (1.023)	Loss 1.96 (1.96)	mAP 48.39 (48.39)
Train: [100/2566]	Time 0.330 (0.338)	Loss 2.69 (2.52)	mAP 46.87 (46.67)
Train: [200/2566]	Time 0.332 (0.334)	Loss 2.12 (2.53)	mAP 46.58 (46.43)
Train: [300/2566]	Time 0.331 (0.333)	Loss 2.48 (2.51)	mAP 42.76 (46.43)
Train: [400/2566]	Time 0.331 (0.333)	Loss 1.82 (2.51)	mAP 48.38 (46.38)
Train: [500/2566]	Time 0.331 (0.332)	Loss 2.77 (2.52)	mAP 45.75 (46.44)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.73 (2.52)	mAP 51.16 (46.54)
Train: [700/2566]	Time 0.333 (0.332)	Loss 3.03 (2.52)	mAP 46.59 (46.60)
Train: [800/2566]	Time 0.331 (0.332)	Loss 2.23 (2.53)	mAP 53.56 (46.51)
Train: [900/2566]	Time 0.332 (0.332)	Loss 2.72 (2.53)	mAP 40.09 (46.47)
Train: [1000/2566]	Time 0.332 (0.332)	Loss 2.24 (2.53)	mAP 39.47 (46.46)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 2.11 (2.53)	mAP 45.20 (46.50)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 2.94 (2.53)	mAP 53.69 (46.53)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 2.34 (2.52)	mAP 51.97 (46.58)
Train: [1400/2566]	Time 0.331 (0.332)	Loss 2.28 (2.53)	mAP 47.68 (46.64)
Train: [1500/2566]	Time 0.331 (0.332)	Loss 2.24 (2.53)	mAP 48.39 (46.69)
Train: [1600/2566]	Time 0.332 (0.332)	Loss 2.80 (2.53)	mAP 55.97 (46.64)
Train: [1700/2566]	Time 0.331 (0.332)	Loss 1.88 (2.53)	mAP 43.61 (46.65)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.36 (2.53)	mAP 41.56 (46.66)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.41 (2.53)	mAP 44.24 (46.70)
Train: [2000/2566]	Time 0.332 (0.331)	Loss 2.24 (2.52)	mAP 46.79 (46.68)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 2.04 (2.52)	mAP 41.47 (46.70)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 2.05 (2.52)	mAP 41.08 (46.69)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.35 (2.52)	mAP 56.57 (46.68)
Train: [2400/2566]	Time 0.332 (0.331)	Loss 2.42 (2.52)	mAP 43.68 (46.71)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.04 (2.52)	mAP 41.13 (46.74)
Train: [19/51]	Time 0.331	Loss 2.52 	mAP 46.75
Test: [0/402]	Time 1.739 (1.739)	Precision 65.59 (65.59)	Recall 74.72 (74.72) 	 P_C 23.87 	 R_C 25.32 	 F_C 23.02 	 P_O 65.59 	 R_O 74.72 	 F_O 69.85
Test: [100/402]	Time 0.500 (0.512)	Precision 83.56 (68.52)	Recall 79.74 (77.31) 	 P_C 64.87 	 R_C 56.62 	 F_C 56.98 	 P_O 66.12 	 R_O 76.26 	 F_O 70.83
Test: [200/402]	Time 0.500 (0.506)	Precision 78.97 (70.58)	Recall 79.65 (77.03) 	 P_C 65.85 	 R_C 58.86 	 F_C 59.15 	 P_O 68.54 	 R_O 76.27 	 F_O 72.20
Test: [300/402]	Time 0.500 (0.504)	Precision 77.41 (70.19)	Recall 75.87 (77.70) 	 P_C 70.24 	 R_C 66.75 	 F_C 66.04 	 P_O 67.84 	 R_O 76.74 	 F_O 72.01
Test: [400/402]	Time 0.500 (0.503)	Precision 45.71 (69.29)	Recall 65.12 (77.55) 	 P_C 71.60 	 R_C 68.36 	 F_C 67.74 	 P_O 67.19 	 R_O 76.85 	 F_O 71.69
Test: [19/51]	  P_C 71.59 	 R_C 68.36 	 F_C 67.74 	 P_O 67.18 	 R_O 76.84 	 F_O 71.68 	 mAP 74.12
Train: [0/2566]	Time 1.035 (1.035)	Loss 3.13 (3.13)	mAP 43.45 (43.45)
Train: [100/2566]	Time 0.332 (0.338)	Loss 2.33 (2.43)	mAP 44.27 (46.96)
Train: [200/2566]	Time 0.330 (0.334)	Loss 2.19 (2.47)	mAP 44.43 (46.95)
Train: [300/2566]	Time 0.331 (0.333)	Loss 2.48 (2.46)	mAP 42.20 (46.91)
Train: [400/2566]	Time 0.330 (0.332)	Loss 2.81 (2.47)	mAP 45.52 (46.93)
Train: [500/2566]	Time 0.331 (0.332)	Loss 2.22 (2.47)	mAP 43.95 (46.95)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.15 (2.47)	mAP 45.94 (46.81)
Train: [700/2566]	Time 0.330 (0.332)	Loss 2.54 (2.48)	mAP 40.72 (46.81)
Train: [800/2566]	Time 0.330 (0.331)	Loss 2.49 (2.48)	mAP 44.97 (46.85)
Train: [900/2566]	Time 0.331 (0.331)	Loss 3.28 (2.48)	mAP 45.34 (46.82)
Train: [1000/2566]	Time 0.332 (0.331)	Loss 2.48 (2.48)	mAP 50.12 (46.88)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 2.08 (2.48)	mAP 51.71 (46.88)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 2.01 (2.48)	mAP 42.72 (46.86)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 2.64 (2.48)	mAP 40.20 (46.84)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.44 (2.48)	mAP 50.97 (46.88)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.91 (2.49)	mAP 50.69 (46.87)
Train: [1600/2566]	Time 0.333 (0.331)	Loss 1.98 (2.49)	mAP 45.50 (46.87)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 2.17 (2.49)	mAP 49.62 (46.88)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 2.42 (2.49)	mAP 43.39 (46.88)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.15 (2.49)	mAP 48.75 (46.88)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.95 (2.49)	mAP 55.58 (46.89)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.82 (2.49)	mAP 51.25 (46.87)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 2.23 (2.49)	mAP 45.96 (46.86)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.41 (2.49)	mAP 46.32 (46.87)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 3.01 (2.49)	mAP 44.91 (46.86)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.51 (2.49)	mAP 46.84 (46.89)
Train: [20/51]	Time 0.331	Loss 2.49 	mAP 46.90
Test: [0/402]	Time 1.718 (1.718)	Precision 66.67 (66.67)	Recall 75.00 (75.00) 	 P_C 23.44 	 R_C 25.72 	 F_C 22.94 	 P_O 66.67 	 R_O 75.00 	 F_O 70.59
Test: [100/402]	Time 0.500 (0.512)	Precision 86.62 (69.77)	Recall 88.89 (78.96) 	 P_C 59.80 	 R_C 60.46 	 F_C 57.49 	 P_O 68.47 	 R_O 77.62 	 F_O 72.76
Test: [200/402]	Time 0.500 (0.506)	Precision 74.09 (70.07)	Recall 87.88 (79.64) 	 P_C 62.27 	 R_C 62.66 	 F_C 60.63 	 P_O 69.02 	 R_O 78.52 	 F_O 73.46
Test: [300/402]	Time 0.500 (0.504)	Precision 78.34 (70.66)	Recall 77.36 (79.68) 	 P_C 66.35 	 R_C 69.74 	 F_C 66.72 	 P_O 69.22 	 R_O 78.52 	 F_O 73.57
Test: [400/402]	Time 0.500 (0.503)	Precision 54.04 (69.90)	Recall 62.21 (79.56) 	 P_C 67.82 	 R_C 71.18 	 F_C 68.14 	 P_O 68.53 	 R_O 78.72 	 F_O 73.28
Test: [20/51]	  P_C 67.83 	 R_C 71.18 	 F_C 68.14 	 P_O 68.53 	 R_O 78.72 	 F_O 73.27 	 mAP 74.76
Train: [0/2566]	Time 0.992 (0.992)	Loss 2.89 (2.89)	mAP 47.78 (47.78)
Train: [100/2566]	Time 0.332 (0.337)	Loss 1.96 (2.42)	mAP 37.95 (46.73)
Train: [200/2566]	Time 0.330 (0.334)	Loss 2.80 (2.44)	mAP 39.39 (46.54)
Train: [300/2566]	Time 0.331 (0.333)	Loss 2.22 (2.45)	mAP 51.70 (46.71)
Train: [400/2566]	Time 0.330 (0.332)	Loss 2.64 (2.45)	mAP 39.50 (46.78)
Train: [500/2566]	Time 0.331 (0.332)	Loss 2.37 (2.47)	mAP 44.53 (46.91)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.99 (2.46)	mAP 49.34 (46.90)
Train: [700/2566]	Time 0.330 (0.332)	Loss 3.39 (2.47)	mAP 49.10 (47.01)
Train: [800/2566]	Time 0.330 (0.332)	Loss 2.35 (2.47)	mAP 42.05 (47.06)
Train: [900/2566]	Time 0.332 (0.331)	Loss 2.41 (2.47)	mAP 46.20 (47.07)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 2.26 (2.47)	mAP 48.34 (47.09)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 2.91 (2.47)	mAP 49.46 (47.07)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 2.02 (2.47)	mAP 48.34 (47.05)
Train: [1300/2566]	Time 0.332 (0.331)	Loss 1.89 (2.47)	mAP 51.78 (47.08)
Train: [1400/2566]	Time 0.332 (0.331)	Loss 2.76 (2.46)	mAP 46.84 (47.07)
Train: [1500/2566]	Time 0.332 (0.331)	Loss 2.45 (2.47)	mAP 52.18 (47.09)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 2.16 (2.47)	mAP 52.48 (47.07)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 2.36 (2.47)	mAP 48.60 (47.06)
Train: [1800/2566]	Time 0.332 (0.331)	Loss 2.36 (2.47)	mAP 40.75 (47.09)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.24 (2.47)	mAP 53.83 (47.10)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.98 (2.47)	mAP 42.29 (47.09)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.59 (2.47)	mAP 49.38 (47.07)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.90 (2.47)	mAP 43.35 (47.07)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.46 (2.47)	mAP 46.46 (47.04)
Train: [2400/2566]	Time 0.332 (0.331)	Loss 1.85 (2.47)	mAP 42.85 (47.03)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.27 (2.47)	mAP 44.30 (47.02)
Train: [21/51]	Time 0.331	Loss 2.47 	mAP 47.04
Test: [0/402]	Time 1.797 (1.797)	Precision 60.26 (60.26)	Recall 79.26 (79.26) 	 P_C 24.46 	 R_C 28.81 	 F_C 24.83 	 P_O 60.26 	 R_O 79.26 	 F_O 68.47
Test: [100/402]	Time 0.500 (0.513)	Precision 75.27 (63.78)	Recall 91.50 (82.59) 	 P_C 51.79 	 R_C 65.89 	 F_C 55.15 	 P_O 62.25 	 R_O 81.46 	 F_O 70.57
Test: [200/402]	Time 0.499 (0.506)	Precision 63.52 (64.29)	Recall 87.45 (82.51) 	 P_C 54.08 	 R_C 68.83 	 F_C 58.44 	 P_O 63.11 	 R_O 81.65 	 F_O 71.19
Test: [300/402]	Time 0.500 (0.504)	Precision 73.13 (64.25)	Recall 77.86 (83.17) 	 P_C 59.01 	 R_C 75.14 	 F_C 64.53 	 P_O 62.91 	 R_O 82.33 	 F_O 71.32
Test: [400/402]	Time 0.499 (0.503)	Precision 46.27 (64.10)	Recall 68.60 (82.93) 	 P_C 61.68 	 R_C 76.38 	 F_C 66.47 	 P_O 62.82 	 R_O 82.32 	 F_O 71.26
Test: [21/51]	  P_C 61.68 	 R_C 76.38 	 F_C 66.47 	 P_O 62.81 	 R_O 82.32 	 F_O 71.25 	 mAP 75.00
Train: [0/2566]	Time 1.016 (1.016)	Loss 2.29 (2.29)	mAP 44.12 (44.12)
Train: [100/2566]	Time 0.332 (0.338)	Loss 2.13 (2.40)	mAP 49.15 (46.25)
Train: [200/2566]	Time 0.331 (0.335)	Loss 2.72 (2.41)	mAP 42.34 (46.94)
Train: [300/2566]	Time 0.331 (0.334)	Loss 2.52 (2.41)	mAP 54.00 (46.99)
Train: [400/2566]	Time 0.330 (0.333)	Loss 2.04 (2.43)	mAP 48.26 (46.84)
Train: [500/2566]	Time 0.332 (0.332)	Loss 2.83 (2.44)	mAP 46.74 (46.98)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.97 (2.43)	mAP 45.50 (47.14)
Train: [700/2566]	Time 0.331 (0.332)	Loss 2.01 (2.44)	mAP 45.26 (47.14)
Train: [800/2566]	Time 0.332 (0.332)	Loss 2.61 (2.44)	mAP 47.34 (47.19)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.96 (2.45)	mAP 49.10 (47.20)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 2.55 (2.45)	mAP 51.90 (47.19)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 2.02 (2.45)	mAP 52.73 (47.19)
Train: [1200/2566]	Time 0.330 (0.332)	Loss 1.70 (2.45)	mAP 41.38 (47.21)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 2.70 (2.45)	mAP 49.02 (47.18)
Train: [1400/2566]	Time 0.331 (0.332)	Loss 2.16 (2.45)	mAP 46.82 (47.24)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.97 (2.45)	mAP 38.45 (47.18)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 2.51 (2.45)	mAP 47.62 (47.21)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 2.90 (2.45)	mAP 56.71 (47.17)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 2.29 (2.45)	mAP 56.34 (47.21)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.46 (2.44)	mAP 40.90 (47.19)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 2.20 (2.44)	mAP 43.86 (47.15)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.95 (2.44)	mAP 46.89 (47.18)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.37 (2.44)	mAP 44.72 (47.19)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 2.35 (2.44)	mAP 60.91 (47.18)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.34 (2.44)	mAP 51.47 (47.22)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 2.30 (2.44)	mAP 51.12 (47.23)
Train: [22/51]	Time 0.331	Loss 2.44 	mAP 47.23
Test: [0/402]	Time 1.730 (1.730)	Precision 71.31 (71.31)	Recall 72.73 (72.73) 	 P_C 22.47 	 R_C 24.26 	 F_C 22.38 	 P_O 71.31 	 R_O 72.73 	 F_O 72.01
Test: [100/402]	Time 0.499 (0.512)	Precision 91.97 (72.71)	Recall 82.35 (78.33) 	 P_C 66.26 	 R_C 57.28 	 F_C 57.97 	 P_O 70.54 	 R_O 77.02 	 F_O 73.64
Test: [200/402]	Time 0.500 (0.506)	Precision 83.19 (74.07)	Recall 81.39 (78.36) 	 P_C 67.73 	 R_C 59.82 	 F_C 60.45 	 P_O 72.07 	 R_O 77.26 	 F_O 74.57
Test: [300/402]	Time 0.499 (0.504)	Precision 81.94 (74.84)	Recall 73.38 (77.72) 	 P_C 72.16 	 R_C 66.75 	 F_C 67.02 	 P_O 72.88 	 R_O 76.42 	 F_O 74.61
Test: [400/402]	Time 0.500 (0.503)	Precision 58.06 (74.13)	Recall 62.79 (77.45) 	 P_C 73.85 	 R_C 68.44 	 F_C 68.83 	 P_O 72.50 	 R_O 76.48 	 F_O 74.44
Test: [22/51]	  P_C 73.85 	 R_C 68.43 	 F_C 68.83 	 P_O 72.49 	 R_O 76.48 	 F_O 74.43 	 mAP 76.02
Train: [0/2566]	Time 0.966 (0.966)	Loss 2.46 (2.46)	mAP 54.55 (54.55)
Train: [100/2566]	Time 0.330 (0.337)	Loss 2.87 (2.45)	mAP 47.26 (47.33)
Train: [200/2566]	Time 0.330 (0.334)	Loss 2.25 (2.43)	mAP 41.08 (47.52)
Train: [300/2566]	Time 0.330 (0.333)	Loss 2.51 (2.42)	mAP 51.43 (47.15)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.92 (2.41)	mAP 44.13 (47.16)
Train: [500/2566]	Time 0.332 (0.332)	Loss 2.46 (2.41)	mAP 50.08 (47.36)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.35 (2.41)	mAP 44.02 (47.40)
Train: [700/2566]	Time 0.331 (0.332)	Loss 2.33 (2.41)	mAP 55.72 (47.38)
Train: [800/2566]	Time 0.330 (0.332)	Loss 2.59 (2.41)	mAP 45.96 (47.40)
Train: [900/2566]	Time 0.330 (0.332)	Loss 2.91 (2.42)	mAP 46.20 (47.44)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 1.64 (2.42)	mAP 52.18 (47.42)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 3.12 (2.42)	mAP 43.29 (47.45)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 2.12 (2.42)	mAP 48.86 (47.45)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 2.32 (2.42)	mAP 53.12 (47.47)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.39 (2.43)	mAP 50.65 (47.50)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 2.45 (2.43)	mAP 44.56 (47.56)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 2.23 (2.43)	mAP 48.31 (47.52)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 2.28 (2.42)	mAP 49.01 (47.55)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.88 (2.42)	mAP 44.97 (47.56)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.23 (2.42)	mAP 56.62 (47.51)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 2.67 (2.41)	mAP 48.39 (47.48)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 2.79 (2.42)	mAP 45.45 (47.47)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.26 (2.41)	mAP 49.83 (47.47)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.62 (2.42)	mAP 47.04 (47.49)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.09 (2.41)	mAP 52.98 (47.48)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 2.15 (2.41)	mAP 48.33 (47.47)
Train: [23/51]	Time 0.331	Loss 2.41 	mAP 47.43
Test: [0/402]	Time 1.730 (1.730)	Precision 61.78 (61.78)	Recall 78.98 (78.98) 	 P_C 21.87 	 R_C 28.77 	 F_C 23.64 	 P_O 61.78 	 R_O 78.98 	 F_O 69.33
Test: [100/402]	Time 0.500 (0.512)	Precision 81.98 (62.41)	Recall 92.16 (84.24) 	 P_C 51.52 	 R_C 69.48 	 F_C 56.11 	 P_O 60.34 	 R_O 83.44 	 F_O 70.04
Test: [200/402]	Time 0.500 (0.506)	Precision 71.43 (63.32)	Recall 86.58 (84.22) 	 P_C 54.70 	 R_C 72.01 	 F_C 59.66 	 P_O 61.87 	 R_O 83.51 	 F_O 71.08
Test: [300/402]	Time 0.500 (0.504)	Precision 72.08 (63.63)	Recall 82.84 (84.94) 	 P_C 59.28 	 R_C 78.41 	 F_C 65.74 	 P_O 61.74 	 R_O 84.25 	 F_O 71.26
Test: [400/402]	Time 0.499 (0.503)	Precision 41.75 (62.94)	Recall 72.09 (84.99) 	 P_C 60.85 	 R_C 79.65 	 F_C 67.26 	 P_O 61.16 	 R_O 84.52 	 F_O 70.97
Test: [23/51]	  P_C 60.85 	 R_C 79.65 	 F_C 67.26 	 P_O 61.15 	 R_O 84.51 	 F_O 70.96 	 mAP 76.58
Train: [0/2566]	Time 0.953 (0.953)	Loss 2.59 (2.59)	mAP 44.90 (44.90)
Train: [100/2566]	Time 0.332 (0.337)	Loss 2.61 (2.43)	mAP 47.18 (47.28)
Train: [200/2566]	Time 0.332 (0.334)	Loss 2.97 (2.43)	mAP 48.54 (47.59)
Train: [300/2566]	Time 0.330 (0.333)	Loss 2.35 (2.41)	mAP 55.54 (47.42)
Train: [400/2566]	Time 0.332 (0.333)	Loss 2.31 (2.40)	mAP 38.68 (47.34)
Train: [500/2566]	Time 0.332 (0.332)	Loss 2.14 (2.39)	mAP 46.15 (47.41)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.40 (2.39)	mAP 50.15 (47.46)
Train: [700/2566]	Time 0.330 (0.332)	Loss 1.97 (2.39)	mAP 40.40 (47.56)
Train: [800/2566]	Time 0.331 (0.332)	Loss 2.50 (2.39)	mAP 49.36 (47.57)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.16 (2.40)	mAP 45.82 (47.61)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 2.78 (2.40)	mAP 45.60 (47.55)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 2.89 (2.40)	mAP 52.55 (47.61)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 2.24 (2.40)	mAP 44.24 (47.56)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 2.62 (2.40)	mAP 47.26 (47.53)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.65 (2.40)	mAP 47.54 (47.56)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.36 (2.40)	mAP 55.36 (47.58)
Train: [1600/2566]	Time 0.332 (0.331)	Loss 2.84 (2.40)	mAP 44.38 (47.57)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 2.36 (2.40)	mAP 45.02 (47.61)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.88 (2.40)	mAP 51.41 (47.68)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.11 (2.40)	mAP 51.53 (47.64)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 2.50 (2.40)	mAP 44.29 (47.63)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.88 (2.40)	mAP 56.71 (47.62)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.30 (2.40)	mAP 56.09 (47.64)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.20 (2.40)	mAP 47.71 (47.66)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.63 (2.40)	mAP 45.37 (47.67)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.63 (2.40)	mAP 53.27 (47.69)
Train: [24/51]	Time 0.331	Loss 2.39 	mAP 47.67
Test: [0/402]	Time 1.755 (1.755)	Precision 61.89 (61.89)	Recall 76.14 (76.14) 	 P_C 22.72 	 R_C 27.56 	 F_C 23.05 	 P_O 61.89 	 R_O 76.14 	 F_O 68.28
Test: [100/402]	Time 0.500 (0.512)	Precision 74.03 (63.81)	Recall 87.58 (83.07) 	 P_C 57.79 	 R_C 66.04 	 F_C 58.60 	 P_O 61.94 	 R_O 82.17 	 F_O 70.64
Test: [200/402]	Time 0.500 (0.506)	Precision 68.81 (64.58)	Recall 87.88 (83.37) 	 P_C 59.01 	 R_C 68.16 	 F_C 60.80 	 P_O 62.79 	 R_O 82.60 	 F_O 71.35
Test: [300/402]	Time 0.499 (0.504)	Precision 76.92 (66.54)	Recall 79.60 (83.18) 	 P_C 63.08 	 R_C 75.75 	 F_C 67.25 	 P_O 64.78 	 R_O 82.28 	 F_O 72.49
Test: [400/402]	Time 0.500 (0.503)	Precision 53.74 (66.41)	Recall 66.86 (83.09) 	 P_C 64.89 	 R_C 77.11 	 F_C 69.11 	 P_O 64.98 	 R_O 82.39 	 F_O 72.66
Test: [24/51]	  P_C 64.89 	 R_C 77.10 	 F_C 69.11 	 P_O 64.97 	 R_O 82.39 	 F_O 72.65 	 mAP 76.70
Train: [0/2566]	Time 0.964 (0.964)	Loss 2.54 (2.54)	mAP 47.58 (47.58)
Train: [100/2566]	Time 0.332 (0.338)	Loss 2.25 (2.35)	mAP 42.19 (48.11)
Train: [200/2566]	Time 0.332 (0.335)	Loss 2.29 (2.38)	mAP 52.54 (47.94)
Train: [300/2566]	Time 0.332 (0.334)	Loss 2.03 (2.39)	mAP 44.48 (47.77)
Train: [400/2566]	Time 0.330 (0.333)	Loss 2.23 (2.39)	mAP 44.92 (47.77)
Train: [500/2566]	Time 0.331 (0.333)	Loss 2.77 (2.39)	mAP 47.72 (47.72)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.15 (2.39)	mAP 46.83 (47.74)
Train: [700/2566]	Time 0.330 (0.332)	Loss 2.39 (2.38)	mAP 43.84 (47.72)
Train: [800/2566]	Time 0.330 (0.332)	Loss 2.11 (2.38)	mAP 42.79 (47.63)
Train: [900/2566]	Time 0.332 (0.332)	Loss 2.19 (2.38)	mAP 51.45 (47.61)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 2.39 (2.38)	mAP 50.07 (47.61)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 1.64 (2.38)	mAP 42.24 (47.65)
Train: [1200/2566]	Time 0.330 (0.332)	Loss 2.73 (2.38)	mAP 40.82 (47.61)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 3.29 (2.38)	mAP 49.02 (47.55)
Train: [1400/2566]	Time 0.331 (0.332)	Loss 2.30 (2.37)	mAP 51.95 (47.52)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.52 (2.37)	mAP 41.14 (47.58)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 2.69 (2.37)	mAP 52.70 (47.62)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 2.20 (2.37)	mAP 49.80 (47.61)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 2.44 (2.37)	mAP 55.04 (47.61)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.33 (2.37)	mAP 50.06 (47.63)
Train: [2000/2566]	Time 0.332 (0.331)	Loss 2.76 (2.37)	mAP 45.73 (47.63)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 2.63 (2.37)	mAP 41.92 (47.62)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.39 (2.37)	mAP 48.55 (47.62)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.33 (2.37)	mAP 40.15 (47.63)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.88 (2.37)	mAP 47.23 (47.64)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.55 (2.37)	mAP 43.05 (47.65)
Train: [25/51]	Time 0.331	Loss 2.37 	mAP 47.64
Test: [0/402]	Time 1.732 (1.732)	Precision 66.08 (66.08)	Recall 75.28 (75.28) 	 P_C 27.91 	 R_C 28.43 	 F_C 26.12 	 P_O 66.08 	 R_O 75.28 	 F_O 70.39
Test: [100/402]	Time 0.500 (0.512)	Precision 83.33 (68.87)	Recall 91.50 (82.07) 	 P_C 57.34 	 R_C 66.11 	 F_C 58.74 	 P_O 66.91 	 R_O 80.96 	 F_O 73.26
Test: [200/402]	Time 0.500 (0.506)	Precision 79.76 (69.75)	Recall 87.01 (81.80) 	 P_C 59.87 	 R_C 68.36 	 F_C 61.28 	 P_O 68.39 	 R_O 80.90 	 F_O 74.12
Test: [300/402]	Time 0.500 (0.504)	Precision 77.80 (69.51)	Recall 79.35 (82.33) 	 P_C 64.67 	 R_C 75.26 	 F_C 67.87 	 P_O 68.34 	 R_O 81.34 	 F_O 74.27
Test: [400/402]	Time 0.500 (0.503)	Precision 50.22 (69.01)	Recall 65.12 (82.31) 	 P_C 66.61 	 R_C 76.54 	 F_C 69.93 	 P_O 67.87 	 R_O 81.55 	 F_O 74.09
Test: [25/51]	  P_C 66.61 	 R_C 76.54 	 F_C 69.93 	 P_O 67.86 	 R_O 81.55 	 F_O 74.08 	 mAP 77.34
Train: [0/2566]	Time 0.998 (0.998)	Loss 2.79 (2.79)	mAP 47.10 (47.10)
Train: [100/2566]	Time 0.331 (0.338)	Loss 2.39 (2.34)	mAP 38.36 (47.77)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.91 (2.33)	mAP 50.28 (47.73)
Train: [300/2566]	Time 0.333 (0.334)	Loss 1.82 (2.34)	mAP 41.78 (47.77)
Train: [400/2566]	Time 0.332 (0.333)	Loss 2.73 (2.34)	mAP 45.45 (47.65)
Train: [500/2566]	Time 0.330 (0.333)	Loss 2.62 (2.35)	mAP 44.12 (47.59)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.92 (2.34)	mAP 52.36 (47.70)
Train: [700/2566]	Time 0.332 (0.332)	Loss 2.21 (2.34)	mAP 53.26 (47.67)
Train: [800/2566]	Time 0.332 (0.332)	Loss 2.34 (2.34)	mAP 53.08 (47.68)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.22 (2.35)	mAP 42.60 (47.77)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.92 (2.35)	mAP 42.85 (47.79)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 2.44 (2.35)	mAP 48.88 (47.82)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 2.09 (2.35)	mAP 43.77 (47.84)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 2.59 (2.35)	mAP 42.29 (47.84)
Train: [1400/2566]	Time 0.332 (0.332)	Loss 2.29 (2.35)	mAP 45.93 (47.85)
Train: [1500/2566]	Time 0.330 (0.332)	Loss 1.88 (2.34)	mAP 54.67 (47.80)
Train: [1600/2566]	Time 0.331 (0.332)	Loss 2.47 (2.35)	mAP 43.01 (47.82)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 2.19 (2.35)	mAP 50.59 (47.82)
Train: [1800/2566]	Time 0.332 (0.331)	Loss 2.00 (2.35)	mAP 42.56 (47.80)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.02 (2.35)	mAP 49.78 (47.80)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 2.49 (2.35)	mAP 55.22 (47.81)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.82 (2.35)	mAP 51.10 (47.79)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.10 (2.35)	mAP 45.31 (47.78)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.35 (2.35)	mAP 47.45 (47.76)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.56 (2.35)	mAP 49.88 (47.76)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.17 (2.35)	mAP 42.35 (47.77)
Train: [26/51]	Time 0.331	Loss 2.35 	mAP 47.75
Test: [0/402]	Time 1.719 (1.719)	Precision 59.83 (59.83)	Recall 78.69 (78.69) 	 P_C 22.31 	 R_C 30.21 	 F_C 24.05 	 P_O 59.83 	 R_O 78.69 	 F_O 67.98
Test: [100/402]	Time 0.501 (0.512)	Precision 81.10 (61.52)	Recall 86.93 (85.68) 	 P_C 51.39 	 R_C 71.38 	 F_C 56.70 	 P_O 59.08 	 R_O 85.06 	 F_O 69.73
Test: [200/402]	Time 0.500 (0.506)	Precision 71.43 (63.08)	Recall 86.58 (85.47) 	 P_C 53.63 	 R_C 74.11 	 F_C 59.17 	 P_O 61.07 	 R_O 85.02 	 F_O 71.08
Test: [300/402]	Time 0.500 (0.504)	Precision 70.33 (62.50)	Recall 84.33 (86.37) 	 P_C 58.87 	 R_C 80.63 	 F_C 65.75 	 P_O 60.67 	 R_O 85.86 	 F_O 71.10
Test: [400/402]	Time 0.500 (0.503)	Precision 40.00 (61.64)	Recall 75.58 (86.56) 	 P_C 60.66 	 R_C 82.05 	 F_C 67.74 	 P_O 59.89 	 R_O 86.22 	 F_O 70.68
Test: [26/51]	  P_C 60.65 	 R_C 82.05 	 F_C 67.74 	 P_O 59.88 	 R_O 86.21 	 F_O 70.67 	 mAP 77.86
Train: [0/2566]	Time 1.039 (1.039)	Loss 3.94 (3.94)	mAP 51.65 (51.65)
Train: [100/2566]	Time 0.331 (0.338)	Loss 2.68 (2.29)	mAP 52.85 (47.84)
Train: [200/2566]	Time 0.330 (0.335)	Loss 2.46 (2.31)	mAP 49.27 (47.95)
Train: [300/2566]	Time 0.330 (0.333)	Loss 2.02 (2.32)	mAP 42.77 (47.72)
Train: [400/2566]	Time 0.330 (0.333)	Loss 1.77 (2.33)	mAP 51.06 (47.72)
Train: [500/2566]	Time 0.330 (0.332)	Loss 3.01 (2.34)	mAP 50.43 (47.75)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.75 (2.33)	mAP 50.03 (47.75)
Train: [700/2566]	Time 0.331 (0.332)	Loss 2.13 (2.33)	mAP 53.37 (47.83)
Train: [800/2566]	Time 0.332 (0.332)	Loss 2.98 (2.32)	mAP 45.67 (47.82)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.23 (2.33)	mAP 50.56 (47.91)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 2.14 (2.32)	mAP 47.88 (47.89)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 1.80 (2.32)	mAP 47.87 (47.91)
Train: [1200/2566]	Time 0.332 (0.332)	Loss 2.56 (2.33)	mAP 59.26 (47.98)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 2.35 (2.33)	mAP 47.30 (48.04)
Train: [1400/2566]	Time 0.330 (0.332)	Loss 3.32 (2.33)	mAP 49.23 (48.03)
Train: [1500/2566]	Time 0.331 (0.332)	Loss 2.12 (2.33)	mAP 53.29 (48.03)
Train: [1600/2566]	Time 0.330 (0.332)	Loss 2.19 (2.33)	mAP 40.69 (48.03)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 2.44 (2.33)	mAP 39.70 (47.97)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.20 (2.33)	mAP 46.18 (48.01)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.22 (2.33)	mAP 48.49 (48.00)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 2.83 (2.33)	mAP 47.11 (47.98)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.37 (2.33)	mAP 43.41 (47.98)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.57 (2.33)	mAP 46.19 (48.00)
Train: [2300/2566]	Time 0.333 (0.331)	Loss 2.12 (2.33)	mAP 42.22 (47.99)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.76 (2.33)	mAP 51.67 (47.98)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 2.08 (2.33)	mAP 47.36 (47.96)
Train: [27/51]	Time 0.331	Loss 2.32 	mAP 47.94
Test: [0/402]	Time 1.771 (1.771)	Precision 74.42 (74.42)	Recall 72.73 (72.73) 	 P_C 25.90 	 R_C 25.19 	 F_C 24.34 	 P_O 74.42 	 R_O 72.73 	 F_O 73.56
Test: [100/402]	Time 0.499 (0.512)	Precision 91.39 (74.78)	Recall 90.20 (79.23) 	 P_C 66.78 	 R_C 61.40 	 F_C 60.98 	 P_O 73.18 	 R_O 77.82 	 F_O 75.43
Test: [200/402]	Time 0.501 (0.506)	Precision 82.43 (75.81)	Recall 85.28 (79.79) 	 P_C 67.96 	 R_C 63.80 	 F_C 63.51 	 P_O 74.32 	 R_O 78.72 	 F_O 76.45
Test: [300/402]	Time 0.500 (0.504)	Precision 82.75 (76.43)	Recall 76.37 (79.68) 	 P_C 71.49 	 R_C 70.64 	 F_C 69.78 	 P_O 74.93 	 R_O 78.46 	 F_O 76.66
Test: [400/402]	Time 0.501 (0.503)	Precision 56.76 (75.69)	Recall 61.05 (79.34) 	 P_C 73.06 	 R_C 72.23 	 F_C 71.40 	 P_O 74.42 	 R_O 78.35 	 F_O 76.33
Test: [27/51]	  P_C 73.07 	 R_C 72.23 	 F_C 71.40 	 P_O 74.42 	 R_O 78.34 	 F_O 76.33 	 mAP 78.07
Train: [0/2566]	Time 1.044 (1.044)	Loss 2.06 (2.06)	mAP 51.42 (51.42)
Train: [100/2566]	Time 0.332 (0.338)	Loss 2.58 (2.24)	mAP 51.41 (48.05)
Train: [200/2566]	Time 0.331 (0.335)	Loss 1.99 (2.28)	mAP 43.90 (48.27)
Train: [300/2566]	Time 0.332 (0.334)	Loss 1.91 (2.29)	mAP 40.93 (48.11)
Train: [400/2566]	Time 0.330 (0.333)	Loss 2.47 (2.29)	mAP 44.32 (48.16)
Train: [500/2566]	Time 0.330 (0.333)	Loss 2.10 (2.29)	mAP 46.45 (47.96)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.93 (2.29)	mAP 45.24 (47.92)
Train: [700/2566]	Time 0.331 (0.332)	Loss 3.29 (2.29)	mAP 52.72 (47.96)
Train: [800/2566]	Time 0.330 (0.332)	Loss 2.19 (2.29)	mAP 49.03 (47.96)
Train: [900/2566]	Time 0.332 (0.332)	Loss 2.08 (2.29)	mAP 45.77 (47.96)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 2.56 (2.29)	mAP 46.50 (47.98)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 2.42 (2.29)	mAP 45.11 (48.01)
Train: [1200/2566]	Time 0.332 (0.332)	Loss 2.16 (2.29)	mAP 50.64 (47.96)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 2.23 (2.29)	mAP 50.72 (47.96)
Train: [1400/2566]	Time 0.332 (0.332)	Loss 2.26 (2.29)	mAP 47.72 (47.95)
Train: [1500/2566]	Time 0.332 (0.332)	Loss 1.96 (2.29)	mAP 43.10 (47.92)
Train: [1600/2566]	Time 0.331 (0.332)	Loss 2.20 (2.29)	mAP 54.28 (47.97)
Train: [1700/2566]	Time 0.331 (0.332)	Loss 1.79 (2.29)	mAP 48.09 (47.99)
Train: [1800/2566]	Time 0.330 (0.332)	Loss 2.78 (2.29)	mAP 49.51 (47.97)
Train: [1900/2566]	Time 0.331 (0.332)	Loss 2.99 (2.29)	mAP 46.27 (47.93)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 2.35 (2.29)	mAP 50.76 (47.91)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 2.94 (2.29)	mAP 42.12 (47.91)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.15 (2.29)	mAP 44.56 (47.91)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 2.59 (2.30)	mAP 49.02 (47.95)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.53 (2.30)	mAP 49.55 (47.93)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 2.49 (2.30)	mAP 48.08 (47.94)
Train: [28/51]	Time 0.331	Loss 2.30 	mAP 47.94
Test: [0/402]	Time 1.770 (1.770)	Precision 68.75 (68.75)	Recall 75.00 (75.00) 	 P_C 26.32 	 R_C 27.21 	 F_C 25.49 	 P_O 68.75 	 R_O 75.00 	 F_O 71.74
Test: [100/402]	Time 0.500 (0.512)	Precision 93.62 (74.62)	Recall 86.27 (80.01) 	 P_C 65.29 	 R_C 63.76 	 F_C 61.56 	 P_O 71.59 	 R_O 79.11 	 F_O 75.17
Test: [200/402]	Time 0.500 (0.506)	Precision 81.59 (76.05)	Recall 84.42 (80.09) 	 P_C 67.02 	 R_C 66.42 	 F_C 64.49 	 P_O 73.51 	 R_O 79.34 	 F_O 76.31
Test: [300/402]	Time 0.500 (0.504)	Precision 76.63 (75.68)	Recall 79.10 (80.69) 	 P_C 70.59 	 R_C 73.12 	 F_C 70.30 	 P_O 72.94 	 R_O 79.96 	 F_O 76.29
Test: [400/402]	Time 0.500 (0.503)	Precision 54.46 (74.10)	Recall 63.95 (81.18) 	 P_C 72.40 	 R_C 74.44 	 F_C 71.99 	 P_O 71.54 	 R_O 80.73 	 F_O 75.86
Test: [28/51]	  P_C 72.40 	 R_C 74.43 	 F_C 71.99 	 P_O 71.53 	 R_O 80.72 	 F_O 75.85 	 mAP 78.48
Train: [0/2566]	Time 0.992 (0.992)	Loss 2.45 (2.45)	mAP 50.90 (50.90)
Train: [100/2566]	Time 0.332 (0.337)	Loss 1.90 (2.27)	mAP 47.19 (47.75)
Train: [200/2566]	Time 0.332 (0.334)	Loss 2.54 (2.27)	mAP 49.70 (47.79)
Train: [300/2566]	Time 0.331 (0.333)	Loss 2.20 (2.25)	mAP 46.79 (47.80)
Train: [400/2566]	Time 0.332 (0.333)	Loss 2.45 (2.24)	mAP 49.73 (47.81)
Train: [500/2566]	Time 0.331 (0.333)	Loss 1.73 (2.25)	mAP 40.66 (47.76)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.95 (2.26)	mAP 50.28 (47.72)
Train: [700/2566]	Time 0.332 (0.332)	Loss 2.28 (2.26)	mAP 52.99 (47.67)
Train: [800/2566]	Time 0.331 (0.332)	Loss 2.29 (2.26)	mAP 40.98 (47.80)
Train: [900/2566]	Time 0.332 (0.332)	Loss 1.93 (2.27)	mAP 46.16 (47.84)
Train: [1000/2566]	Time 0.332 (0.332)	Loss 2.33 (2.28)	mAP 43.66 (47.88)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 2.79 (2.28)	mAP 48.62 (47.91)
Train: [1200/2566]	Time 0.332 (0.332)	Loss 2.19 (2.28)	mAP 50.60 (47.86)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 2.11 (2.29)	mAP 51.01 (47.85)
Train: [1400/2566]	Time 0.332 (0.332)	Loss 2.45 (2.29)	mAP 48.38 (47.89)
Train: [1500/2566]	Time 0.330 (0.332)	Loss 2.19 (2.29)	mAP 53.31 (47.89)
Train: [1600/2566]	Time 0.330 (0.332)	Loss 2.29 (2.28)	mAP 50.64 (47.92)
Train: [1700/2566]	Time 0.332 (0.332)	Loss 2.91 (2.29)	mAP 50.82 (47.94)
Train: [1800/2566]	Time 0.329 (0.331)	Loss 2.16 (2.29)	mAP 56.03 (47.98)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.68 (2.29)	mAP 39.41 (48.00)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.72 (2.29)	mAP 44.12 (47.97)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.15 (2.29)	mAP 46.92 (48.00)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.00 (2.29)	mAP 48.57 (48.00)
Train: [2300/2566]	Time 0.332 (0.331)	Loss 2.42 (2.29)	mAP 52.76 (48.02)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.79 (2.29)	mAP 39.30 (48.00)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 2.32 (2.29)	mAP 49.26 (48.00)
Train: [29/51]	Time 0.331	Loss 2.28 	mAP 48.01
Test: [0/402]	Time 1.769 (1.769)	Precision 73.16 (73.16)	Recall 73.58 (73.58) 	 P_C 28.47 	 R_C 28.60 	 F_C 27.33 	 P_O 73.16 	 R_O 73.58 	 F_O 73.37
Test: [100/402]	Time 0.500 (0.512)	Precision 90.67 (76.92)	Recall 88.89 (79.16) 	 P_C 68.67 	 R_C 62.10 	 F_C 63.44 	 P_O 74.37 	 R_O 78.15 	 F_O 76.21
Test: [200/402]	Time 0.500 (0.506)	Precision 84.21 (78.42)	Recall 83.12 (78.95) 	 P_C 69.31 	 R_C 64.80 	 F_C 65.64 	 P_O 76.32 	 R_O 78.17 	 F_O 77.24
Test: [300/402]	Time 0.500 (0.504)	Precision 80.38 (78.51)	Recall 73.38 (79.40) 	 P_C 72.67 	 R_C 71.49 	 F_C 71.17 	 P_O 76.30 	 R_O 78.56 	 F_O 77.41
Test: [400/402]	Time 0.501 (0.503)	Precision 57.75 (77.33)	Recall 62.79 (79.54) 	 P_C 74.19 	 R_C 72.90 	 F_C 72.65 	 P_O 75.35 	 R_O 78.91 	 F_O 77.09
Test: [29/51]	  P_C 74.19 	 R_C 72.90 	 F_C 72.65 	 P_O 75.34 	 R_O 78.90 	 F_O 77.08 	 mAP 78.87
Train: [0/2566]	Time 0.988 (0.988)	Loss 2.30 (2.30)	mAP 49.86 (49.86)
Train: [100/2566]	Time 0.332 (0.338)	Loss 1.82 (2.21)	mAP 43.49 (47.89)
Train: [200/2566]	Time 0.330 (0.334)	Loss 2.38 (2.26)	mAP 51.78 (47.79)
Train: [300/2566]	Time 0.330 (0.333)	Loss 2.51 (2.24)	mAP 47.31 (47.89)
Train: [400/2566]	Time 0.331 (0.333)	Loss 2.81 (2.24)	mAP 55.98 (48.13)
Train: [500/2566]	Time 0.332 (0.332)	Loss 2.06 (2.25)	mAP 56.36 (48.16)
Train: [600/2566]	Time 0.332 (0.332)	Loss 2.14 (2.25)	mAP 46.50 (48.18)
Train: [700/2566]	Time 0.330 (0.332)	Loss 2.20 (2.25)	mAP 51.67 (48.18)
Train: [800/2566]	Time 0.330 (0.332)	Loss 2.32 (2.26)	mAP 45.02 (48.14)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.29 (2.26)	mAP 49.41 (48.05)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.76 (2.27)	mAP 52.99 (48.08)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 2.60 (2.27)	mAP 46.68 (48.03)
Train: [1200/2566]	Time 0.330 (0.332)	Loss 1.95 (2.27)	mAP 44.79 (48.06)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 2.13 (2.27)	mAP 44.99 (48.04)
Train: [1400/2566]	Time 0.331 (0.332)	Loss 2.32 (2.27)	mAP 52.14 (48.05)
Train: [1500/2566]	Time 0.332 (0.332)	Loss 2.04 (2.27)	mAP 43.42 (48.11)
Train: [1600/2566]	Time 0.330 (0.332)	Loss 2.56 (2.27)	mAP 46.67 (48.09)
Train: [1700/2566]	Time 0.330 (0.332)	Loss 2.57 (2.27)	mAP 66.13 (48.08)
Train: [1800/2566]	Time 0.330 (0.332)	Loss 1.86 (2.27)	mAP 47.95 (48.04)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.70 (2.27)	mAP 50.40 (48.05)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 2.38 (2.27)	mAP 50.50 (48.04)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 2.22 (2.27)	mAP 46.39 (48.05)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.99 (2.27)	mAP 44.12 (48.08)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 2.36 (2.27)	mAP 52.69 (48.07)
Train: [2400/2566]	Time 0.333 (0.331)	Loss 2.48 (2.27)	mAP 46.39 (48.06)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 2.56 (2.26)	mAP 48.21 (48.05)
Train: [30/51]	Time 0.331	Loss 2.26 	mAP 48.04
Test: [0/402]	Time 1.766 (1.766)	Precision 68.96 (68.96)	Recall 76.99 (76.99) 	 P_C 25.63 	 R_C 29.56 	 F_C 26.30 	 P_O 68.96 	 R_O 76.99 	 F_O 72.75
Test: [100/402]	Time 0.500 (0.512)	Precision 83.65 (70.96)	Recall 86.93 (82.63) 	 P_C 62.84 	 R_C 67.37 	 F_C 61.82 	 P_O 68.13 	 R_O 81.95 	 F_O 74.41
Test: [200/402]	Time 0.499 (0.506)	Precision 79.12 (72.46)	Recall 85.28 (82.53) 	 P_C 64.13 	 R_C 70.05 	 F_C 64.38 	 P_O 70.23 	 R_O 81.96 	 F_O 75.65
Test: [300/402]	Time 0.501 (0.504)	Precision 76.47 (72.51)	Recall 80.85 (83.18) 	 P_C 67.94 	 R_C 76.94 	 F_C 70.31 	 P_O 70.07 	 R_O 82.61 	 F_O 75.82
Test: [400/402]	Time 0.500 (0.503)	Precision 46.74 (71.07)	Recall 70.93 (83.63) 	 P_C 69.39 	 R_C 78.17 	 F_C 71.95 	 P_O 68.80 	 R_O 83.25 	 F_O 75.34
Test: [30/51]	  P_C 69.39 	 R_C 78.17 	 F_C 71.95 	 P_O 68.79 	 R_O 83.24 	 F_O 75.33 	 mAP 78.86
Train: [0/2566]	Time 0.967 (0.967)	Loss 2.08 (2.08)	mAP 43.06 (43.06)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.99 (2.25)	mAP 44.20 (47.41)
Train: [200/2566]	Time 0.332 (0.335)	Loss 2.04 (2.30)	mAP 42.45 (47.99)
Train: [300/2566]	Time 0.331 (0.334)	Loss 2.28 (2.25)	mAP 42.48 (48.12)
Train: [400/2566]	Time 0.332 (0.333)	Loss 2.66 (2.26)	mAP 53.03 (48.19)
Train: [500/2566]	Time 0.332 (0.333)	Loss 2.18 (2.25)	mAP 38.53 (48.29)
Train: [600/2566]	Time 0.332 (0.332)	Loss 2.16 (2.25)	mAP 45.40 (48.21)
Train: [700/2566]	Time 0.331 (0.332)	Loss 2.17 (2.24)	mAP 46.19 (48.14)
Train: [800/2566]	Time 0.331 (0.332)	Loss 2.05 (2.24)	mAP 43.73 (48.18)
Train: [900/2566]	Time 0.330 (0.332)	Loss 2.01 (2.25)	mAP 42.54 (48.13)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 2.48 (2.25)	mAP 50.23 (48.12)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 1.51 (2.24)	mAP 46.45 (48.15)
Train: [1200/2566]	Time 0.330 (0.332)	Loss 2.96 (2.25)	mAP 45.92 (48.16)
Train: [1300/2566]	Time 0.330 (0.332)	Loss 1.99 (2.25)	mAP 43.08 (48.16)
Train: [1400/2566]	Time 0.330 (0.332)	Loss 2.20 (2.25)	mAP 48.62 (48.19)
Train: [1500/2566]	Time 0.330 (0.332)	Loss 2.67 (2.25)	mAP 48.81 (48.16)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.88 (2.25)	mAP 52.23 (48.20)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 2.16 (2.25)	mAP 50.19 (48.20)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.72 (2.25)	mAP 50.11 (48.17)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.44 (2.25)	mAP 48.80 (48.19)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 2.27 (2.25)	mAP 48.41 (48.18)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.29 (2.25)	mAP 40.55 (48.18)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.06 (2.24)	mAP 47.01 (48.17)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.80 (2.24)	mAP 51.19 (48.16)
Train: [2400/2566]	Time 0.332 (0.331)	Loss 2.65 (2.24)	mAP 45.01 (48.19)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.18 (2.24)	mAP 52.51 (48.16)
Train: [31/51]	Time 0.331	Loss 2.24 	mAP 48.17
Test: [0/402]	Time 1.771 (1.771)	Precision 66.51 (66.51)	Recall 79.55 (79.55) 	 P_C 26.75 	 R_C 30.63 	 F_C 27.24 	 P_O 66.51 	 R_O 79.55 	 F_O 72.45
Test: [100/402]	Time 0.500 (0.512)	Precision 82.56 (71.22)	Recall 92.81 (83.53) 	 P_C 59.87 	 R_C 69.88 	 F_C 62.49 	 P_O 68.86 	 R_O 82.66 	 F_O 75.13
Test: [200/402]	Time 0.500 (0.506)	Precision 77.82 (72.15)	Recall 86.58 (83.71) 	 P_C 61.79 	 R_C 72.43 	 F_C 65.46 	 P_O 70.23 	 R_O 83.00 	 F_O 76.09
Test: [300/402]	Time 0.500 (0.504)	Precision 76.39 (71.73)	Recall 82.09 (84.50) 	 P_C 66.41 	 R_C 78.23 	 F_C 70.72 	 P_O 69.60 	 R_O 83.80 	 F_O 76.04
Test: [400/402]	Time 0.500 (0.503)	Precision 50.83 (70.44)	Recall 70.93 (84.80) 	 P_C 67.80 	 R_C 79.59 	 F_C 72.41 	 P_O 68.48 	 R_O 84.34 	 F_O 75.58
Test: [31/51]	  P_C 67.80 	 R_C 79.59 	 F_C 72.41 	 P_O 68.47 	 R_O 84.33 	 F_O 75.58 	 mAP 79.36
Train: [0/2566]	Time 0.949 (0.949)	Loss 2.00 (2.00)	mAP 44.95 (44.95)
Train: [100/2566]	Time 0.331 (0.337)	Loss 1.91 (2.21)	mAP 54.22 (48.51)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.90 (2.19)	mAP 51.44 (48.88)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.84 (2.20)	mAP 45.95 (48.67)
Train: [400/2566]	Time 0.330 (0.332)	Loss 2.54 (2.22)	mAP 43.71 (48.55)
Train: [500/2566]	Time 0.331 (0.332)	Loss 2.11 (2.22)	mAP 48.23 (48.53)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.50 (2.23)	mAP 57.05 (48.57)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.86 (2.22)	mAP 48.86 (48.57)
Train: [800/2566]	Time 0.331 (0.332)	Loss 2.57 (2.21)	mAP 49.17 (48.43)
Train: [900/2566]	Time 0.332 (0.331)	Loss 2.16 (2.21)	mAP 47.38 (48.41)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 2.62 (2.21)	mAP 46.85 (48.37)
Train: [1100/2566]	Time 0.332 (0.331)	Loss 1.90 (2.21)	mAP 40.90 (48.37)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.88 (2.21)	mAP 43.95 (48.37)
Train: [1300/2566]	Time 0.332 (0.331)	Loss 1.46 (2.21)	mAP 46.79 (48.39)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.10 (2.21)	mAP 44.18 (48.35)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.54 (2.21)	mAP 44.71 (48.31)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 2.37 (2.21)	mAP 44.84 (48.30)
Train: [1700/2566]	Time 0.332 (0.331)	Loss 1.91 (2.21)	mAP 47.64 (48.25)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.06 (2.21)	mAP 48.30 (48.25)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.84 (2.22)	mAP 51.28 (48.26)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 2.08 (2.22)	mAP 49.34 (48.27)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.87 (2.21)	mAP 49.16 (48.25)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.90 (2.22)	mAP 44.76 (48.25)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.54 (2.22)	mAP 54.97 (48.26)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.28 (2.22)	mAP 44.91 (48.25)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.93 (2.22)	mAP 42.70 (48.23)
Train: [32/51]	Time 0.331	Loss 2.22 	mAP 48.21
Test: [0/402]	Time 1.730 (1.730)	Precision 74.15 (74.15)	Recall 74.15 (74.15) 	 P_C 28.62 	 R_C 26.77 	 F_C 26.34 	 P_O 74.15 	 R_O 74.15 	 F_O 74.15
Test: [100/402]	Time 0.500 (0.512)	Precision 90.32 (74.88)	Recall 91.50 (80.51) 	 P_C 64.77 	 R_C 62.55 	 F_C 60.70 	 P_O 73.50 	 R_O 79.23 	 F_O 76.26
Test: [200/402]	Time 0.499 (0.506)	Precision 79.28 (75.39)	Recall 86.15 (81.11) 	 P_C 67.47 	 R_C 65.24 	 F_C 64.28 	 P_O 74.25 	 R_O 80.14 	 F_O 77.08
Test: [300/402]	Time 0.500 (0.504)	Precision 81.03 (75.79)	Recall 78.61 (81.63) 	 P_C 71.08 	 R_C 72.91 	 F_C 70.85 	 P_O 74.57 	 R_O 80.57 	 F_O 77.46
Test: [400/402]	Time 0.499 (0.503)	Precision 57.98 (75.11)	Recall 63.37 (81.39) 	 P_C 72.88 	 R_C 74.38 	 F_C 72.63 	 P_O 74.10 	 R_O 80.54 	 F_O 77.19
Test: [32/51]	  P_C 72.88 	 R_C 74.38 	 F_C 72.62 	 P_O 74.09 	 R_O 80.54 	 F_O 77.18 	 mAP 79.33
Train: [0/2566]	Time 0.997 (0.997)	Loss 2.11 (2.11)	mAP 44.38 (44.38)
Train: [100/2566]	Time 0.331 (0.337)	Loss 2.67 (2.16)	mAP 43.46 (47.55)
Train: [200/2566]	Time 0.331 (0.334)	Loss 2.09 (2.20)	mAP 51.92 (48.21)
Train: [300/2566]	Time 0.331 (0.333)	Loss 2.36 (2.20)	mAP 44.77 (48.11)
Train: [400/2566]	Time 0.331 (0.332)	Loss 2.03 (2.20)	mAP 49.11 (48.20)
Train: [500/2566]	Time 0.331 (0.332)	Loss 2.19 (2.20)	mAP 41.10 (48.21)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.51 (2.19)	mAP 50.89 (48.23)
Train: [700/2566]	Time 0.330 (0.332)	Loss 2.13 (2.19)	mAP 47.88 (48.30)
Train: [800/2566]	Time 0.330 (0.332)	Loss 2.02 (2.20)	mAP 43.63 (48.36)
Train: [900/2566]	Time 0.332 (0.331)	Loss 2.64 (2.20)	mAP 51.19 (48.42)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.98 (2.20)	mAP 49.23 (48.40)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 2.69 (2.20)	mAP 47.09 (48.39)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 2.04 (2.20)	mAP 54.64 (48.35)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 2.08 (2.20)	mAP 44.17 (48.38)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.07 (2.20)	mAP 55.22 (48.37)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.42 (2.20)	mAP 42.69 (48.37)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 2.29 (2.20)	mAP 39.65 (48.41)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 2.14 (2.20)	mAP 54.08 (48.41)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 2.38 (2.20)	mAP 53.17 (48.37)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.04 (2.21)	mAP 51.70 (48.37)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.91 (2.20)	mAP 47.63 (48.34)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.06 (2.20)	mAP 48.20 (48.32)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.10 (2.20)	mAP 53.19 (48.32)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 2.77 (2.20)	mAP 43.73 (48.33)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.65 (2.21)	mAP 53.53 (48.33)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 2.57 (2.21)	mAP 44.97 (48.31)
Train: [33/51]	Time 0.331	Loss 2.20 	mAP 48.30
Test: [0/402]	Time 1.736 (1.736)	Precision 65.25 (65.25)	Recall 78.41 (78.41) 	 P_C 28.33 	 R_C 30.46 	 F_C 27.57 	 P_O 65.25 	 R_O 78.41 	 F_O 71.23
Test: [100/402]	Time 0.499 (0.512)	Precision 87.58 (71.07)	Recall 87.58 (82.46) 	 P_C 61.99 	 R_C 66.60 	 F_C 61.71 	 P_O 69.44 	 R_O 81.31 	 F_O 74.90
Test: [200/402]	Time 0.500 (0.506)	Precision 76.72 (70.91)	Recall 87.01 (83.47) 	 P_C 63.70 	 R_C 68.93 	 F_C 65.00 	 P_O 69.63 	 R_O 82.51 	 F_O 75.53
Test: [300/402]	Time 0.500 (0.504)	Precision 77.01 (71.72)	Recall 80.85 (83.68) 	 P_C 68.78 	 R_C 74.94 	 F_C 70.77 	 P_O 70.34 	 R_O 82.68 	 F_O 76.01
Test: [400/402]	Time 0.499 (0.503)	Precision 50.46 (71.10)	Recall 63.95 (83.55) 	 P_C 70.33 	 R_C 76.52 	 F_C 72.34 	 P_O 70.01 	 R_O 82.78 	 F_O 75.86
Test: [33/51]	  P_C 70.34 	 R_C 76.52 	 F_C 72.34 	 P_O 70.00 	 R_O 82.77 	 F_O 75.85 	 mAP 79.17
Train: [0/2566]	Time 1.006 (1.006)	Loss 2.22 (2.22)	mAP 56.11 (56.11)
Train: [100/2566]	Time 0.332 (0.338)	Loss 2.40 (2.17)	mAP 49.53 (49.14)
Train: [200/2566]	Time 0.333 (0.335)	Loss 2.70 (2.16)	mAP 40.43 (48.49)
Train: [300/2566]	Time 0.332 (0.334)	Loss 2.44 (2.17)	mAP 52.53 (48.59)
Train: [400/2566]	Time 0.332 (0.333)	Loss 2.10 (2.16)	mAP 50.15 (48.42)
Train: [500/2566]	Time 0.332 (0.333)	Loss 2.09 (2.17)	mAP 46.32 (48.34)
Train: [600/2566]	Time 0.333 (0.333)	Loss 2.43 (2.18)	mAP 49.41 (48.42)
Train: [700/2566]	Time 0.330 (0.333)	Loss 2.28 (2.19)	mAP 46.39 (48.50)
Train: [800/2566]	Time 0.331 (0.333)	Loss 2.21 (2.20)	mAP 52.17 (48.52)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.13 (2.20)	mAP 48.27 (48.42)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 2.38 (2.20)	mAP 49.05 (48.42)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 2.05 (2.20)	mAP 48.24 (48.44)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 2.76 (2.20)	mAP 39.58 (48.45)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 1.70 (2.20)	mAP 46.43 (48.47)
Train: [1400/2566]	Time 0.330 (0.332)	Loss 1.91 (2.20)	mAP 53.31 (48.51)
Train: [1500/2566]	Time 0.331 (0.332)	Loss 2.21 (2.20)	mAP 45.33 (48.48)
Train: [1600/2566]	Time 0.332 (0.332)	Loss 1.99 (2.20)	mAP 52.73 (48.48)
Train: [1700/2566]	Time 0.331 (0.332)	Loss 2.28 (2.20)	mAP 53.54 (48.44)
Train: [1800/2566]	Time 0.330 (0.332)	Loss 1.95 (2.20)	mAP 50.05 (48.46)
Train: [1900/2566]	Time 0.330 (0.332)	Loss 2.63 (2.19)	mAP 46.70 (48.45)
Train: [2000/2566]	Time 0.331 (0.332)	Loss 2.68 (2.19)	mAP 44.99 (48.44)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 2.25 (2.19)	mAP 47.30 (48.41)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.93 (2.19)	mAP 46.44 (48.42)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.12 (2.19)	mAP 47.63 (48.40)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.58 (2.19)	mAP 43.91 (48.41)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 1.68 (2.19)	mAP 42.99 (48.41)
Train: [34/51]	Time 0.331	Loss 2.19 	mAP 48.38
Test: [0/402]	Time 1.774 (1.774)	Precision 76.18 (76.18)	Recall 73.58 (73.58) 	 P_C 27.87 	 R_C 27.40 	 F_C 26.42 	 P_O 76.18 	 R_O 73.58 	 F_O 74.86
Test: [100/402]	Time 0.500 (0.512)	Precision 91.67 (78.23)	Recall 86.27 (79.49) 	 P_C 69.12 	 R_C 63.28 	 F_C 63.07 	 P_O 76.03 	 R_O 78.09 	 F_O 77.04
Test: [200/402]	Time 0.500 (0.506)	Precision 86.28 (79.68)	Recall 84.42 (79.52) 	 P_C 69.04 	 R_C 66.07 	 F_C 65.03 	 P_O 77.93 	 R_O 78.48 	 F_O 78.20
Test: [300/402]	Time 0.501 (0.504)	Precision 81.15 (79.85)	Recall 73.88 (79.69) 	 P_C 73.63 	 R_C 72.02 	 F_C 71.18 	 P_O 78.05 	 R_O 78.58 	 F_O 78.31
Test: [400/402]	Time 0.499 (0.503)	Precision 58.95 (78.69)	Recall 65.12 (79.83) 	 P_C 75.64 	 R_C 73.58 	 F_C 73.13 	 P_O 77.05 	 R_O 78.93 	 F_O 77.98
Test: [34/51]	  P_C 75.64 	 R_C 73.57 	 F_C 73.13 	 P_O 77.04 	 R_O 78.93 	 F_O 77.98 	 mAP 79.85
Train: [0/2566]	Time 0.943 (0.943)	Loss 1.84 (1.84)	mAP 43.52 (43.52)
Train: [100/2566]	Time 0.331 (0.337)	Loss 2.39 (2.11)	mAP 48.73 (48.86)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.89 (2.14)	mAP 53.54 (48.77)
Train: [300/2566]	Time 0.332 (0.333)	Loss 1.61 (2.15)	mAP 54.13 (48.74)
Train: [400/2566]	Time 0.331 (0.333)	Loss 1.96 (2.17)	mAP 47.51 (48.65)
Train: [500/2566]	Time 0.330 (0.332)	Loss 2.24 (2.17)	mAP 48.17 (48.44)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.16 (2.17)	mAP 48.34 (48.44)
Train: [700/2566]	Time 0.330 (0.332)	Loss 2.40 (2.17)	mAP 51.78 (48.42)
Train: [800/2566]	Time 0.331 (0.332)	Loss 1.67 (2.17)	mAP 44.69 (48.47)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.59 (2.17)	mAP 43.43 (48.37)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 1.97 (2.17)	mAP 53.01 (48.40)
Train: [1100/2566]	Time 0.333 (0.332)	Loss 2.11 (2.17)	mAP 50.13 (48.37)
Train: [1200/2566]	Time 0.330 (0.332)	Loss 2.17 (2.17)	mAP 50.23 (48.34)
Train: [1300/2566]	Time 0.330 (0.332)	Loss 2.47 (2.17)	mAP 43.56 (48.34)
Train: [1400/2566]	Time 0.331 (0.332)	Loss 1.75 (2.17)	mAP 44.07 (48.31)
Train: [1500/2566]	Time 0.333 (0.332)	Loss 2.82 (2.17)	mAP 51.97 (48.34)
Train: [1600/2566]	Time 0.330 (0.332)	Loss 2.05 (2.17)	mAP 55.00 (48.35)
Train: [1700/2566]	Time 0.331 (0.332)	Loss 1.85 (2.17)	mAP 43.04 (48.35)
Train: [1800/2566]	Time 0.331 (0.332)	Loss 2.07 (2.17)	mAP 47.45 (48.39)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.91 (2.18)	mAP 51.57 (48.37)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.71 (2.17)	mAP 44.96 (48.35)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 2.16 (2.17)	mAP 50.00 (48.34)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.86 (2.17)	mAP 44.52 (48.36)
Train: [2300/2566]	Time 0.332 (0.331)	Loss 1.63 (2.17)	mAP 46.43 (48.37)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.60 (2.17)	mAP 52.37 (48.38)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.63 (2.17)	mAP 42.48 (48.36)
Train: [35/51]	Time 0.331	Loss 2.17 	mAP 48.35
Test: [0/402]	Time 1.716 (1.716)	Precision 73.83 (73.83)	Recall 76.14 (76.14) 	 P_C 27.15 	 R_C 29.51 	 F_C 27.37 	 P_O 73.83 	 R_O 76.14 	 F_O 74.97
Test: [100/402]	Time 0.500 (0.512)	Precision 84.76 (74.00)	Recall 90.85 (82.11) 	 P_C 63.67 	 R_C 67.22 	 F_C 62.91 	 P_O 71.69 	 R_O 81.24 	 F_O 76.16
Test: [200/402]	Time 0.500 (0.506)	Precision 81.97 (75.80)	Recall 86.58 (81.75) 	 P_C 65.28 	 R_C 69.84 	 F_C 65.74 	 P_O 73.94 	 R_O 81.03 	 F_O 77.32
Test: [300/402]	Time 0.500 (0.504)	Precision 76.51 (74.91)	Recall 78.61 (82.69) 	 P_C 69.64 	 R_C 75.74 	 F_C 70.98 	 P_O 72.82 	 R_O 81.82 	 F_O 77.06
Test: [400/402]	Time 0.500 (0.503)	Precision 51.27 (73.97)	Recall 70.35 (82.63) 	 P_C 71.03 	 R_C 77.07 	 F_C 72.76 	 P_O 72.07 	 R_O 81.94 	 F_O 76.69
Test: [35/51]	  P_C 71.03 	 R_C 77.06 	 F_C 72.76 	 P_O 72.06 	 R_O 81.94 	 F_O 76.68 	 mAP 80.02
Train: [0/2566]	Time 0.947 (0.947)	Loss 2.52 (2.52)	mAP 49.46 (49.46)
Train: [100/2566]	Time 0.332 (0.337)	Loss 2.10 (2.16)	mAP 51.57 (48.51)
Train: [200/2566]	Time 0.332 (0.334)	Loss 2.16 (2.19)	mAP 43.67 (48.65)
Train: [300/2566]	Time 0.332 (0.334)	Loss 1.97 (2.18)	mAP 48.38 (48.72)
Train: [400/2566]	Time 0.333 (0.333)	Loss 2.22 (2.17)	mAP 46.19 (48.51)
Train: [500/2566]	Time 0.330 (0.333)	Loss 2.42 (2.16)	mAP 47.11 (48.39)
Train: [600/2566]	Time 0.332 (0.332)	Loss 2.56 (2.16)	mAP 45.15 (48.44)
Train: [700/2566]	Time 0.332 (0.332)	Loss 1.98 (2.16)	mAP 46.02 (48.36)
Train: [800/2566]	Time 0.330 (0.332)	Loss 2.19 (2.16)	mAP 53.40 (48.42)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.32 (2.16)	mAP 45.75 (48.39)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 2.01 (2.16)	mAP 49.44 (48.35)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.99 (2.16)	mAP 48.04 (48.34)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.80 (2.16)	mAP 49.80 (48.31)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 2.59 (2.16)	mAP 55.13 (48.32)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 2.39 (2.16)	mAP 44.34 (48.34)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.43 (2.17)	mAP 47.38 (48.35)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 2.65 (2.16)	mAP 46.37 (48.33)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 2.06 (2.16)	mAP 49.13 (48.30)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.21 (2.16)	mAP 49.73 (48.27)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.93 (2.16)	mAP 47.70 (48.29)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.97 (2.16)	mAP 54.40 (48.34)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.91 (2.16)	mAP 49.19 (48.36)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.20 (2.16)	mAP 49.41 (48.34)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.94 (2.16)	mAP 47.65 (48.38)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.27 (2.16)	mAP 52.35 (48.37)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.90 (2.16)	mAP 46.83 (48.39)
Train: [36/51]	Time 0.331	Loss 2.16 	mAP 48.40
Test: [0/402]	Time 1.729 (1.729)	Precision 72.15 (72.15)	Recall 77.27 (77.27) 	 P_C 27.29 	 R_C 30.44 	 F_C 28.02 	 P_O 72.15 	 R_O 77.27 	 F_O 74.62
Test: [100/402]	Time 0.500 (0.512)	Precision 83.13 (73.85)	Recall 90.20 (82.53) 	 P_C 60.39 	 R_C 68.57 	 F_C 61.99 	 P_O 71.36 	 R_O 81.63 	 F_O 76.15
Test: [200/402]	Time 0.501 (0.506)	Precision 79.45 (75.33)	Recall 87.01 (82.43) 	 P_C 63.90 	 R_C 71.18 	 F_C 65.61 	 P_O 73.42 	 R_O 81.67 	 F_O 77.32
Test: [300/402]	Time 0.500 (0.504)	Precision 79.02 (75.00)	Recall 80.60 (82.96) 	 P_C 68.40 	 R_C 76.92 	 F_C 71.02 	 P_O 73.10 	 R_O 82.11 	 F_O 77.34
Test: [400/402]	Time 0.500 (0.503)	Precision 50.00 (73.84)	Recall 64.53 (82.87) 	 P_C 70.02 	 R_C 78.04 	 F_C 72.69 	 P_O 72.16 	 R_O 82.23 	 F_O 76.87
Test: [36/51]	  P_C 70.02 	 R_C 78.04 	 F_C 72.69 	 P_O 72.15 	 R_O 82.22 	 F_O 76.86 	 mAP 79.78
Train: [0/2566]	Time 0.960 (0.960)	Loss 2.65 (2.65)	mAP 50.99 (50.99)
Train: [100/2566]	Time 0.331 (0.337)	Loss 2.33 (2.19)	mAP 43.36 (49.05)
Train: [200/2566]	Time 0.332 (0.334)	Loss 2.46 (2.15)	mAP 45.77 (48.95)
Train: [300/2566]	Time 0.332 (0.333)	Loss 2.65 (2.15)	mAP 46.20 (48.78)
Train: [400/2566]	Time 0.333 (0.333)	Loss 1.83 (2.14)	mAP 46.78 (48.74)
Train: [500/2566]	Time 0.333 (0.333)	Loss 2.85 (2.15)	mAP 47.48 (48.60)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.46 (2.14)	mAP 45.34 (48.50)
Train: [700/2566]	Time 0.331 (0.332)	Loss 3.09 (2.14)	mAP 50.25 (48.51)
Train: [800/2566]	Time 0.332 (0.332)	Loss 1.88 (2.15)	mAP 49.65 (48.59)
Train: [900/2566]	Time 0.331 (0.332)	Loss 1.89 (2.15)	mAP 48.74 (48.50)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 2.57 (2.14)	mAP 57.39 (48.42)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 2.35 (2.14)	mAP 44.69 (48.41)
Train: [1200/2566]	Time 0.332 (0.332)	Loss 2.48 (2.15)	mAP 50.24 (48.42)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 1.86 (2.14)	mAP 48.25 (48.45)
Train: [1400/2566]	Time 0.331 (0.332)	Loss 1.99 (2.14)	mAP 54.86 (48.48)
Train: [1500/2566]	Time 0.331 (0.332)	Loss 2.35 (2.13)	mAP 49.14 (48.46)
Train: [1600/2566]	Time 0.331 (0.332)	Loss 1.75 (2.13)	mAP 45.43 (48.43)
Train: [1700/2566]	Time 0.332 (0.332)	Loss 1.93 (2.14)	mAP 48.85 (48.43)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.83 (2.13)	mAP 53.24 (48.41)
Train: [1900/2566]	Time 0.332 (0.331)	Loss 1.96 (2.14)	mAP 51.90 (48.43)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 2.45 (2.14)	mAP 44.36 (48.47)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 2.97 (2.14)	mAP 47.65 (48.46)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.06 (2.14)	mAP 52.85 (48.48)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 2.34 (2.14)	mAP 48.00 (48.47)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.68 (2.14)	mAP 46.30 (48.46)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 2.04 (2.14)	mAP 49.97 (48.46)
Train: [37/51]	Time 0.331	Loss 2.14 	mAP 48.48
Test: [0/402]	Time 1.860 (1.860)	Precision 67.91 (67.91)	Recall 77.56 (77.56) 	 P_C 26.99 	 R_C 28.90 	 F_C 26.49 	 P_O 67.91 	 R_O 77.56 	 F_O 72.41
Test: [100/402]	Time 0.499 (0.513)	Precision 84.91 (69.59)	Recall 88.24 (84.02) 	 P_C 62.27 	 R_C 67.18 	 F_C 62.56 	 P_O 68.21 	 R_O 82.97 	 F_O 74.87
Test: [200/402]	Time 0.500 (0.506)	Precision 77.25 (70.01)	Recall 85.28 (84.28) 	 P_C 63.72 	 R_C 69.84 	 F_C 65.12 	 P_O 68.65 	 R_O 83.47 	 F_O 75.34
Test: [300/402]	Time 0.499 (0.504)	Precision 75.59 (70.78)	Recall 80.10 (84.70) 	 P_C 68.39 	 R_C 76.53 	 F_C 71.23 	 P_O 69.31 	 R_O 83.84 	 F_O 75.88
Test: [400/402]	Time 0.500 (0.503)	Precision 51.61 (70.15)	Recall 65.12 (84.54) 	 P_C 70.11 	 R_C 77.96 	 F_C 72.89 	 P_O 68.96 	 R_O 83.88 	 F_O 75.69
Test: [37/51]	  P_C 70.12 	 R_C 77.96 	 F_C 72.89 	 P_O 68.95 	 R_O 83.88 	 F_O 75.69 	 mAP 79.92
Train: [0/2566]	Time 0.929 (0.929)	Loss 2.34 (2.34)	mAP 49.78 (49.78)
Train: [100/2566]	Time 0.330 (0.337)	Loss 2.21 (2.14)	mAP 51.27 (48.02)
Train: [200/2566]	Time 0.332 (0.334)	Loss 2.07 (2.14)	mAP 45.54 (48.09)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.81 (2.13)	mAP 48.10 (48.14)
Train: [400/2566]	Time 0.332 (0.333)	Loss 1.83 (2.13)	mAP 45.28 (48.37)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.95 (2.12)	mAP 50.57 (48.32)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.70 (2.13)	mAP 48.31 (48.53)
Train: [700/2566]	Time 0.331 (0.332)	Loss 2.42 (2.12)	mAP 51.29 (48.44)
Train: [800/2566]	Time 0.331 (0.332)	Loss 2.50 (2.12)	mAP 53.38 (48.54)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.21 (2.12)	mAP 49.88 (48.53)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.57 (2.12)	mAP 50.66 (48.50)
Train: [1100/2566]	Time 0.330 (0.332)	Loss 2.74 (2.12)	mAP 53.25 (48.54)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 2.09 (2.12)	mAP 47.48 (48.55)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.51 (2.12)	mAP 46.50 (48.45)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.84 (2.13)	mAP 47.66 (48.50)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.40 (2.12)	mAP 44.02 (48.50)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 2.08 (2.12)	mAP 48.11 (48.48)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.92 (2.12)	mAP 47.63 (48.51)
Train: [1800/2566]	Time 0.332 (0.331)	Loss 2.16 (2.12)	mAP 48.92 (48.48)
Train: [1900/2566]	Time 0.332 (0.331)	Loss 1.87 (2.12)	mAP 52.58 (48.51)
Train: [2000/2566]	Time 0.332 (0.331)	Loss 2.60 (2.13)	mAP 48.46 (48.51)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 2.07 (2.13)	mAP 51.03 (48.52)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 2.02 (2.13)	mAP 45.57 (48.50)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.25 (2.13)	mAP 43.40 (48.49)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.31 (2.13)	mAP 44.81 (48.48)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 2.86 (2.13)	mAP 48.91 (48.47)
Train: [38/51]	Time 0.331	Loss 2.13 	mAP 48.48
Test: [0/402]	Time 1.782 (1.782)	Precision 73.94 (73.94)	Recall 78.98 (78.98) 	 P_C 28.25 	 R_C 29.79 	 F_C 27.82 	 P_O 73.94 	 R_O 78.98 	 F_O 76.37
Test: [100/402]	Time 0.500 (0.512)	Precision 87.04 (73.78)	Recall 92.16 (82.87) 	 P_C 64.58 	 R_C 66.37 	 F_C 63.24 	 P_O 72.32 	 R_O 81.55 	 F_O 76.66
Test: [200/402]	Time 0.500 (0.506)	Precision 78.66 (74.33)	Recall 86.15 (83.20) 	 P_C 66.59 	 R_C 68.90 	 F_C 66.41 	 P_O 73.26 	 R_O 82.17 	 F_O 77.46
Test: [300/402]	Time 0.500 (0.504)	Precision 82.32 (74.72)	Recall 77.61 (83.43) 	 P_C 70.62 	 R_C 75.04 	 F_C 71.71 	 P_O 73.36 	 R_O 82.36 	 F_O 77.60
Test: [400/402]	Time 0.499 (0.503)	Precision 52.34 (74.18)	Recall 65.12 (83.14) 	 P_C 72.39 	 R_C 76.59 	 F_C 73.41 	 P_O 73.01 	 R_O 82.28 	 F_O 77.37
Test: [38/51]	  P_C 72.39 	 R_C 76.59 	 F_C 73.41 	 P_O 73.00 	 R_O 82.28 	 F_O 77.36 	 mAP 80.35
Train: [0/2566]	Time 0.965 (0.965)	Loss 2.05 (2.05)	mAP 54.04 (54.04)
Train: [100/2566]	Time 0.330 (0.338)	Loss 2.57 (2.10)	mAP 54.77 (47.26)
Train: [200/2566]	Time 0.330 (0.334)	Loss 2.04 (2.08)	mAP 52.27 (47.77)
Train: [300/2566]	Time 0.333 (0.333)	Loss 2.53 (2.12)	mAP 56.61 (47.61)
Train: [400/2566]	Time 0.331 (0.333)	Loss 2.24 (2.13)	mAP 51.49 (48.06)
Train: [500/2566]	Time 0.331 (0.333)	Loss 1.96 (2.13)	mAP 47.41 (48.15)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.24 (2.12)	mAP 45.04 (48.26)
Train: [700/2566]	Time 0.332 (0.332)	Loss 2.42 (2.12)	mAP 47.97 (48.21)
Train: [800/2566]	Time 0.332 (0.332)	Loss 1.89 (2.13)	mAP 49.97 (48.19)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.21 (2.13)	mAP 56.44 (48.27)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 2.33 (2.13)	mAP 47.02 (48.28)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 2.00 (2.12)	mAP 53.79 (48.32)
Train: [1200/2566]	Time 0.332 (0.332)	Loss 1.89 (2.12)	mAP 52.49 (48.32)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 2.14 (2.12)	mAP 50.18 (48.30)
Train: [1400/2566]	Time 0.331 (0.332)	Loss 1.98 (2.12)	mAP 54.04 (48.30)
Train: [1500/2566]	Time 0.330 (0.332)	Loss 1.97 (2.12)	mAP 47.85 (48.32)
Train: [1600/2566]	Time 0.330 (0.332)	Loss 2.34 (2.12)	mAP 53.56 (48.37)
Train: [1700/2566]	Time 0.330 (0.332)	Loss 2.57 (2.12)	mAP 53.31 (48.35)
Train: [1800/2566]	Time 0.331 (0.332)	Loss 2.68 (2.12)	mAP 58.74 (48.36)
Train: [1900/2566]	Time 0.330 (0.332)	Loss 2.45 (2.12)	mAP 49.17 (48.40)
Train: [2000/2566]	Time 0.332 (0.332)	Loss 2.22 (2.13)	mAP 49.45 (48.40)
Train: [2100/2566]	Time 0.332 (0.332)	Loss 2.13 (2.12)	mAP 58.16 (48.38)
Train: [2200/2566]	Time 0.331 (0.332)	Loss 2.00 (2.12)	mAP 51.53 (48.38)
Train: [2300/2566]	Time 0.332 (0.332)	Loss 2.36 (2.12)	mAP 46.66 (48.38)
Train: [2400/2566]	Time 0.333 (0.332)	Loss 2.65 (2.12)	mAP 42.00 (48.37)
Train: [2500/2566]	Time 0.331 (0.332)	Loss 1.89 (2.13)	mAP 43.25 (48.38)
Train: [39/51]	Time 0.331	Loss 2.13 	mAP 48.40
Test: [0/402]	Time 1.815 (1.815)	Precision 71.09 (71.09)	Recall 76.14 (76.14) 	 P_C 28.68 	 R_C 29.78 	 F_C 27.67 	 P_O 71.09 	 R_O 76.14 	 F_O 73.53
Test: [100/402]	Time 0.500 (0.513)	Precision 80.12 (72.26)	Recall 89.54 (83.42) 	 P_C 59.16 	 R_C 68.46 	 F_C 61.73 	 P_O 71.02 	 R_O 82.09 	 F_O 76.15
Test: [200/402]	Time 0.500 (0.506)	Precision 76.78 (72.46)	Recall 88.74 (83.92) 	 P_C 61.48 	 R_C 71.17 	 F_C 64.95 	 P_O 71.53 	 R_O 82.87 	 F_O 76.78
Test: [300/402]	Time 0.500 (0.504)	Precision 80.39 (73.20)	Recall 81.59 (84.09) 	 P_C 67.11 	 R_C 77.04 	 F_C 71.09 	 P_O 72.23 	 R_O 82.91 	 F_O 77.20
Test: [400/402]	Time 0.500 (0.503)	Precision 54.33 (72.64)	Recall 65.70 (83.90) 	 P_C 68.80 	 R_C 78.57 	 F_C 72.88 	 P_O 71.87 	 R_O 82.94 	 F_O 77.01
Test: [39/51]	  P_C 68.80 	 R_C 78.56 	 F_C 72.87 	 P_O 71.87 	 R_O 82.94 	 F_O 77.01 	 mAP 80.18
Train: [0/2566]	Time 0.935 (0.935)	Loss 2.26 (2.26)	mAP 36.88 (36.88)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.44 (2.15)	mAP 49.38 (48.25)
Train: [200/2566]	Time 0.331 (0.335)	Loss 2.42 (2.15)	mAP 53.05 (48.49)
Train: [300/2566]	Time 0.330 (0.334)	Loss 2.34 (2.13)	mAP 50.17 (48.42)
Train: [400/2566]	Time 0.330 (0.333)	Loss 2.16 (2.13)	mAP 51.95 (48.54)
Train: [500/2566]	Time 0.330 (0.333)	Loss 2.23 (2.12)	mAP 48.79 (48.46)
Train: [600/2566]	Time 0.332 (0.332)	Loss 2.85 (2.11)	mAP 50.96 (48.47)
Train: [700/2566]	Time 0.330 (0.332)	Loss 2.07 (2.10)	mAP 42.94 (48.49)
Train: [800/2566]	Time 0.330 (0.332)	Loss 2.48 (2.10)	mAP 52.74 (48.45)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.35 (2.11)	mAP 45.16 (48.52)
Train: [1000/2566]	Time 0.332 (0.332)	Loss 2.23 (2.11)	mAP 48.34 (48.52)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 1.94 (2.11)	mAP 44.60 (48.51)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 2.53 (2.11)	mAP 42.04 (48.52)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 2.39 (2.11)	mAP 45.91 (48.52)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.21 (2.11)	mAP 46.57 (48.54)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.18 (2.11)	mAP 46.75 (48.55)
Train: [1600/2566]	Time 0.332 (0.331)	Loss 1.83 (2.11)	mAP 44.68 (48.57)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.95 (2.11)	mAP 56.46 (48.56)
Train: [1800/2566]	Time 0.332 (0.331)	Loss 2.31 (2.11)	mAP 43.18 (48.55)
Train: [1900/2566]	Time 0.332 (0.331)	Loss 2.32 (2.11)	mAP 47.95 (48.57)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 2.14 (2.11)	mAP 53.56 (48.57)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.49 (2.11)	mAP 49.75 (48.57)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 1.98 (2.11)	mAP 50.77 (48.55)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.26 (2.11)	mAP 48.84 (48.51)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.11 (2.11)	mAP 52.82 (48.52)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.19 (2.11)	mAP 47.38 (48.49)
Train: [40/51]	Time 0.331	Loss 2.11 	mAP 48.47
Test: [0/402]	Time 1.810 (1.810)	Precision 73.67 (73.67)	Recall 74.72 (74.72) 	 P_C 29.44 	 R_C 26.87 	 F_C 26.70 	 P_O 73.67 	 R_O 74.72 	 F_O 74.19
Test: [100/402]	Time 0.499 (0.513)	Precision 83.75 (75.41)	Recall 87.58 (81.36) 	 P_C 63.95 	 R_C 65.62 	 F_C 62.60 	 P_O 74.44 	 R_O 79.69 	 F_O 76.98
Test: [200/402]	Time 0.500 (0.506)	Precision 79.84 (75.47)	Recall 87.45 (82.09) 	 P_C 66.26 	 R_C 68.13 	 F_C 65.32 	 P_O 74.70 	 R_O 80.82 	 F_O 77.64
Test: [300/402]	Time 0.500 (0.504)	Precision 80.51 (76.00)	Recall 79.10 (82.43) 	 P_C 70.91 	 R_C 74.71 	 F_C 71.70 	 P_O 75.18 	 R_O 81.04 	 F_O 78.00
Test: [400/402]	Time 0.500 (0.503)	Precision 59.04 (75.63)	Recall 64.53 (82.24) 	 P_C 72.75 	 R_C 76.20 	 F_C 73.76 	 P_O 75.02 	 R_O 81.09 	 F_O 77.94
Test: [40/51]	  P_C 72.75 	 R_C 76.20 	 F_C 73.76 	 P_O 75.01 	 R_O 81.09 	 F_O 77.93 	 mAP 80.35
Train: [0/2566]	Time 0.967 (0.967)	Loss 1.59 (1.59)	mAP 45.67 (45.67)
Train: [100/2566]	Time 0.330 (0.337)	Loss 2.01 (2.07)	mAP 54.32 (48.64)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.93 (2.06)	mAP 52.77 (48.68)
Train: [300/2566]	Time 0.331 (0.333)	Loss 2.61 (2.08)	mAP 53.75 (48.66)
Train: [400/2566]	Time 0.332 (0.332)	Loss 1.81 (2.09)	mAP 56.35 (48.73)
Train: [500/2566]	Time 0.330 (0.332)	Loss 2.10 (2.08)	mAP 36.60 (48.67)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.15 (2.10)	mAP 49.65 (48.62)
Train: [700/2566]	Time 0.331 (0.332)	Loss 2.15 (2.10)	mAP 59.28 (48.45)
Train: [800/2566]	Time 0.330 (0.332)	Loss 1.67 (2.10)	mAP 52.07 (48.44)
Train: [900/2566]	Time 0.332 (0.332)	Loss 1.89 (2.10)	mAP 50.02 (48.48)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 2.27 (2.09)	mAP 56.59 (48.44)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 2.23 (2.10)	mAP 50.16 (48.49)
Train: [1200/2566]	Time 0.332 (0.331)	Loss 1.93 (2.10)	mAP 45.35 (48.52)
Train: [1300/2566]	Time 0.332 (0.331)	Loss 1.92 (2.10)	mAP 47.96 (48.54)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.89 (2.10)	mAP 43.67 (48.53)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.99 (2.10)	mAP 53.19 (48.55)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.98 (2.10)	mAP 45.59 (48.57)
Train: [1700/2566]	Time 0.332 (0.331)	Loss 2.38 (2.10)	mAP 54.59 (48.57)
Train: [1800/2566]	Time 0.332 (0.331)	Loss 2.31 (2.10)	mAP 43.85 (48.57)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.68 (2.10)	mAP 52.17 (48.56)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 2.56 (2.10)	mAP 43.85 (48.52)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.85 (2.10)	mAP 40.78 (48.53)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.87 (2.10)	mAP 50.00 (48.55)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 2.12 (2.10)	mAP 44.51 (48.55)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 2.27 (2.10)	mAP 43.50 (48.54)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.12 (2.10)	mAP 53.19 (48.55)
Train: [41/51]	Time 0.331	Loss 2.10 	mAP 48.54
Test: [0/402]	Time 1.767 (1.767)	Precision 75.36 (75.36)	Recall 74.72 (74.72) 	 P_C 28.23 	 R_C 25.63 	 F_C 25.28 	 P_O 75.36 	 R_O 74.72 	 F_O 75.04
Test: [100/402]	Time 0.500 (0.512)	Precision 88.46 (76.35)	Recall 90.20 (81.37) 	 P_C 64.68 	 R_C 65.58 	 F_C 63.42 	 P_O 75.06 	 R_O 79.92 	 F_O 77.41
Test: [200/402]	Time 0.499 (0.506)	Precision 81.71 (76.40)	Recall 87.01 (82.10) 	 P_C 66.78 	 R_C 68.16 	 F_C 66.63 	 P_O 75.48 	 R_O 80.97 	 F_O 78.13
Test: [300/402]	Time 0.500 (0.504)	Precision 81.91 (77.13)	Recall 78.86 (82.31) 	 P_C 71.41 	 R_C 74.63 	 F_C 72.41 	 P_O 76.13 	 R_O 81.08 	 F_O 78.53
Test: [400/402]	Time 0.500 (0.503)	Precision 59.22 (76.62)	Recall 61.63 (82.03) 	 P_C 73.16 	 R_C 76.10 	 F_C 74.15 	 P_O 75.79 	 R_O 81.04 	 F_O 78.33
Test: [41/51]	  P_C 73.17 	 R_C 76.09 	 F_C 74.15 	 P_O 75.79 	 R_O 81.04 	 F_O 78.33 	 mAP 80.35
Train: [0/2566]	Time 0.992 (0.992)	Loss 1.77 (1.77)	mAP 43.72 (43.72)
Train: [100/2566]	Time 0.331 (0.338)	Loss 2.36 (2.12)	mAP 45.93 (48.45)
Train: [200/2566]	Time 0.332 (0.335)	Loss 2.09 (2.10)	mAP 50.80 (48.47)
Train: [300/2566]	Time 0.330 (0.334)	Loss 1.55 (2.10)	mAP 41.86 (48.41)
Train: [400/2566]	Time 0.331 (0.333)	Loss 2.08 (2.10)	mAP 43.17 (48.61)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.70 (2.10)	mAP 57.89 (48.70)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.23 (2.09)	mAP 43.19 (48.58)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.77 (2.09)	mAP 53.92 (48.71)
Train: [800/2566]	Time 0.332 (0.332)	Loss 2.29 (2.08)	mAP 55.10 (48.68)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.24 (2.09)	mAP 49.85 (48.61)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 1.83 (2.09)	mAP 48.99 (48.57)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 1.77 (2.08)	mAP 43.28 (48.57)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.90 (2.08)	mAP 46.34 (48.61)
Train: [1300/2566]	Time 0.332 (0.331)	Loss 2.08 (2.08)	mAP 42.35 (48.58)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 2.19 (2.08)	mAP 45.63 (48.57)
Train: [1500/2566]	Time 0.332 (0.331)	Loss 1.49 (2.08)	mAP 51.90 (48.57)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.80 (2.08)	mAP 46.42 (48.61)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.69 (2.08)	mAP 42.96 (48.57)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.30 (2.08)	mAP 50.51 (48.58)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.31 (2.08)	mAP 54.20 (48.55)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.77 (2.08)	mAP 52.70 (48.56)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.23 (2.08)	mAP 52.63 (48.54)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 2.03 (2.09)	mAP 50.00 (48.53)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.69 (2.08)	mAP 43.67 (48.52)
Train: [2400/2566]	Time 0.332 (0.331)	Loss 2.43 (2.09)	mAP 53.15 (48.55)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.90 (2.09)	mAP 57.77 (48.60)
Train: [42/51]	Time 0.331	Loss 2.09 	mAP 48.58
Test: [0/402]	Time 1.778 (1.778)	Precision 75.35 (75.35)	Recall 77.27 (77.27) 	 P_C 29.64 	 R_C 29.26 	 F_C 28.55 	 P_O 75.35 	 R_O 77.27 	 F_O 76.30
Test: [100/402]	Time 0.500 (0.512)	Precision 92.76 (76.02)	Recall 92.16 (82.23) 	 P_C 64.78 	 R_C 67.13 	 F_C 64.10 	 P_O 73.71 	 R_O 81.25 	 F_O 77.30
Test: [200/402]	Time 0.500 (0.506)	Precision 81.22 (77.05)	Recall 86.15 (82.27) 	 P_C 66.55 	 R_C 69.63 	 F_C 67.06 	 P_O 75.45 	 R_O 81.46 	 F_O 78.34
Test: [300/402]	Time 0.500 (0.504)	Precision 79.36 (76.87)	Recall 80.35 (82.97) 	 P_C 71.09 	 R_C 76.02 	 F_C 72.67 	 P_O 75.09 	 R_O 82.15 	 F_O 78.46
Test: [400/402]	Time 0.500 (0.503)	Precision 56.16 (75.71)	Recall 66.28 (83.07) 	 P_C 72.86 	 R_C 77.45 	 F_C 74.36 	 P_O 74.08 	 R_O 82.49 	 F_O 78.06
Test: [42/51]	  P_C 72.86 	 R_C 77.45 	 F_C 74.36 	 P_O 74.08 	 R_O 82.49 	 F_O 78.06 	 mAP 80.44
Train: [0/2566]	Time 0.950 (0.950)	Loss 1.95 (1.95)	mAP 48.85 (48.85)
Train: [100/2566]	Time 0.332 (0.337)	Loss 2.34 (2.03)	mAP 47.30 (49.00)
Train: [200/2566]	Time 0.332 (0.335)	Loss 2.08 (2.05)	mAP 52.04 (48.67)
Train: [300/2566]	Time 0.332 (0.334)	Loss 1.98 (2.05)	mAP 42.47 (48.60)
Train: [400/2566]	Time 0.331 (0.333)	Loss 2.21 (2.08)	mAP 52.21 (48.66)
Train: [500/2566]	Time 0.331 (0.333)	Loss 1.77 (2.08)	mAP 44.78 (48.72)
Train: [600/2566]	Time 0.333 (0.333)	Loss 1.87 (2.08)	mAP 44.44 (48.74)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.81 (2.08)	mAP 48.04 (48.63)
Train: [800/2566]	Time 0.330 (0.332)	Loss 1.78 (2.08)	mAP 48.85 (48.56)
Train: [900/2566]	Time 0.332 (0.332)	Loss 2.36 (2.08)	mAP 51.23 (48.61)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 2.75 (2.08)	mAP 48.72 (48.63)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 2.36 (2.08)	mAP 46.82 (48.65)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 2.42 (2.08)	mAP 45.13 (48.65)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 1.68 (2.08)	mAP 48.21 (48.66)
Train: [1400/2566]	Time 0.330 (0.332)	Loss 1.83 (2.08)	mAP 46.23 (48.67)
Train: [1500/2566]	Time 0.332 (0.332)	Loss 2.36 (2.08)	mAP 45.03 (48.71)
Train: [1600/2566]	Time 0.331 (0.332)	Loss 1.44 (2.08)	mAP 48.92 (48.70)
Train: [1700/2566]	Time 0.332 (0.332)	Loss 1.89 (2.08)	mAP 51.37 (48.67)
Train: [1800/2566]	Time 0.330 (0.332)	Loss 1.64 (2.08)	mAP 40.13 (48.65)
Train: [1900/2566]	Time 0.332 (0.332)	Loss 2.26 (2.08)	mAP 42.10 (48.69)
Train: [2000/2566]	Time 0.333 (0.331)	Loss 2.43 (2.08)	mAP 44.98 (48.68)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.40 (2.08)	mAP 48.60 (48.74)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.30 (2.08)	mAP 46.92 (48.73)
Train: [2300/2566]	Time 0.332 (0.331)	Loss 2.67 (2.08)	mAP 44.37 (48.73)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.99 (2.08)	mAP 41.30 (48.70)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.11 (2.08)	mAP 50.72 (48.71)
Train: [43/51]	Time 0.331	Loss 2.08 	mAP 48.68
Test: [0/402]	Time 1.719 (1.719)	Precision 75.77 (75.77)	Recall 76.42 (76.42) 	 P_C 30.06 	 R_C 29.45 	 F_C 28.54 	 P_O 75.77 	 R_O 76.42 	 F_O 76.10
Test: [100/402]	Time 0.500 (0.512)	Precision 89.26 (76.45)	Recall 86.93 (81.60) 	 P_C 66.19 	 R_C 66.12 	 F_C 64.05 	 P_O 74.63 	 R_O 80.38 	 F_O 77.40
Test: [200/402]	Time 0.499 (0.506)	Precision 80.65 (76.91)	Recall 86.58 (82.27) 	 P_C 67.81 	 R_C 68.62 	 F_C 66.87 	 P_O 75.48 	 R_O 81.31 	 F_O 78.29
Test: [300/402]	Time 0.500 (0.504)	Precision 79.13 (77.25)	Recall 81.09 (82.66) 	 P_C 71.66 	 R_C 75.07 	 F_C 72.48 	 P_O 75.69 	 R_O 81.67 	 F_O 78.57
Test: [400/402]	Time 0.500 (0.503)	Precision 58.92 (76.18)	Recall 63.37 (82.68) 	 P_C 73.45 	 R_C 76.53 	 F_C 74.29 	 P_O 74.84 	 R_O 81.91 	 F_O 78.22
Test: [43/51]	  P_C 73.46 	 R_C 76.53 	 F_C 74.29 	 P_O 74.84 	 R_O 81.91 	 F_O 78.22 	 mAP 80.66
Train: [0/2566]	Time 0.963 (0.963)	Loss 2.04 (2.04)	mAP 46.82 (46.82)
Train: [100/2566]	Time 0.331 (0.337)	Loss 2.63 (2.07)	mAP 45.29 (49.22)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.99 (2.06)	mAP 47.55 (48.68)
Train: [300/2566]	Time 0.331 (0.333)	Loss 2.28 (2.08)	mAP 43.53 (48.59)
Train: [400/2566]	Time 0.332 (0.332)	Loss 2.29 (2.08)	mAP 51.45 (48.73)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.58 (2.07)	mAP 56.30 (48.82)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.17 (2.07)	mAP 51.46 (48.83)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.88 (2.07)	mAP 49.14 (48.78)
Train: [800/2566]	Time 0.332 (0.332)	Loss 1.74 (2.07)	mAP 46.07 (48.76)
Train: [900/2566]	Time 0.332 (0.332)	Loss 1.89 (2.08)	mAP 45.91 (48.64)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 2.16 (2.08)	mAP 45.10 (48.61)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 2.07 (2.08)	mAP 48.97 (48.63)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.67 (2.08)	mAP 56.96 (48.59)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.88 (2.07)	mAP 47.67 (48.62)
Train: [1400/2566]	Time 0.332 (0.331)	Loss 1.71 (2.07)	mAP 46.70 (48.61)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.02 (2.07)	mAP 42.08 (48.61)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.80 (2.07)	mAP 41.06 (48.66)
Train: [1700/2566]	Time 0.333 (0.331)	Loss 2.16 (2.07)	mAP 50.84 (48.65)
Train: [1800/2566]	Time 0.332 (0.331)	Loss 2.03 (2.07)	mAP 49.58 (48.67)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.24 (2.07)	mAP 45.12 (48.63)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 2.54 (2.07)	mAP 45.40 (48.62)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 2.00 (2.07)	mAP 46.48 (48.63)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 1.49 (2.07)	mAP 49.47 (48.63)
Train: [2300/2566]	Time 0.333 (0.331)	Loss 1.59 (2.06)	mAP 38.81 (48.61)
Train: [2400/2566]	Time 0.332 (0.331)	Loss 1.91 (2.06)	mAP 46.22 (48.59)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.64 (2.07)	mAP 54.71 (48.60)
Train: [44/51]	Time 0.331	Loss 2.07 	mAP 48.60
Test: [0/402]	Time 1.712 (1.712)	Precision 70.54 (70.54)	Recall 77.56 (77.56) 	 P_C 28.87 	 R_C 29.26 	 F_C 27.61 	 P_O 70.54 	 R_O 77.56 	 F_O 73.88
Test: [100/402]	Time 0.500 (0.512)	Precision 83.44 (72.72)	Recall 88.89 (83.85) 	 P_C 59.75 	 R_C 69.27 	 F_C 62.93 	 P_O 71.21 	 R_O 82.69 	 F_O 76.52
Test: [200/402]	Time 0.500 (0.506)	Precision 76.43 (73.20)	Recall 87.01 (84.22) 	 P_C 62.32 	 R_C 72.13 	 F_C 66.16 	 P_O 71.99 	 R_O 83.30 	 F_O 77.23
Test: [300/402]	Time 0.500 (0.504)	Precision 77.75 (73.52)	Recall 80.85 (84.84) 	 P_C 67.63 	 R_C 78.19 	 F_C 71.96 	 P_O 72.16 	 R_O 83.89 	 F_O 77.59
Test: [400/402]	Time 0.500 (0.503)	Precision 53.74 (72.79)	Recall 66.86 (84.76) 	 P_C 69.21 	 R_C 79.51 	 F_C 73.64 	 P_O 71.61 	 R_O 84.01 	 F_O 77.32
Test: [44/51]	  P_C 69.21 	 R_C 79.51 	 F_C 73.64 	 P_O 71.61 	 R_O 84.00 	 F_O 77.31 	 mAP 80.56
Train: [0/2566]	Time 0.979 (0.979)	Loss 2.24 (2.24)	mAP 49.25 (49.25)
Train: [100/2566]	Time 0.331 (0.337)	Loss 1.92 (2.02)	mAP 44.40 (48.68)
Train: [200/2566]	Time 0.331 (0.334)	Loss 2.28 (2.05)	mAP 52.76 (49.12)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.88 (2.06)	mAP 39.25 (48.98)
Train: [400/2566]	Time 0.331 (0.333)	Loss 1.98 (2.07)	mAP 49.57 (49.01)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.94 (2.06)	mAP 54.44 (48.98)
Train: [600/2566]	Time 0.332 (0.332)	Loss 1.94 (2.06)	mAP 46.28 (48.96)
Train: [700/2566]	Time 0.332 (0.332)	Loss 2.18 (2.07)	mAP 49.66 (48.93)
Train: [800/2566]	Time 0.331 (0.332)	Loss 1.98 (2.07)	mAP 50.53 (48.88)
Train: [900/2566]	Time 0.331 (0.332)	Loss 1.71 (2.07)	mAP 46.78 (48.86)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 2.06 (2.07)	mAP 50.49 (48.87)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 2.14 (2.07)	mAP 52.02 (48.89)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 1.92 (2.07)	mAP 44.24 (48.88)
Train: [1300/2566]	Time 0.332 (0.332)	Loss 1.92 (2.07)	mAP 46.36 (48.85)
Train: [1400/2566]	Time 0.332 (0.332)	Loss 1.99 (2.07)	mAP 50.55 (48.85)
Train: [1500/2566]	Time 0.330 (0.332)	Loss 2.06 (2.07)	mAP 55.11 (48.83)
Train: [1600/2566]	Time 0.332 (0.332)	Loss 2.11 (2.07)	mAP 41.74 (48.79)
Train: [1700/2566]	Time 0.332 (0.332)	Loss 2.03 (2.07)	mAP 44.16 (48.75)
Train: [1800/2566]	Time 0.331 (0.332)	Loss 1.98 (2.07)	mAP 47.70 (48.78)
Train: [1900/2566]	Time 0.330 (0.332)	Loss 1.98 (2.07)	mAP 49.25 (48.75)
Train: [2000/2566]	Time 0.330 (0.332)	Loss 2.02 (2.06)	mAP 50.14 (48.78)
Train: [2100/2566]	Time 0.330 (0.332)	Loss 2.19 (2.06)	mAP 54.36 (48.77)
Train: [2200/2566]	Time 0.332 (0.332)	Loss 2.44 (2.07)	mAP 43.72 (48.77)
Train: [2300/2566]	Time 0.331 (0.332)	Loss 1.89 (2.07)	mAP 44.11 (48.76)
Train: [2400/2566]	Time 0.330 (0.332)	Loss 2.10 (2.06)	mAP 48.65 (48.71)
Train: [2500/2566]	Time 0.332 (0.332)	Loss 2.02 (2.06)	mAP 44.57 (48.71)
Train: [45/51]	Time 0.331	Loss 2.06 	mAP 48.68
Test: [0/402]	Time 1.816 (1.816)	Precision 73.24 (73.24)	Recall 76.99 (76.99) 	 P_C 29.99 	 R_C 28.80 	 F_C 28.11 	 P_O 73.24 	 R_O 76.99 	 F_O 75.07
Test: [100/402]	Time 0.500 (0.513)	Precision 88.05 (75.18)	Recall 91.50 (82.73) 	 P_C 63.96 	 R_C 66.90 	 F_C 63.87 	 P_O 73.80 	 R_O 81.43 	 F_O 77.43
Test: [200/402]	Time 0.500 (0.507)	Precision 78.43 (75.32)	Recall 86.58 (83.29) 	 P_C 65.96 	 R_C 69.72 	 F_C 66.99 	 P_O 74.30 	 R_O 82.27 	 F_O 78.08
Test: [300/402]	Time 0.501 (0.504)	Precision 80.85 (75.83)	Recall 80.85 (83.60) 	 P_C 70.53 	 R_C 75.92 	 F_C 72.50 	 P_O 74.70 	 R_O 82.55 	 F_O 78.43
Test: [400/402]	Time 0.500 (0.503)	Precision 55.61 (75.05)	Recall 63.37 (83.46) 	 P_C 72.44 	 R_C 77.29 	 F_C 74.34 	 P_O 74.11 	 R_O 82.62 	 F_O 78.14
Test: [45/51]	  P_C 72.44 	 R_C 77.28 	 F_C 74.34 	 P_O 74.10 	 R_O 82.62 	 F_O 78.13 	 mAP 80.66
Train: [0/2566]	Time 0.957 (0.957)	Loss 1.63 (1.63)	mAP 48.60 (48.60)
Train: [100/2566]	Time 0.332 (0.337)	Loss 2.68 (2.07)	mAP 53.98 (48.75)
Train: [200/2566]	Time 0.332 (0.334)	Loss 1.69 (2.04)	mAP 47.77 (48.77)
Train: [300/2566]	Time 0.330 (0.334)	Loss 2.12 (2.06)	mAP 46.03 (48.61)
Train: [400/2566]	Time 0.331 (0.333)	Loss 2.28 (2.05)	mAP 50.33 (48.50)
Train: [500/2566]	Time 0.331 (0.332)	Loss 2.26 (2.06)	mAP 39.56 (48.55)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.05 (2.07)	mAP 48.67 (48.58)
Train: [700/2566]	Time 0.330 (0.332)	Loss 1.87 (2.07)	mAP 55.08 (48.59)
Train: [800/2566]	Time 0.332 (0.332)	Loss 2.42 (2.07)	mAP 44.41 (48.60)
Train: [900/2566]	Time 0.330 (0.332)	Loss 1.95 (2.07)	mAP 47.63 (48.74)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.82 (2.07)	mAP 51.10 (48.70)
Train: [1100/2566]	Time 0.332 (0.332)	Loss 1.97 (2.07)	mAP 47.83 (48.65)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 2.20 (2.07)	mAP 45.99 (48.59)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.68 (2.07)	mAP 43.53 (48.54)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.10 (2.07)	mAP 45.20 (48.53)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.97 (2.07)	mAP 44.66 (48.54)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 2.00 (2.07)	mAP 51.57 (48.56)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 2.06 (2.07)	mAP 52.90 (48.56)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.75 (2.07)	mAP 46.72 (48.57)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.23 (2.07)	mAP 45.55 (48.58)
Train: [2000/2566]	Time 0.332 (0.331)	Loss 2.05 (2.07)	mAP 46.70 (48.56)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 1.96 (2.06)	mAP 48.19 (48.54)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 2.12 (2.06)	mAP 43.92 (48.57)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.39 (2.06)	mAP 47.28 (48.56)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.34 (2.06)	mAP 50.08 (48.58)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 2.01 (2.06)	mAP 47.75 (48.60)
Train: [46/51]	Time 0.331	Loss 2.06 	mAP 48.59
Test: [0/402]	Time 1.807 (1.807)	Precision 73.51 (73.51)	Recall 77.27 (77.27) 	 P_C 28.23 	 R_C 29.44 	 F_C 27.88 	 P_O 73.51 	 R_O 77.27 	 F_O 75.35
Test: [100/402]	Time 0.500 (0.513)	Precision 89.54 (75.73)	Recall 89.54 (82.84) 	 P_C 63.31 	 R_C 68.65 	 F_C 64.19 	 P_O 73.56 	 R_O 81.83 	 F_O 77.47
Test: [200/402]	Time 0.500 (0.506)	Precision 81.97 (76.41)	Recall 86.58 (83.08) 	 P_C 65.07 	 R_C 71.53 	 F_C 67.14 	 P_O 74.84 	 R_O 82.28 	 F_O 78.38
Test: [300/402]	Time 0.501 (0.504)	Precision 79.00 (76.10)	Recall 82.34 (83.86) 	 P_C 69.63 	 R_C 77.75 	 F_C 72.78 	 P_O 74.35 	 R_O 83.07 	 F_O 78.47
Test: [400/402]	Time 0.500 (0.503)	Precision 53.52 (74.88)	Recall 66.28 (84.02) 	 P_C 71.33 	 R_C 78.97 	 F_C 74.50 	 P_O 73.29 	 R_O 83.44 	 F_O 78.04
Test: [46/51]	  P_C 71.33 	 R_C 78.97 	 F_C 74.50 	 P_O 73.29 	 R_O 83.43 	 F_O 78.03 	 mAP 80.71
Train: [0/2566]	Time 0.971 (0.971)	Loss 1.80 (1.80)	mAP 48.81 (48.81)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.58 (2.03)	mAP 48.87 (48.85)
Train: [200/2566]	Time 0.332 (0.334)	Loss 1.84 (2.04)	mAP 50.34 (48.70)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.85 (2.06)	mAP 52.40 (48.65)
Train: [400/2566]	Time 0.332 (0.333)	Loss 2.11 (2.05)	mAP 50.07 (48.62)
Train: [500/2566]	Time 0.332 (0.333)	Loss 1.89 (2.04)	mAP 50.64 (48.65)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.21 (2.03)	mAP 50.32 (48.66)
Train: [700/2566]	Time 0.331 (0.332)	Loss 2.36 (2.03)	mAP 46.11 (48.63)
Train: [800/2566]	Time 0.331 (0.332)	Loss 2.18 (2.03)	mAP 49.03 (48.63)
Train: [900/2566]	Time 0.331 (0.332)	Loss 2.06 (2.03)	mAP 49.69 (48.55)
Train: [1000/2566]	Time 0.330 (0.332)	Loss 2.11 (2.04)	mAP 49.96 (48.56)
Train: [1100/2566]	Time 0.331 (0.332)	Loss 1.93 (2.04)	mAP 48.35 (48.63)
Train: [1200/2566]	Time 0.332 (0.332)	Loss 1.91 (2.04)	mAP 52.22 (48.64)
Train: [1300/2566]	Time 0.332 (0.332)	Loss 2.21 (2.04)	mAP 53.94 (48.68)
Train: [1400/2566]	Time 0.330 (0.332)	Loss 2.31 (2.04)	mAP 46.22 (48.70)
Train: [1500/2566]	Time 0.332 (0.332)	Loss 1.69 (2.04)	mAP 47.04 (48.68)
Train: [1600/2566]	Time 0.332 (0.332)	Loss 2.28 (2.04)	mAP 48.21 (48.66)
Train: [1700/2566]	Time 0.330 (0.332)	Loss 1.75 (2.04)	mAP 50.30 (48.68)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 2.09 (2.04)	mAP 49.83 (48.67)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.20 (2.04)	mAP 52.52 (48.64)
Train: [2000/2566]	Time 0.332 (0.331)	Loss 1.55 (2.04)	mAP 40.15 (48.65)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 2.07 (2.04)	mAP 55.99 (48.65)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.48 (2.04)	mAP 50.20 (48.66)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.88 (2.05)	mAP 50.48 (48.65)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.95 (2.04)	mAP 48.03 (48.64)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 2.16 (2.04)	mAP 52.27 (48.65)
Train: [47/51]	Time 0.331	Loss 2.04 	mAP 48.66
Test: [0/402]	Time 1.720 (1.720)	Precision 72.80 (72.80)	Recall 77.56 (77.56) 	 P_C 28.48 	 R_C 29.44 	 F_C 28.08 	 P_O 72.80 	 R_O 77.56 	 F_O 75.10
Test: [100/402]	Time 0.500 (0.513)	Precision 87.42 (74.76)	Recall 90.85 (83.28) 	 P_C 62.38 	 R_C 68.68 	 F_C 63.94 	 P_O 72.83 	 R_O 82.26 	 F_O 77.26
Test: [200/402]	Time 0.500 (0.507)	Precision 79.37 (75.64)	Recall 86.58 (83.41) 	 P_C 64.30 	 R_C 71.42 	 F_C 66.77 	 P_O 74.17 	 R_O 82.61 	 F_O 78.16
Test: [300/402]	Time 0.500 (0.504)	Precision 79.51 (75.70)	Recall 81.09 (84.02) 	 P_C 69.24 	 R_C 77.74 	 F_C 72.63 	 P_O 74.16 	 R_O 83.17 	 F_O 78.41
Test: [400/402]	Time 0.500 (0.503)	Precision 54.98 (74.82)	Recall 67.44 (84.00) 	 P_C 70.90 	 R_C 78.99 	 F_C 74.36 	 P_O 73.46 	 R_O 83.34 	 F_O 78.09
Test: [47/51]	  P_C 70.90 	 R_C 78.99 	 F_C 74.36 	 P_O 73.46 	 R_O 83.34 	 F_O 78.09 	 mAP 80.73
Train: [0/2566]	Time 0.936 (0.936)	Loss 1.72 (1.72)	mAP 54.12 (54.12)
Train: [100/2566]	Time 0.332 (0.337)	Loss 2.12 (2.01)	mAP 53.87 (48.71)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.70 (2.00)	mAP 42.24 (48.41)
Train: [300/2566]	Time 0.340 (0.335)	Loss 1.76 (2.01)	mAP 41.27 (48.66)
Train: [400/2566]	Time 0.331 (0.335)	Loss 1.88 (2.01)	mAP 52.71 (48.70)
Train: [500/2566]	Time 0.331 (0.335)	Loss 1.60 (2.02)	mAP 52.05 (48.76)
Train: [600/2566]	Time 0.330 (0.334)	Loss 1.70 (2.02)	mAP 49.51 (48.62)
Train: [700/2566]	Time 0.331 (0.334)	Loss 2.42 (2.02)	mAP 46.49 (48.59)
Train: [800/2566]	Time 0.331 (0.333)	Loss 1.99 (2.03)	mAP 46.29 (48.62)
Train: [900/2566]	Time 0.331 (0.333)	Loss 2.33 (2.03)	mAP 49.59 (48.62)
Train: [1000/2566]	Time 0.332 (0.333)	Loss 2.21 (2.03)	mAP 53.35 (48.65)
Train: [1100/2566]	Time 0.340 (0.333)	Loss 1.77 (2.03)	mAP 41.15 (48.67)
Train: [1200/2566]	Time 0.332 (0.333)	Loss 2.28 (2.03)	mAP 53.87 (48.65)
Train: [1300/2566]	Time 0.336 (0.333)	Loss 2.11 (2.03)	mAP 51.01 (48.60)
Train: [1400/2566]	Time 0.333 (0.333)	Loss 1.92 (2.03)	mAP 53.47 (48.59)
Train: [1500/2566]	Time 0.331 (0.333)	Loss 1.80 (2.03)	mAP 43.37 (48.58)
Train: [1600/2566]	Time 0.343 (0.333)	Loss 1.92 (2.03)	mAP 42.01 (48.60)
Train: [1700/2566]	Time 0.335 (0.333)	Loss 2.31 (2.03)	mAP 51.38 (48.58)
Train: [1800/2566]	Time 0.341 (0.334)	Loss 2.40 (2.03)	mAP 51.55 (48.57)
Train: [1900/2566]	Time 0.331 (0.334)	Loss 1.68 (2.03)	mAP 40.60 (48.59)
Train: [2000/2566]	Time 0.332 (0.334)	Loss 2.33 (2.03)	mAP 46.32 (48.59)
Train: [2100/2566]	Time 0.332 (0.334)	Loss 1.76 (2.03)	mAP 50.31 (48.56)
Train: [2200/2566]	Time 0.331 (0.334)	Loss 2.09 (2.03)	mAP 46.19 (48.58)
Train: [2300/2566]	Time 0.340 (0.334)	Loss 1.87 (2.04)	mAP 55.82 (48.60)
Train: [2400/2566]	Time 0.333 (0.335)	Loss 1.81 (2.04)	mAP 44.72 (48.62)
Train: [2500/2566]	Time 0.331 (0.335)	Loss 2.69 (2.04)	mAP 48.82 (48.65)
Train: [48/51]	Time 0.335	Loss 2.04 	mAP 48.68
Test: [0/402]	Time 2.414 (2.414)	Precision 72.53 (72.53)	Recall 77.27 (77.27) 	 P_C 28.26 	 R_C 29.72 	 F_C 27.94 	 P_O 72.53 	 R_O 77.27 	 F_O 74.83
Test: [100/402]	Time 0.509 (0.524)	Precision 87.90 (74.46)	Recall 90.20 (83.69) 	 P_C 61.66 	 R_C 69.21 	 F_C 64.03 	 P_O 72.36 	 R_O 82.70 	 F_O 77.19
Test: [200/402]	Time 0.499 (0.514)	Precision 78.82 (75.07)	Recall 87.01 (83.82) 	 P_C 63.93 	 R_C 71.94 	 F_C 67.03 	 P_O 73.48 	 R_O 83.04 	 F_O 77.97
Test: [300/402]	Time 0.500 (0.510)	Precision 77.93 (74.86)	Recall 82.59 (84.55) 	 P_C 68.65 	 R_C 78.07 	 F_C 72.48 	 P_O 73.10 	 R_O 83.80 	 F_O 78.08
Test: [400/402]	Time 0.499 (0.507)	Precision 54.17 (73.72)	Recall 68.02 (84.64) 	 P_C 70.33 	 R_C 79.39 	 F_C 74.17 	 P_O 72.15 	 R_O 84.09 	 F_O 77.66
Test: [48/51]	  P_C 70.34 	 R_C 79.39 	 F_C 74.17 	 P_O 72.15 	 R_O 84.08 	 F_O 77.66 	 mAP 80.72
Train: [0/2566]	Time 1.012 (1.012)	Loss 1.95 (1.95)	mAP 53.02 (53.02)
Train: [100/2566]	Time 0.332 (0.340)	Loss 2.16 (2.05)	mAP 53.74 (48.94)
Train: [200/2566]	Time 0.332 (0.337)	Loss 1.73 (2.04)	mAP 51.32 (48.83)
Train: [300/2566]	Time 0.332 (0.335)	Loss 2.65 (2.05)	mAP 48.25 (49.10)
Train: [400/2566]	Time 0.331 (0.334)	Loss 1.44 (2.04)	mAP 51.93 (49.02)
Train: [500/2566]	Time 0.331 (0.334)	Loss 2.35 (2.04)	mAP 42.66 (48.88)
Train: [600/2566]	Time 0.331 (0.333)	Loss 1.92 (2.05)	mAP 53.71 (48.91)
Train: [700/2566]	Time 0.347 (0.333)	Loss 2.08 (2.04)	mAP 47.98 (48.89)
Train: [800/2566]	Time 0.331 (0.333)	Loss 2.00 (2.04)	mAP 45.26 (48.93)
Train: [900/2566]	Time 0.330 (0.333)	Loss 2.44 (2.04)	mAP 48.85 (48.87)
Train: [1000/2566]	Time 0.332 (0.332)	Loss 1.81 (2.04)	mAP 51.25 (48.83)
Train: [1100/2566]	Time 0.330 (0.332)	Loss 1.68 (2.04)	mAP 50.97 (48.81)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 1.89 (2.05)	mAP 46.36 (48.75)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 2.95 (2.04)	mAP 44.30 (48.76)
Train: [1400/2566]	Time 0.331 (0.332)	Loss 1.49 (2.04)	mAP 47.02 (48.69)
Train: [1500/2566]	Time 0.331 (0.332)	Loss 1.76 (2.04)	mAP 47.72 (48.66)
Train: [1600/2566]	Time 0.341 (0.332)	Loss 2.30 (2.04)	mAP 48.69 (48.64)
Train: [1700/2566]	Time 0.348 (0.333)	Loss 1.85 (2.04)	mAP 53.85 (48.66)
Train: [1800/2566]	Time 0.338 (0.333)	Loss 1.93 (2.04)	mAP 47.41 (48.66)
Train: [1900/2566]	Time 0.334 (0.333)	Loss 2.29 (2.04)	mAP 50.32 (48.65)
Train: [2000/2566]	Time 0.338 (0.333)	Loss 1.69 (2.04)	mAP 42.69 (48.64)
Train: [2100/2566]	Time 0.331 (0.333)	Loss 2.35 (2.04)	mAP 44.12 (48.65)
Train: [2200/2566]	Time 0.338 (0.334)	Loss 2.03 (2.04)	mAP 51.51 (48.67)
Train: [2300/2566]	Time 0.340 (0.334)	Loss 2.04 (2.04)	mAP 45.65 (48.65)
Train: [2400/2566]	Time 0.333 (0.334)	Loss 2.16 (2.04)	mAP 48.95 (48.65)
Train: [2500/2566]	Time 0.338 (0.334)	Loss 2.03 (2.04)	mAP 43.43 (48.68)
Train: [49/51]	Time 0.334	Loss 2.04 	mAP 48.66
Test: [0/402]	Time 2.015 (2.015)	Precision 73.44 (73.44)	Recall 76.99 (76.99) 	 P_C 29.79 	 R_C 29.71 	 F_C 28.63 	 P_O 73.44 	 R_O 76.99 	 F_O 75.17
Test: [100/402]	Time 0.499 (0.520)	Precision 91.28 (76.00)	Recall 88.89 (82.72) 	 P_C 65.39 	 R_C 67.37 	 F_C 64.90 	 P_O 74.11 	 R_O 81.62 	 F_O 77.68
Test: [200/402]	Time 0.503 (0.513)	Precision 80.40 (76.82)	Recall 87.01 (83.04) 	 P_C 67.26 	 R_C 70.11 	 F_C 67.88 	 P_O 75.39 	 R_O 82.18 	 F_O 78.64
Test: [300/402]	Time 0.500 (0.509)	Precision 78.83 (76.91)	Recall 80.60 (83.59) 	 P_C 71.50 	 R_C 76.42 	 F_C 73.29 	 P_O 75.37 	 R_O 82.69 	 F_O 78.86
Test: [400/402]	Time 0.500 (0.506)	Precision 54.68 (76.00)	Recall 64.53 (83.53) 	 P_C 73.18 	 R_C 77.80 	 F_C 74.98 	 P_O 74.66 	 R_O 82.85 	 F_O 78.54
Test: [49/51]	  P_C 73.18 	 R_C 77.80 	 F_C 74.98 	 P_O 74.65 	 R_O 82.84 	 F_O 78.53 	 mAP 80.83
Train: [0/2566]	Time 0.978 (0.978)	Loss 1.92 (1.92)	mAP 53.67 (53.67)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.54 (2.08)	mAP 52.83 (48.68)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.75 (2.01)	mAP 40.98 (48.50)
Train: [300/2566]	Time 0.330 (0.333)	Loss 2.42 (2.03)	mAP 43.61 (48.72)
Train: [400/2566]	Time 0.330 (0.333)	Loss 2.06 (2.05)	mAP 54.90 (48.74)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.67 (2.05)	mAP 50.17 (48.68)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.88 (2.05)	mAP 40.27 (48.80)
Train: [700/2566]	Time 0.331 (0.332)	Loss 2.03 (2.04)	mAP 52.52 (48.78)
Train: [800/2566]	Time 0.330 (0.332)	Loss 1.99 (2.04)	mAP 43.73 (48.69)
Train: [900/2566]	Time 0.332 (0.332)	Loss 1.84 (2.04)	mAP 43.83 (48.64)
Train: [1000/2566]	Time 0.345 (0.333)	Loss 1.81 (2.03)	mAP 44.73 (48.60)
Train: [1100/2566]	Time 0.335 (0.333)	Loss 2.58 (2.03)	mAP 50.45 (48.53)
Train: [1200/2566]	Time 0.331 (0.334)	Loss 2.12 (2.03)	mAP 54.68 (48.50)
Train: [1300/2566]	Time 0.336 (0.334)	Loss 2.61 (2.03)	mAP 57.68 (48.50)
Train: [1400/2566]	Time 0.342 (0.334)	Loss 2.17 (2.03)	mAP 48.04 (48.52)
Train: [1500/2566]	Time 0.332 (0.334)	Loss 2.11 (2.03)	mAP 51.26 (48.50)
Train: [1600/2566]	Time 0.339 (0.335)	Loss 1.67 (2.03)	mAP 51.24 (48.58)
Train: [1700/2566]	Time 0.333 (0.335)	Loss 2.04 (2.03)	mAP 48.55 (48.56)
Train: [1800/2566]	Time 0.336 (0.335)	Loss 1.85 (2.03)	mAP 44.85 (48.58)
Train: [1900/2566]	Time 0.341 (0.335)	Loss 1.92 (2.03)	mAP 45.36 (48.58)
Train: [2000/2566]	Time 0.337 (0.335)	Loss 2.28 (2.03)	mAP 52.69 (48.61)
Train: [2100/2566]	Time 0.330 (0.335)	Loss 2.29 (2.04)	mAP 49.52 (48.60)
Train: [2200/2566]	Time 0.342 (0.335)	Loss 2.28 (2.04)	mAP 52.26 (48.60)
Train: [2300/2566]	Time 0.335 (0.335)	Loss 2.06 (2.04)	mAP 50.06 (48.58)
Train: [2400/2566]	Time 0.331 (0.335)	Loss 2.82 (2.04)	mAP 47.68 (48.56)
Train: [2500/2566]	Time 0.331 (0.335)	Loss 2.35 (2.04)	mAP 48.70 (48.56)
Train: [50/51]	Time 0.335	Loss 2.04 	mAP 48.56
Test: [0/402]	Time 1.734 (1.734)	Precision 72.92 (72.92)	Recall 77.27 (77.27) 	 P_C 27.91 	 R_C 29.58 	 F_C 27.66 	 P_O 72.92 	 R_O 77.27 	 F_O 75.03
Test: [100/402]	Time 0.500 (0.512)	Precision 89.86 (76.00)	Recall 86.93 (82.69) 	 P_C 64.40 	 R_C 67.82 	 F_C 64.76 	 P_O 73.96 	 R_O 81.65 	 F_O 77.62
Test: [200/402]	Time 0.500 (0.506)	Precision 80.57 (76.91)	Recall 86.15 (82.96) 	 P_C 66.10 	 R_C 70.40 	 F_C 67.52 	 P_O 75.36 	 R_O 82.15 	 F_O 78.61
Test: [300/402]	Time 0.500 (0.504)	Precision 79.37 (76.99)	Recall 81.34 (83.51) 	 P_C 70.69 	 R_C 76.76 	 F_C 73.06 	 P_O 75.35 	 R_O 82.65 	 F_O 78.83
Test: [400/402]	Time 0.500 (0.503)	Precision 56.37 (76.00)	Recall 66.86 (83.50) 	 P_C 72.45 	 R_C 78.14 	 F_C 74.82 	 P_O 74.58 	 R_O 82.84 	 F_O 78.49
Test: [50/51]	  P_C 72.45 	 R_C 78.14 	 F_C 74.82 	 P_O 74.58 	 R_O 82.83 	 F_O 78.49 	 mAP 80.76
Train: [0/2566]	Time 0.946 (0.946)	Loss 1.90 (1.90)	mAP 43.48 (43.48)
Train: [100/2566]	Time 0.337 (0.342)	Loss 1.78 (2.00)	mAP 48.96 (47.64)
Train: [200/2566]	Time 0.338 (0.339)	Loss 1.69 (2.02)	mAP 40.49 (48.13)
Train: [300/2566]	Time 0.337 (0.338)	Loss 1.70 (2.02)	mAP 52.37 (48.36)
Train: [400/2566]	Time 0.338 (0.338)	Loss 1.86 (2.03)	mAP 51.47 (48.44)
Train: [500/2566]	Time 0.334 (0.338)	Loss 1.64 (2.03)	mAP 49.67 (48.61)
Train: [600/2566]	Time 0.333 (0.337)	Loss 3.00 (2.04)	mAP 54.22 (48.73)
Train: [700/2566]	Time 0.335 (0.337)	Loss 2.14 (2.04)	mAP 41.47 (48.72)
Train: [800/2566]	Time 0.337 (0.337)	Loss 2.22 (2.03)	mAP 58.63 (48.65)
Train: [900/2566]	Time 0.336 (0.337)	Loss 1.77 (2.04)	mAP 39.94 (48.63)
Train: [1000/2566]	Time 0.335 (0.337)	Loss 1.85 (2.03)	mAP 56.40 (48.59)
Train: [1100/2566]	Time 0.344 (0.337)	Loss 1.56 (2.03)	mAP 42.35 (48.59)
Train: [1200/2566]	Time 0.335 (0.337)	Loss 2.28 (2.03)	mAP 58.54 (48.60)
Train: [1300/2566]	Time 0.332 (0.336)	Loss 2.22 (2.03)	mAP 44.80 (48.60)
Train: [1400/2566]	Time 0.332 (0.337)	Loss 2.01 (2.03)	mAP 42.48 (48.56)
Train: [1500/2566]	Time 0.342 (0.336)	Loss 1.68 (2.03)	mAP 51.66 (48.56)
Train: [1600/2566]	Time 0.334 (0.336)	Loss 1.77 (2.03)	mAP 43.35 (48.59)
Train: [1700/2566]	Time 0.335 (0.336)	Loss 2.04 (2.03)	mAP 45.86 (48.56)
Train: [1800/2566]	Time 0.336 (0.336)	Loss 2.13 (2.03)	mAP 47.89 (48.53)
Train: [1900/2566]	Time 0.337 (0.336)	Loss 1.87 (2.03)	mAP 45.46 (48.58)
Train: [2000/2566]	Time 0.332 (0.336)	Loss 1.99 (2.03)	mAP 54.27 (48.59)
Train: [2100/2566]	Time 0.331 (0.336)	Loss 2.23 (2.03)	mAP 50.12 (48.61)
Train: [2200/2566]	Time 0.343 (0.336)	Loss 2.31 (2.03)	mAP 49.42 (48.60)
Train: [2300/2566]	Time 0.332 (0.336)	Loss 1.96 (2.03)	mAP 58.32 (48.59)
Train: [2400/2566]	Time 0.333 (0.336)	Loss 2.08 (2.03)	mAP 48.78 (48.64)
Train: [2500/2566]	Time 0.331 (0.336)	Loss 2.27 (2.03)	mAP 52.33 (48.63)
Train: [51/51]	Time 0.336	Loss 2.04 	mAP 48.61
Test: [0/402]	Time 2.105 (2.105)	Precision 73.51 (73.51)	Recall 77.27 (77.27) 	 P_C 29.50 	 R_C 29.51 	 F_C 28.40 	 P_O 73.51 	 R_O 77.27 	 F_O 75.35
Test: [100/402]	Time 0.504 (0.520)	Precision 90.73 (75.58)	Recall 89.54 (82.95) 	 P_C 64.04 	 R_C 67.87 	 F_C 64.52 	 P_O 73.64 	 R_O 81.90 	 F_O 77.55
Test: [200/402]	Time 0.506 (0.512)	Precision 80.00 (76.21)	Recall 86.58 (83.27) 	 P_C 65.81 	 R_C 70.56 	 F_C 67.42 	 P_O 74.74 	 R_O 82.47 	 F_O 78.41
Test: [300/402]	Time 0.506 (0.509)	Precision 79.28 (76.34)	Recall 81.84 (83.87) 	 P_C 70.44 	 R_C 76.89 	 F_C 72.99 	 P_O 74.76 	 R_O 83.03 	 F_O 78.68
Test: [400/402]	Time 0.508 (0.508)	Precision 55.07 (75.37)	Recall 66.28 (83.86) 	 P_C 72.21 	 R_C 78.27 	 F_C 74.72 	 P_O 74.01 	 R_O 83.22 	 F_O 78.34
Test: [51/51]	  P_C 72.21 	 R_C 78.27 	 F_C 74.72 	 P_O 74.00 	 R_O 83.21 	 F_O 78.34 	 mAP 80.80
Evaluating the best model
Evaluate with threshold 0.50
... loading pretrained weights from ./output/coco-DualCoop-RN101-cosine-bs32-e51/model_best.pth.tar
Test: [0/402]	Time 1.880 (1.880)	Precision 73.44 (73.44)	Recall 76.99 (76.99) 	 P_C 29.79 	 R_C 29.71 	 F_C 28.63 	 P_O 73.44 	 R_O 76.99 	 F_O 75.17
Test: [100/402]	Time 0.504 (0.515)	Precision 91.28 (76.00)	Recall 88.89 (82.72) 	 P_C 65.39 	 R_C 67.37 	 F_C 64.90 	 P_O 74.11 	 R_O 81.62 	 F_O 77.68
Test: [200/402]	Time 0.503 (0.510)	Precision 80.40 (76.82)	Recall 87.01 (83.04) 	 P_C 67.26 	 R_C 70.11 	 F_C 67.88 	 P_O 75.39 	 R_O 82.18 	 F_O 78.64
Test: [300/402]	Time 0.500 (0.508)	Precision 78.83 (76.91)	Recall 80.60 (83.59) 	 P_C 71.50 	 R_C 76.42 	 F_C 73.29 	 P_O 75.37 	 R_O 82.69 	 F_O 78.86
Test: [400/402]	Time 0.499 (0.506)	Precision 54.68 (76.00)	Recall 64.53 (83.53) 	 P_C 73.18 	 R_C 77.80 	 F_C 74.98 	 P_O 74.66 	 R_O 82.85 	 F_O 78.54
Test: [49/51]	  P_C 73.18 	 R_C 77.80 	 F_C 74.98 	 P_O 74.65 	 R_O 82.84 	 F_O 78.53 	 mAP 80.83
