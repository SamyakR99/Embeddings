nohup: ignoring input
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/multilabel/data/coco14
data_split = train2014
PARTIAL_PORTION= 0.900000
INPUT.SIZE = 448
loading annotations into memory...
Done (t=7.00s)
creating index...
index created!
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/multilabel/data/coco14
data_split = val2014
PARTIAL_PORTION= 0.900000
INPUT.SIZE = 448
loading annotations into memory...
Done (t=3.03s)
creating index...
index created!
 -------------------- Building Dataset ----------------------
DATASET.ROOT = /home/samyakr2/multilabel/data/coco14
data_split = val2014
PARTIAL_PORTION= 0.900000
INPUT.SIZE = 448
loading annotations into memory...
Done (t=3.23s)
creating index...
index created!
Loading CLIP (backbone: RN101)
Building dualcoop
Freeze the backbone weights
Freeze the attn weights
image_encoder.attnpool.positional_embedding
image_encoder.attnpool.k_proj.weight
image_encoder.attnpool.k_proj.bias
image_encoder.attnpool.q_proj.weight
image_encoder.attnpool.q_proj.bias
image_encoder.attnpool.v_proj.weight
image_encoder.attnpool.v_proj.bias
image_encoder.attnpool.c_proj.weight
image_encoder.attnpool.c_proj.bias
train.py --config_file configs/models/rn101_ep50.yaml --datadir /home/samyakr2/multilabel/data/coco14 --dataset_config_file /home/samyakr2/Redundancy/DualCoOp/configs/datasets/coco.yaml --input_size 448 --lr 0.002 --loss_w 0.05 -pp 0.9 --csc --max_epochs 50
Namespace(prefix='', resume=None, pretrained=None, auto_resume=False, datadir='/home/samyakr2/multilabel/data/coco14', input_size=448, train_input_size=None, num_train_cls=100, test_input_size=None, thre=0.5, single_prompt='pos', output_dir='', print_freq=100, val_freq_in_epoch=-1, evaluate=False, config_file='configs/models/rn101_ep50.yaml', dataset_config_file='/home/samyakr2/Redundancy/DualCoOp/configs/datasets/coco.yaml', positive_prompt=None, negative_prompt=None, n_ctx_pos=None, n_ctx_neg=None, lr=0.002, loss_w=0.05, csc=True, logit_scale=100.0, gamma_neg=2.0, gamma_pos=1.0, portion=1.0, partial_portion=0.9, mask_file=None, train_batch_size=None, stop_epochs=None, max_epochs=50, finetune=False, finetune_backbone=False, finetune_attn=False, finetune_text=False, base_lr_mult=None, backbone_lr_mult=None, text_lr_mult=None, attn_lr_mult=None, val_every_n_epochs=1, warmup_epochs=1, top_k=3)
DualCoop(
  (image_encoder): ModifiedResNet_conv_proj(
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (relu): ReLU(inplace=True)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (avgpool): Identity()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (attnpool): AttentionConv(
      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (c_proj): Linear(in_features=2048, out_features=512, bias=True)
    )
  )
  (text_encoder): TextEncoder(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (clip_model): CLIP_conv_proj(
    (visual): ModifiedResNet_conv_proj(
      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
      (relu): ReLU(inplace=True)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (attnpool): AttentionConv(
        (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (c_proj): Linear(in_features=2048, out_features=512, bias=True)
      )
    )
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (text_projector): Sequential(
    (0): Linear(in_features=512, out_features=384, bias=False)
    (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=384, out_features=256, bias=False)
  )
  (image_projector): Sequential(
    (0): Linear(in_features=512, out_features=256, bias=False)
  )
)
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 3
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
    SHUFFLE: False
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    PARTIAL_PORTION: 0.9
    PORTION: 1.0
    SAMPLER: RandomSampler
    SHUFFLE: True
  VAL:
    BATCH_SIZE: 100
    SHUFFLE: False
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  MASK_FILE: None
  NAME: coco
  NUM_LABELED: -1
  NUM_SHOTS: -1
  ROOT: /home/samyakr2/multilabel/data/coco14
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  TARGET_DOMAINS: ()
  TEST_GZSL_SPLIT: val2014
  TEST_SPLIT: val2014
  TRAIN_SPLIT: train2014
  VAL_GZSL_SPLIT: val2014
  VAL_PERCENT: 0.1
  VAL_SPLIT: val2014
  ZS_TEST: instances_val2014_gzsi_48_17.json
  ZS_TEST_UNSEEN: instances_val2014_unseen_48_17.json
  ZS_TRAIN: instances_train2014_seen_48_17.json
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (448, 448)
  TEST:
    SIZE: (448, 448)
  TRAIN:
    SIZE: (448, 448)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MLCCLIP:
  FLOAT: False
  NEGATIVE_PROMPT: 
  POSITIVE_PROMPT: 
MODEL:
  BACKBONE:
    NAME: RN101
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  ATTN_LR_MULT: 0.1
  BACKBONE_LR_MULT: 0.1
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: ./output
RESUME: 
SEED: -1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 100
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COOP_MLC:
    ASL_GAMMA_NEG: 2.0
    ASL_GAMMA_POS: 1.0
    CSC: True
    LS: 100.0
    NEGATIVE_PROMPT_INIT: 
    N_CTX_NEG: 16
    N_CTX_POS: 16
    POSITIVE_PROMPT_INIT: 
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FINETUNE: False
  FINETUNE_ATTN: False
  FINETUNE_BACKBONE: False
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: 
  RESNET_IMAGENET:
    DEPTH: 50
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
/home/samyakr2/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:836: UserWarning: non-inplace resize is deprecated
  warnings.warn("non-inplace resize is deprecated")
Train: [0/2566]	Time 1.987 (1.987)	Loss 20.80 (20.80)	mAP 10.28 (10.28)
Train: [100/2566]	Time 0.323 (0.338)	Loss 5.66 (6.94)	mAP 10.56 (10.09)
Train: [200/2566]	Time 0.328 (0.332)	Loss 4.99 (6.11)	mAP 10.36 (10.61)
Train: [300/2566]	Time 0.329 (0.331)	Loss 5.28 (5.72)	mAP 13.68 (11.02)
Train: [400/2566]	Time 0.330 (0.330)	Loss 4.94 (5.49)	mAP 14.15 (11.57)
Train: [500/2566]	Time 0.330 (0.330)	Loss 4.22 (5.33)	mAP 16.66 (12.26)
Train: [600/2566]	Time 0.329 (0.330)	Loss 4.47 (5.20)	mAP 17.36 (12.87)
Train: [700/2566]	Time 0.329 (0.330)	Loss 4.30 (5.11)	mAP 16.88 (13.42)
Train: [800/2566]	Time 0.330 (0.330)	Loss 4.60 (5.03)	mAP 15.84 (13.92)
Train: [900/2566]	Time 0.330 (0.330)	Loss 4.30 (4.96)	mAP 17.97 (14.41)
Train: [1000/2566]	Time 0.330 (0.330)	Loss 3.91 (4.90)	mAP 20.88 (14.86)
Train: [1100/2566]	Time 0.330 (0.330)	Loss 4.18 (4.84)	mAP 17.43 (15.26)
Train: [1200/2566]	Time 0.330 (0.330)	Loss 4.57 (4.79)	mAP 17.56 (15.69)
Train: [1300/2566]	Time 0.330 (0.330)	Loss 4.38 (4.74)	mAP 20.37 (16.03)
Train: [1400/2566]	Time 0.330 (0.330)	Loss 4.22 (4.69)	mAP 19.24 (16.33)
Train: [1500/2566]	Time 0.330 (0.330)	Loss 4.22 (4.66)	mAP 22.45 (16.66)
Train: [1600/2566]	Time 0.331 (0.330)	Loss 3.54 (4.62)	mAP 19.20 (16.98)
Train: [1700/2566]	Time 0.331 (0.330)	Loss 3.59 (4.58)	mAP 24.66 (17.29)
Train: [1800/2566]	Time 0.331 (0.330)	Loss 3.78 (4.55)	mAP 23.66 (17.60)
Train: [1900/2566]	Time 0.330 (0.330)	Loss 3.93 (4.51)	mAP 22.20 (17.90)
Train: [2000/2566]	Time 0.331 (0.330)	Loss 3.34 (4.48)	mAP 23.40 (18.17)
Train: [2100/2566]	Time 0.330 (0.330)	Loss 3.53 (4.45)	mAP 23.73 (18.48)
Train: [2200/2566]	Time 0.331 (0.330)	Loss 3.25 (4.43)	mAP 29.81 (18.79)
Train: [2300/2566]	Time 0.330 (0.330)	Loss 3.23 (4.40)	mAP 25.16 (19.04)
Train: [2400/2566]	Time 0.330 (0.330)	Loss 4.23 (4.37)	mAP 21.86 (19.30)
Train: [2500/2566]	Time 0.330 (0.330)	Loss 3.40 (4.35)	mAP 29.76 (19.56)
Train: [1/50]	Time 0.330	Loss 4.33 	mAP 19.72
Test: [0/402]	Time 2.039 (2.039)	Precision 38.32 (38.32)	Recall 36.36 (36.36) 	 P_C 10.80 	 R_C 13.75 	 F_C 10.56 	 P_O 38.32 	 R_O 36.36 	 F_O 37.32
Test: [100/402]	Time 0.500 (0.515)	Precision 23.96 (40.40)	Recall 15.03 (42.66) 	 P_C 23.01 	 R_C 23.76 	 F_C 20.58 	 P_O 41.93 	 R_O 43.92 	 F_O 42.90
Test: [200/402]	Time 0.499 (0.508)	Precision 39.73 (38.22)	Recall 38.53 (38.93) 	 P_C 22.51 	 R_C 24.03 	 F_C 20.89 	 P_O 39.37 	 R_O 40.16 	 F_O 39.76
Test: [300/402]	Time 0.500 (0.505)	Precision 50.79 (40.63)	Recall 47.76 (40.63) 	 P_C 24.78 	 R_C 25.60 	 F_C 22.98 	 P_O 41.28 	 R_O 41.70 	 F_O 41.49
Test: [400/402]	Time 0.499 (0.504)	Precision 25.41 (41.78)	Recall 26.74 (41.80) 	 P_C 25.88 	 R_C 26.44 	 F_C 24.11 	 P_O 42.69 	 R_O 43.55 	 F_O 43.12
Test: [1/50]	  P_C 25.88 	 R_C 26.43 	 F_C 24.11 	 P_O 42.67 	 R_O 43.54 	 F_O 43.10 	 mAP 23.19
Train: [0/2566]	Time 1.052 (1.052)	Loss 3.96 (3.96)	mAP 27.44 (27.44)
Train: [100/2566]	Time 0.330 (0.337)	Loss 4.98 (5.14)	mAP 17.24 (15.72)
Train: [200/2566]	Time 0.331 (0.334)	Loss 4.48 (4.67)	mAP 24.91 (18.40)
Train: [300/2566]	Time 0.330 (0.333)	Loss 3.26 (4.39)	mAP 28.30 (21.39)
Train: [400/2566]	Time 0.331 (0.332)	Loss 3.15 (4.16)	mAP 31.68 (23.67)
Train: [500/2566]	Time 0.333 (0.332)	Loss 2.70 (3.96)	mAP 30.16 (25.62)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.28 (3.80)	mAP 28.42 (27.25)
Train: [700/2566]	Time 0.331 (0.332)	Loss 2.39 (3.66)	mAP 35.07 (28.67)
Train: [800/2566]	Time 0.331 (0.331)	Loss 3.14 (3.53)	mAP 34.19 (29.91)
Train: [900/2566]	Time 0.331 (0.331)	Loss 2.13 (3.43)	mAP 40.02 (30.97)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 2.60 (3.33)	mAP 39.91 (31.92)
Train: [1100/2566]	Time 0.329 (0.331)	Loss 1.97 (3.26)	mAP 40.00 (32.74)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 2.09 (3.18)	mAP 41.34 (33.44)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 2.30 (3.11)	mAP 43.44 (34.06)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.53 (3.06)	mAP 40.58 (34.68)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.05 (3.01)	mAP 47.82 (35.17)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.96 (2.96)	mAP 41.19 (35.62)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.91 (2.91)	mAP 39.92 (36.07)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 2.57 (2.87)	mAP 31.99 (36.43)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.90 (2.83)	mAP 39.00 (36.74)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.76 (2.79)	mAP 39.91 (37.02)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 2.40 (2.76)	mAP 43.49 (37.29)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.52 (2.73)	mAP 40.40 (37.54)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 2.11 (2.70)	mAP 40.00 (37.77)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.20 (2.68)	mAP 41.54 (38.00)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 3.10 (2.66)	mAP 45.32 (38.20)
Train: [2/50]	Time 0.331	Loss 2.64 	mAP 38.35
Test: [0/402]	Time 1.752 (1.752)	Precision 69.10 (69.10)	Recall 78.12 (78.12) 	 P_C 28.37 	 R_C 31.90 	 F_C 28.28 	 P_O 69.10 	 R_O 78.12 	 F_O 73.33
Test: [100/402]	Time 0.500 (0.512)	Precision 84.28 (70.00)	Recall 87.58 (82.60) 	 P_C 60.18 	 R_C 66.85 	 F_C 61.01 	 P_O 67.16 	 R_O 81.55 	 F_O 73.66
Test: [200/402]	Time 0.499 (0.506)	Precision 79.68 (71.03)	Recall 86.58 (82.36) 	 P_C 62.41 	 R_C 69.52 	 F_C 63.66 	 P_O 68.85 	 R_O 81.56 	 F_O 74.67
Test: [300/402]	Time 0.500 (0.504)	Precision 76.35 (70.20)	Recall 81.09 (83.34) 	 P_C 65.84 	 R_C 76.23 	 F_C 68.85 	 P_O 67.62 	 R_O 82.51 	 F_O 74.33
Test: [400/402]	Time 0.500 (0.503)	Precision 44.00 (68.84)	Recall 70.35 (83.41) 	 P_C 67.62 	 R_C 77.35 	 F_C 70.52 	 P_O 66.44 	 R_O 82.78 	 F_O 73.71
Test: [2/50]	  P_C 67.61 	 R_C 77.35 	 F_C 70.52 	 P_O 66.42 	 R_O 82.77 	 F_O 73.70 	 mAP 76.51
Train: [0/2566]	Time 1.015 (1.015)	Loss 1.93 (1.93)	mAP 42.10 (42.10)
Train: [100/2566]	Time 0.329 (0.337)	Loss 1.75 (2.03)	mAP 37.46 (42.64)
Train: [200/2566]	Time 0.330 (0.333)	Loss 1.82 (2.03)	mAP 40.47 (42.87)
Train: [300/2566]	Time 0.329 (0.332)	Loss 2.21 (2.02)	mAP 43.37 (43.16)
Train: [400/2566]	Time 0.330 (0.332)	Loss 2.05 (2.03)	mAP 41.49 (43.19)
Train: [500/2566]	Time 0.330 (0.331)	Loss 1.80 (2.03)	mAP 38.98 (43.31)
Train: [600/2566]	Time 0.331 (0.331)	Loss 1.96 (2.03)	mAP 41.03 (43.22)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.69 (2.02)	mAP 45.70 (43.38)
Train: [800/2566]	Time 0.330 (0.331)	Loss 2.39 (2.02)	mAP 44.40 (43.41)
Train: [900/2566]	Time 0.330 (0.331)	Loss 2.29 (2.01)	mAP 43.44 (43.38)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.86 (2.01)	mAP 47.60 (43.42)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 2.54 (2.01)	mAP 42.04 (43.49)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 2.27 (2.00)	mAP 43.84 (43.54)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 2.05 (2.00)	mAP 46.60 (43.57)
Train: [1400/2566]	Time 0.329 (0.331)	Loss 1.61 (2.00)	mAP 46.67 (43.64)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.74 (2.00)	mAP 41.92 (43.65)
Train: [1600/2566]	Time 0.331 (0.330)	Loss 1.60 (2.00)	mAP 48.21 (43.70)
Train: [1700/2566]	Time 0.330 (0.330)	Loss 1.56 (2.00)	mAP 43.71 (43.71)
Train: [1800/2566]	Time 0.331 (0.330)	Loss 1.97 (2.00)	mAP 42.70 (43.72)
Train: [1900/2566]	Time 0.330 (0.330)	Loss 1.38 (2.00)	mAP 43.02 (43.75)
Train: [2000/2566]	Time 0.331 (0.330)	Loss 1.96 (1.99)	mAP 42.91 (43.78)
Train: [2100/2566]	Time 0.331 (0.330)	Loss 2.40 (1.99)	mAP 37.04 (43.80)
Train: [2200/2566]	Time 0.331 (0.330)	Loss 1.77 (1.98)	mAP 46.43 (43.77)
Train: [2300/2566]	Time 0.331 (0.330)	Loss 1.94 (1.98)	mAP 45.06 (43.75)
Train: [2400/2566]	Time 0.331 (0.330)	Loss 2.09 (1.98)	mAP 46.56 (43.73)
Train: [2500/2566]	Time 0.330 (0.330)	Loss 2.38 (1.98)	mAP 45.28 (43.75)
Train: [3/50]	Time 0.330	Loss 1.98 	mAP 43.79
Test: [0/402]	Time 1.821 (1.821)	Precision 64.87 (64.87)	Recall 78.69 (78.69) 	 P_C 25.61 	 R_C 28.57 	 F_C 25.62 	 P_O 64.87 	 R_O 78.69 	 F_O 71.12
Test: [100/402]	Time 0.500 (0.513)	Precision 80.81 (70.42)	Recall 90.85 (84.10) 	 P_C 60.43 	 R_C 68.86 	 F_C 62.51 	 P_O 69.19 	 R_O 82.65 	 F_O 75.32
Test: [200/402]	Time 0.500 (0.506)	Precision 73.36 (69.99)	Recall 87.01 (84.63) 	 P_C 63.21 	 R_C 71.40 	 F_C 65.53 	 P_O 69.13 	 R_O 83.56 	 F_O 75.66
Test: [300/402]	Time 0.501 (0.504)	Precision 76.54 (70.32)	Recall 83.58 (85.07) 	 P_C 67.46 	 R_C 77.48 	 F_C 71.17 	 P_O 69.28 	 R_O 84.01 	 F_O 75.94
Test: [400/402]	Time 0.500 (0.503)	Precision 51.93 (69.43)	Recall 70.35 (85.09) 	 P_C 69.31 	 R_C 79.07 	 F_C 73.04 	 P_O 68.53 	 R_O 84.29 	 F_O 75.60
Test: [3/50]	  P_C 69.31 	 R_C 79.07 	 F_C 73.03 	 P_O 68.52 	 R_O 84.28 	 F_O 75.59 	 mAP 80.01
Train: [0/2566]	Time 0.992 (0.992)	Loss 1.94 (1.94)	mAP 40.88 (40.88)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.76 (1.91)	mAP 44.97 (43.98)
Train: [200/2566]	Time 0.331 (0.333)	Loss 2.05 (1.90)	mAP 48.14 (43.97)
Train: [300/2566]	Time 0.330 (0.333)	Loss 2.03 (1.91)	mAP 44.86 (44.06)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.58 (1.91)	mAP 44.71 (43.91)
Train: [500/2566]	Time 0.330 (0.332)	Loss 2.04 (1.90)	mAP 46.45 (43.87)
Train: [600/2566]	Time 0.330 (0.331)	Loss 2.11 (1.91)	mAP 53.71 (44.07)
Train: [700/2566]	Time 0.331 (0.331)	Loss 1.75 (1.91)	mAP 46.69 (44.15)
Train: [800/2566]	Time 0.331 (0.331)	Loss 2.37 (1.92)	mAP 43.83 (44.17)
Train: [900/2566]	Time 0.331 (0.331)	Loss 1.94 (1.92)	mAP 39.56 (44.13)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.88 (1.91)	mAP 48.24 (44.08)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 2.48 (1.91)	mAP 38.76 (44.07)
Train: [1200/2566]	Time 0.329 (0.331)	Loss 2.09 (1.91)	mAP 40.02 (44.10)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.79 (1.91)	mAP 38.17 (44.14)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 2.05 (1.90)	mAP 40.27 (44.13)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.96 (1.91)	mAP 43.29 (44.12)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.63 (1.90)	mAP 42.19 (44.18)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.79 (1.90)	mAP 44.63 (44.19)
Train: [1800/2566]	Time 0.329 (0.331)	Loss 1.97 (1.90)	mAP 46.26 (44.20)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.33 (1.90)	mAP 41.99 (44.17)
Train: [2000/2566]	Time 0.329 (0.331)	Loss 1.85 (1.90)	mAP 51.62 (44.21)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 2.04 (1.90)	mAP 44.66 (44.22)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.97 (1.90)	mAP 43.22 (44.25)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.83 (1.90)	mAP 47.57 (44.26)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.95 (1.90)	mAP 42.33 (44.23)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.59 (1.90)	mAP 55.11 (44.27)
Train: [4/50]	Time 0.331	Loss 1.90 	mAP 44.27
Test: [0/402]	Time 1.804 (1.804)	Precision 77.58 (77.58)	Recall 74.72 (74.72) 	 P_C 29.28 	 R_C 28.09 	 F_C 27.82 	 P_O 77.58 	 R_O 74.72 	 F_O 76.12
Test: [100/402]	Time 0.500 (0.513)	Precision 89.12 (79.66)	Recall 85.62 (80.24) 	 P_C 66.90 	 R_C 65.94 	 F_C 64.58 	 P_O 77.61 	 R_O 78.83 	 F_O 78.21
Test: [200/402]	Time 0.502 (0.506)	Precision 80.99 (80.05)	Recall 84.85 (80.72) 	 P_C 69.32 	 R_C 68.96 	 F_C 67.96 	 P_O 78.67 	 R_O 79.61 	 F_O 79.14
Test: [300/402]	Time 0.500 (0.504)	Precision 81.96 (79.82)	Recall 79.10 (81.23) 	 P_C 73.82 	 R_C 74.47 	 F_C 73.19 	 P_O 78.38 	 R_O 80.03 	 F_O 79.20
Test: [400/402]	Time 0.500 (0.503)	Precision 61.33 (78.81)	Recall 64.53 (81.25) 	 P_C 75.48 	 R_C 75.90 	 F_C 75.02 	 P_O 77.56 	 R_O 80.32 	 F_O 78.91
Test: [4/50]	  P_C 75.49 	 R_C 75.90 	 F_C 75.02 	 P_O 77.55 	 R_O 80.31 	 F_O 78.91 	 mAP 81.09
Train: [0/2566]	Time 1.026 (1.026)	Loss 1.69 (1.69)	mAP 37.52 (37.52)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.99 (1.84)	mAP 47.39 (44.43)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.69 (1.86)	mAP 44.03 (44.45)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.65 (1.85)	mAP 46.15 (44.74)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.63 (1.85)	mAP 54.61 (44.72)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.85 (1.85)	mAP 46.76 (44.86)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.27 (1.84)	mAP 52.69 (44.75)
Train: [700/2566]	Time 0.330 (0.332)	Loss 1.58 (1.85)	mAP 45.64 (44.65)
Train: [800/2566]	Time 0.331 (0.331)	Loss 1.87 (1.85)	mAP 35.88 (44.60)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.54 (1.85)	mAP 42.48 (44.59)
Train: [1000/2566]	Time 0.329 (0.331)	Loss 1.91 (1.85)	mAP 43.49 (44.59)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 1.83 (1.86)	mAP 43.81 (44.57)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 2.39 (1.86)	mAP 46.25 (44.56)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.48 (1.86)	mAP 49.08 (44.53)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.88 (1.86)	mAP 47.13 (44.54)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 2.09 (1.86)	mAP 44.53 (44.56)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.41 (1.86)	mAP 48.59 (44.60)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 2.09 (1.86)	mAP 45.66 (44.62)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.50 (1.85)	mAP 43.46 (44.65)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.41 (1.85)	mAP 48.07 (44.67)
Train: [2000/2566]	Time 0.329 (0.331)	Loss 2.60 (1.86)	mAP 33.03 (44.67)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.34 (1.86)	mAP 45.54 (44.67)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.29 (1.86)	mAP 45.09 (44.64)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.93 (1.86)	mAP 45.85 (44.64)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.74 (1.86)	mAP 42.72 (44.61)
Train: [2500/2566]	Time 0.329 (0.331)	Loss 2.07 (1.86)	mAP 51.08 (44.60)
Train: [5/50]	Time 0.331	Loss 1.86 	mAP 44.61
Test: [0/402]	Time 1.824 (1.824)	Precision 77.68 (77.68)	Recall 76.14 (76.14) 	 P_C 29.29 	 R_C 28.20 	 F_C 27.63 	 P_O 77.68 	 R_O 76.14 	 F_O 76.90
Test: [100/402]	Time 0.500 (0.513)	Precision 87.66 (78.71)	Recall 88.24 (81.35) 	 P_C 66.05 	 R_C 67.77 	 F_C 64.69 	 P_O 77.05 	 R_O 79.89 	 F_O 78.44
Test: [200/402]	Time 0.500 (0.506)	Precision 79.44 (79.27)	Recall 85.28 (81.64) 	 P_C 68.61 	 R_C 70.16 	 F_C 67.70 	 P_O 78.11 	 R_O 80.50 	 F_O 79.28
Test: [300/402]	Time 0.501 (0.504)	Precision 87.57 (79.23)	Recall 78.86 (81.99) 	 P_C 73.17 	 R_C 75.52 	 F_C 73.02 	 P_O 78.09 	 R_O 80.71 	 F_O 79.38
Test: [400/402]	Time 0.501 (0.503)	Precision 58.12 (78.44)	Recall 64.53 (81.97) 	 P_C 74.63 	 R_C 77.00 	 F_C 74.98 	 P_O 77.47 	 R_O 80.94 	 F_O 79.17
Test: [5/50]	  P_C 74.63 	 R_C 77.00 	 F_C 74.98 	 P_O 77.47 	 R_O 80.94 	 F_O 79.16 	 mAP 81.55
Train: [0/2566]	Time 0.984 (0.984)	Loss 1.52 (1.52)	mAP 51.69 (51.69)
Train: [100/2566]	Time 0.331 (0.336)	Loss 2.40 (1.83)	mAP 45.00 (44.16)
Train: [200/2566]	Time 0.330 (0.333)	Loss 1.90 (1.84)	mAP 40.93 (44.38)
Train: [300/2566]	Time 0.330 (0.332)	Loss 2.17 (1.84)	mAP 44.80 (44.45)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.89 (1.84)	mAP 47.40 (44.54)
Train: [500/2566]	Time 0.330 (0.332)	Loss 2.11 (1.85)	mAP 42.00 (44.58)
Train: [600/2566]	Time 0.330 (0.331)	Loss 1.64 (1.86)	mAP 45.08 (44.62)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.93 (1.86)	mAP 47.52 (44.53)
Train: [800/2566]	Time 0.330 (0.331)	Loss 1.94 (1.85)	mAP 37.77 (44.49)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.77 (1.85)	mAP 52.01 (44.62)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 2.13 (1.85)	mAP 47.26 (44.51)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 2.01 (1.85)	mAP 40.73 (44.51)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.72 (1.85)	mAP 43.94 (44.52)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.55 (1.85)	mAP 45.56 (44.53)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.80 (1.85)	mAP 47.90 (44.56)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 2.08 (1.85)	mAP 40.98 (44.61)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.65 (1.85)	mAP 41.97 (44.59)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 2.44 (1.84)	mAP 42.96 (44.57)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.58 (1.85)	mAP 42.27 (44.55)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.05 (1.85)	mAP 52.09 (44.58)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 2.02 (1.84)	mAP 44.59 (44.57)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.79 (1.84)	mAP 48.69 (44.58)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.07 (1.84)	mAP 39.50 (44.55)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.84 (1.84)	mAP 44.32 (44.55)
Train: [2400/2566]	Time 0.331 (0.330)	Loss 1.87 (1.84)	mAP 46.86 (44.56)
Train: [2500/2566]	Time 0.330 (0.330)	Loss 2.06 (1.84)	mAP 49.74 (44.57)
Train: [6/50]	Time 0.330	Loss 1.84 	mAP 44.59
Test: [0/402]	Time 1.807 (1.807)	Precision 74.46 (74.46)	Recall 78.69 (78.69) 	 P_C 29.33 	 R_C 29.77 	 F_C 28.26 	 P_O 74.46 	 R_O 78.69 	 F_O 76.52
Test: [100/402]	Time 0.500 (0.513)	Precision 77.09 (73.00)	Recall 90.20 (84.80) 	 P_C 59.02 	 R_C 72.16 	 F_C 63.08 	 P_O 70.71 	 R_O 83.75 	 F_O 76.68
Test: [200/402]	Time 0.500 (0.506)	Precision 82.57 (74.55)	Recall 86.15 (84.74) 	 P_C 62.31 	 R_C 74.96 	 F_C 66.94 	 P_O 72.78 	 R_O 83.90 	 F_O 77.94
Test: [300/402]	Time 0.499 (0.504)	Precision 83.58 (73.71)	Recall 83.58 (85.70) 	 P_C 66.70 	 R_C 80.82 	 F_C 72.19 	 P_O 71.67 	 R_O 84.85 	 F_O 77.71
Test: [400/402]	Time 0.500 (0.503)	Precision 47.01 (72.41)	Recall 73.26 (85.83) 	 P_C 68.19 	 R_C 82.19 	 F_C 73.74 	 P_O 70.48 	 R_O 85.19 	 F_O 77.14
Test: [6/50]	  P_C 68.19 	 R_C 82.20 	 F_C 73.74 	 P_O 70.47 	 R_O 85.19 	 F_O 77.13 	 mAP 82.01
Train: [0/2566]	Time 1.028 (1.028)	Loss 1.93 (1.93)	mAP 45.45 (45.45)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.62 (1.86)	mAP 37.04 (44.10)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.61 (1.83)	mAP 46.78 (44.34)
Train: [300/2566]	Time 0.331 (0.333)	Loss 2.28 (1.82)	mAP 41.52 (44.52)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.81 (1.83)	mAP 45.21 (44.67)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.46 (1.83)	mAP 37.06 (44.83)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.58 (1.83)	mAP 40.90 (44.78)
Train: [700/2566]	Time 0.330 (0.332)	Loss 1.41 (1.83)	mAP 46.44 (44.66)
Train: [800/2566]	Time 0.330 (0.332)	Loss 1.81 (1.82)	mAP 37.96 (44.66)
Train: [900/2566]	Time 0.329 (0.331)	Loss 1.50 (1.82)	mAP 47.07 (44.72)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.83 (1.82)	mAP 44.21 (44.76)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.96 (1.82)	mAP 44.95 (44.74)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.70 (1.82)	mAP 41.55 (44.75)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.73 (1.82)	mAP 41.63 (44.73)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.36 (1.82)	mAP 38.61 (44.71)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 2.02 (1.82)	mAP 44.05 (44.70)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 2.25 (1.82)	mAP 43.77 (44.76)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.75 (1.82)	mAP 44.60 (44.81)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.60 (1.82)	mAP 48.89 (44.81)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.56 (1.82)	mAP 48.25 (44.84)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.62 (1.82)	mAP 49.15 (44.84)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.64 (1.82)	mAP 51.11 (44.83)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.66 (1.82)	mAP 40.45 (44.81)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.53 (1.82)	mAP 37.16 (44.80)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.75 (1.82)	mAP 48.02 (44.80)
Train: [2500/2566]	Time 0.332 (0.331)	Loss 1.58 (1.82)	mAP 49.47 (44.80)
Train: [7/50]	Time 0.331	Loss 1.82 	mAP 44.78
Test: [0/402]	Time 1.799 (1.799)	Precision 73.25 (73.25)	Recall 80.11 (80.11) 	 P_C 28.38 	 R_C 32.28 	 F_C 29.07 	 P_O 73.25 	 R_O 80.11 	 F_O 76.53
Test: [100/402]	Time 0.500 (0.513)	Precision 87.01 (74.38)	Recall 87.58 (84.64) 	 P_C 61.45 	 R_C 71.03 	 F_C 64.01 	 P_O 72.68 	 R_O 83.46 	 F_O 77.70
Test: [200/402]	Time 0.501 (0.506)	Precision 82.57 (75.09)	Recall 86.15 (84.85) 	 P_C 63.74 	 R_C 73.92 	 F_C 67.05 	 P_O 73.67 	 R_O 83.98 	 F_O 78.49
Test: [300/402]	Time 0.500 (0.504)	Precision 82.25 (74.75)	Recall 81.84 (85.61) 	 P_C 68.95 	 R_C 79.45 	 F_C 72.73 	 P_O 73.14 	 R_O 84.75 	 F_O 78.52
Test: [400/402]	Time 0.500 (0.503)	Precision 54.84 (73.84)	Recall 69.19 (85.56) 	 P_C 71.53 	 R_C 80.74 	 F_C 74.75 	 P_O 72.45 	 R_O 84.90 	 F_O 78.18
Test: [7/50]	  P_C 71.53 	 R_C 80.74 	 F_C 74.75 	 P_O 72.44 	 R_O 84.90 	 F_O 78.18 	 mAP 82.33
Train: [0/2566]	Time 1.036 (1.036)	Loss 1.82 (1.82)	mAP 44.83 (44.83)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.66 (1.82)	mAP 50.70 (45.35)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.39 (1.80)	mAP 51.97 (44.92)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.76 (1.79)	mAP 43.40 (45.06)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.12 (1.78)	mAP 37.99 (44.97)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.75 (1.80)	mAP 41.48 (44.96)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.58 (1.80)	mAP 45.63 (45.04)
Train: [700/2566]	Time 0.331 (0.332)	Loss 2.03 (1.80)	mAP 56.15 (44.96)
Train: [800/2566]	Time 0.330 (0.331)	Loss 2.01 (1.80)	mAP 42.75 (44.92)
Train: [900/2566]	Time 0.331 (0.331)	Loss 1.95 (1.80)	mAP 46.42 (44.99)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 1.74 (1.80)	mAP 45.08 (45.01)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.48 (1.80)	mAP 43.25 (44.96)
Train: [1200/2566]	Time 0.329 (0.331)	Loss 1.87 (1.80)	mAP 50.37 (44.93)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 2.65 (1.80)	mAP 41.31 (44.91)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.88 (1.79)	mAP 46.97 (44.96)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.81 (1.80)	mAP 44.23 (44.96)
Train: [1600/2566]	Time 0.329 (0.331)	Loss 1.37 (1.80)	mAP 51.99 (44.99)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.76 (1.80)	mAP 43.44 (44.99)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.90 (1.80)	mAP 44.06 (44.96)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.53 (1.80)	mAP 45.20 (44.97)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.62 (1.80)	mAP 49.49 (44.96)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.52 (1.80)	mAP 46.44 (44.98)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 2.43 (1.80)	mAP 46.63 (44.96)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.05 (1.80)	mAP 47.04 (44.97)
Train: [2400/2566]	Time 0.329 (0.331)	Loss 1.58 (1.80)	mAP 45.01 (44.96)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.73 (1.80)	mAP 45.11 (44.96)
Train: [8/50]	Time 0.331	Loss 1.80 	mAP 44.94
Test: [0/402]	Time 1.849 (1.849)	Precision 71.50 (71.50)	Recall 79.83 (79.83) 	 P_C 29.32 	 R_C 34.06 	 F_C 29.88 	 P_O 71.50 	 R_O 79.83 	 F_O 75.44
Test: [100/402]	Time 0.500 (0.513)	Precision 86.62 (74.53)	Recall 88.89 (84.22) 	 P_C 65.97 	 R_C 69.74 	 F_C 65.58 	 P_O 72.98 	 R_O 82.95 	 F_O 77.65
Test: [200/402]	Time 0.500 (0.506)	Precision 82.11 (75.74)	Recall 87.45 (84.34) 	 P_C 67.96 	 R_C 72.17 	 F_C 68.63 	 P_O 74.47 	 R_O 83.36 	 F_O 78.67
Test: [300/402]	Time 0.499 (0.504)	Precision 83.59 (76.37)	Recall 81.09 (84.79) 	 P_C 72.22 	 R_C 77.82 	 F_C 73.89 	 P_O 75.04 	 R_O 83.74 	 F_O 79.15
Test: [400/402]	Time 0.499 (0.503)	Precision 55.56 (75.63)	Recall 69.77 (84.61) 	 P_C 74.36 	 R_C 79.09 	 F_C 75.57 	 P_O 74.50 	 R_O 83.77 	 F_O 78.87
Test: [8/50]	  P_C 74.36 	 R_C 79.08 	 F_C 75.56 	 P_O 74.49 	 R_O 83.77 	 F_O 78.86 	 mAP 82.46
Train: [0/2566]	Time 1.002 (1.002)	Loss 2.15 (2.15)	mAP 38.87 (38.87)
Train: [100/2566]	Time 0.329 (0.336)	Loss 1.57 (1.74)	mAP 42.09 (44.52)
Train: [200/2566]	Time 0.329 (0.333)	Loss 1.71 (1.77)	mAP 48.04 (44.59)
Train: [300/2566]	Time 0.330 (0.332)	Loss 1.60 (1.78)	mAP 37.65 (44.82)
Train: [400/2566]	Time 0.330 (0.332)	Loss 2.01 (1.78)	mAP 41.08 (44.70)
Train: [500/2566]	Time 0.330 (0.331)	Loss 1.48 (1.78)	mAP 39.03 (44.82)
Train: [600/2566]	Time 0.330 (0.331)	Loss 1.93 (1.78)	mAP 48.75 (44.95)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.71 (1.78)	mAP 48.84 (45.01)
Train: [800/2566]	Time 0.330 (0.331)	Loss 2.17 (1.78)	mAP 42.39 (44.99)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.89 (1.78)	mAP 41.37 (45.01)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.45 (1.79)	mAP 40.50 (45.03)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 1.71 (1.79)	mAP 45.67 (45.01)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.90 (1.79)	mAP 42.01 (45.03)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.40 (1.79)	mAP 47.96 (44.98)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.64 (1.79)	mAP 40.99 (44.97)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.94 (1.79)	mAP 44.62 (44.98)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.81 (1.79)	mAP 47.84 (44.96)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.75 (1.79)	mAP 41.57 (44.95)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 2.25 (1.79)	mAP 52.50 (44.95)
Train: [1900/2566]	Time 0.329 (0.331)	Loss 2.03 (1.79)	mAP 46.06 (44.93)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.84 (1.79)	mAP 44.04 (44.93)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.60 (1.79)	mAP 44.27 (44.90)
Train: [2200/2566]	Time 0.329 (0.330)	Loss 1.94 (1.79)	mAP 45.31 (44.91)
Train: [2300/2566]	Time 0.330 (0.330)	Loss 2.20 (1.79)	mAP 48.37 (44.89)
Train: [2400/2566]	Time 0.330 (0.330)	Loss 1.92 (1.79)	mAP 44.03 (44.87)
Train: [2500/2566]	Time 0.330 (0.330)	Loss 1.77 (1.79)	mAP 48.70 (44.87)
Train: [9/50]	Time 0.330	Loss 1.79 	mAP 44.88
Test: [0/402]	Time 1.809 (1.809)	Precision 74.07 (74.07)	Recall 79.55 (79.55) 	 P_C 29.47 	 R_C 33.58 	 F_C 30.08 	 P_O 74.07 	 R_O 79.55 	 F_O 76.71
Test: [100/402]	Time 0.499 (0.512)	Precision 84.24 (76.59)	Recall 90.85 (83.30) 	 P_C 63.57 	 R_C 70.33 	 F_C 64.86 	 P_O 75.01 	 R_O 82.22 	 F_O 78.45
Test: [200/402]	Time 0.500 (0.506)	Precision 79.61 (77.30)	Recall 87.88 (83.61) 	 P_C 65.83 	 R_C 73.17 	 F_C 68.10 	 P_O 76.10 	 R_O 82.78 	 F_O 79.30
Test: [300/402]	Time 0.500 (0.504)	Precision 81.13 (76.64)	Recall 82.34 (84.50) 	 P_C 70.97 	 R_C 78.47 	 F_C 73.49 	 P_O 75.40 	 R_O 83.62 	 F_O 79.29
Test: [400/402]	Time 0.500 (0.503)	Precision 55.14 (75.71)	Recall 68.60 (84.54) 	 P_C 73.64 	 R_C 79.94 	 F_C 75.41 	 P_O 74.58 	 R_O 83.91 	 F_O 78.97
Test: [9/50]	  P_C 73.64 	 R_C 79.93 	 F_C 75.41 	 P_O 74.57 	 R_O 83.90 	 F_O 78.96 	 mAP 82.74
Train: [0/2566]	Time 1.012 (1.012)	Loss 1.84 (1.84)	mAP 38.49 (38.49)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.43 (1.79)	mAP 43.43 (44.85)
Train: [200/2566]	Time 0.330 (0.333)	Loss 1.20 (1.78)	mAP 50.53 (44.99)
Train: [300/2566]	Time 0.329 (0.332)	Loss 2.02 (1.79)	mAP 46.20 (45.04)
Train: [400/2566]	Time 0.330 (0.332)	Loss 2.00 (1.78)	mAP 41.45 (45.10)
Train: [500/2566]	Time 0.329 (0.331)	Loss 1.48 (1.77)	mAP 40.83 (44.97)
Train: [600/2566]	Time 0.331 (0.331)	Loss 2.17 (1.76)	mAP 51.15 (44.96)
Train: [700/2566]	Time 0.331 (0.331)	Loss 2.27 (1.77)	mAP 38.63 (44.90)
Train: [800/2566]	Time 0.331 (0.331)	Loss 2.19 (1.77)	mAP 42.91 (44.88)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.65 (1.77)	mAP 50.14 (45.01)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.88 (1.77)	mAP 37.59 (45.03)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.68 (1.77)	mAP 41.58 (45.02)
Train: [1200/2566]	Time 0.332 (0.331)	Loss 1.61 (1.78)	mAP 46.80 (45.04)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.54 (1.78)	mAP 43.07 (44.98)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.23 (1.78)	mAP 49.16 (44.97)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.71 (1.78)	mAP 34.91 (44.96)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.61 (1.78)	mAP 45.31 (44.96)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.41 (1.78)	mAP 34.69 (44.92)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.80 (1.78)	mAP 41.15 (44.95)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.29 (1.78)	mAP 42.13 (44.97)
Train: [2000/2566]	Time 0.329 (0.331)	Loss 1.79 (1.78)	mAP 48.40 (44.96)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.97 (1.78)	mAP 41.69 (44.97)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.45 (1.78)	mAP 34.79 (44.99)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.93 (1.78)	mAP 49.64 (44.96)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.64 (1.78)	mAP 40.97 (44.95)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.92 (1.78)	mAP 47.25 (44.96)
Train: [10/50]	Time 0.331	Loss 1.78 	mAP 44.96
Test: [0/402]	Time 1.760 (1.760)	Precision 77.34 (77.34)	Recall 77.56 (77.56) 	 P_C 28.56 	 R_C 28.74 	 F_C 27.74 	 P_O 77.34 	 R_O 77.56 	 F_O 77.45
Test: [100/402]	Time 0.500 (0.512)	Precision 87.18 (79.87)	Recall 88.89 (81.60) 	 P_C 66.57 	 R_C 67.13 	 F_C 65.12 	 P_O 78.23 	 R_O 80.24 	 F_O 79.22
Test: [200/402]	Time 0.499 (0.506)	Precision 85.90 (80.47)	Recall 84.42 (82.35) 	 P_C 69.79 	 R_C 70.12 	 F_C 68.79 	 P_O 79.34 	 R_O 81.30 	 F_O 80.31
Test: [300/402]	Time 0.500 (0.504)	Precision 85.71 (80.17)	Recall 80.60 (82.95) 	 P_C 74.09 	 R_C 76.04 	 F_C 74.10 	 P_O 78.77 	 R_O 81.89 	 F_O 80.30
Test: [400/402]	Time 0.499 (0.503)	Precision 58.76 (79.17)	Recall 66.28 (82.97) 	 P_C 77.01 	 R_C 77.64 	 F_C 76.18 	 P_O 77.97 	 R_O 82.14 	 F_O 80.00
Test: [10/50]	  P_C 77.01 	 R_C 77.64 	 F_C 76.18 	 P_O 77.96 	 R_O 82.14 	 F_O 79.99 	 mAP 83.04
Train: [0/2566]	Time 1.063 (1.063)	Loss 1.91 (1.91)	mAP 44.32 (44.32)
Train: [100/2566]	Time 0.331 (0.338)	Loss 1.33 (1.82)	mAP 28.54 (44.97)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.83 (1.79)	mAP 50.39 (45.32)
Train: [300/2566]	Time 0.330 (0.333)	Loss 2.06 (1.80)	mAP 41.57 (45.25)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.77 (1.78)	mAP 47.68 (45.15)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.90 (1.78)	mAP 46.99 (45.24)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.77 (1.78)	mAP 45.53 (45.21)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.97 (1.79)	mAP 46.50 (45.22)
Train: [800/2566]	Time 0.330 (0.331)	Loss 1.67 (1.79)	mAP 49.40 (45.17)
Train: [900/2566]	Time 0.329 (0.331)	Loss 1.71 (1.79)	mAP 53.87 (45.18)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 1.64 (1.79)	mAP 42.64 (45.14)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 2.11 (1.79)	mAP 41.02 (45.13)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.69 (1.79)	mAP 41.21 (45.11)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.43 (1.79)	mAP 39.99 (45.14)
Train: [1400/2566]	Time 0.329 (0.331)	Loss 1.31 (1.79)	mAP 38.02 (45.10)
Train: [1500/2566]	Time 0.329 (0.331)	Loss 1.41 (1.78)	mAP 44.31 (45.04)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.32 (1.78)	mAP 39.46 (45.03)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.89 (1.78)	mAP 47.01 (45.04)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.65 (1.78)	mAP 47.83 (45.05)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.39 (1.78)	mAP 35.95 (45.04)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.58 (1.78)	mAP 47.64 (45.05)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.94 (1.78)	mAP 49.32 (45.03)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.70 (1.77)	mAP 43.39 (45.02)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.71 (1.77)	mAP 41.91 (45.01)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.73 (1.78)	mAP 41.93 (45.01)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.87 (1.78)	mAP 49.15 (44.99)
Train: [11/50]	Time 0.331	Loss 1.78 	mAP 44.99
Test: [0/402]	Time 1.811 (1.811)	Precision 72.87 (72.87)	Recall 80.11 (80.11) 	 P_C 28.68 	 R_C 32.66 	 F_C 29.19 	 P_O 72.87 	 R_O 80.11 	 F_O 76.32
Test: [100/402]	Time 0.500 (0.513)	Precision 83.73 (74.01)	Recall 90.85 (84.65) 	 P_C 62.87 	 R_C 72.06 	 F_C 65.16 	 P_O 72.33 	 R_O 83.65 	 F_O 77.58
Test: [200/402]	Time 0.500 (0.506)	Precision 80.00 (75.36)	Recall 86.58 (84.63) 	 P_C 65.55 	 R_C 74.60 	 F_C 68.40 	 P_O 74.04 	 R_O 83.82 	 F_O 78.62
Test: [300/402]	Time 0.499 (0.504)	Precision 81.62 (75.17)	Recall 82.84 (85.38) 	 P_C 70.73 	 R_C 79.34 	 F_C 73.55 	 P_O 73.66 	 R_O 84.54 	 F_O 78.72
Test: [400/402]	Time 0.500 (0.503)	Precision 54.89 (74.30)	Recall 75.00 (85.41) 	 P_C 72.93 	 R_C 80.76 	 F_C 75.67 	 P_O 72.97 	 R_O 84.77 	 F_O 78.43
Test: [11/50]	  P_C 72.93 	 R_C 80.75 	 F_C 75.67 	 P_O 72.96 	 R_O 84.77 	 F_O 78.42 	 mAP 83.10
Train: [0/2566]	Time 1.035 (1.035)	Loss 1.69 (1.69)	mAP 51.51 (51.51)
Train: [100/2566]	Time 0.331 (0.337)	Loss 1.92 (1.73)	mAP 41.00 (45.23)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.47 (1.74)	mAP 40.17 (45.03)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.58 (1.74)	mAP 44.47 (45.04)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.60 (1.75)	mAP 38.17 (45.21)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.87 (1.75)	mAP 39.17 (45.14)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.14 (1.74)	mAP 44.58 (45.03)
Train: [700/2566]	Time 0.329 (0.331)	Loss 1.10 (1.75)	mAP 40.85 (45.08)
Train: [800/2566]	Time 0.330 (0.331)	Loss 0.93 (1.75)	mAP 38.36 (45.05)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.74 (1.76)	mAP 49.16 (45.02)
Train: [1000/2566]	Time 0.329 (0.331)	Loss 1.38 (1.76)	mAP 44.46 (45.05)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 2.02 (1.76)	mAP 50.13 (44.95)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 2.10 (1.76)	mAP 42.39 (44.90)
Train: [1300/2566]	Time 0.329 (0.331)	Loss 1.65 (1.76)	mAP 48.55 (44.90)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.76 (1.76)	mAP 41.40 (44.90)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 2.27 (1.76)	mAP 44.98 (44.89)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.63 (1.76)	mAP 49.69 (44.92)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.92 (1.76)	mAP 40.51 (44.91)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.73 (1.76)	mAP 46.57 (44.89)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.33 (1.76)	mAP 34.89 (44.90)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.40 (1.76)	mAP 50.53 (44.92)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.52 (1.76)	mAP 44.49 (44.95)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.84 (1.76)	mAP 37.18 (44.93)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.46 (1.76)	mAP 49.05 (44.93)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.82 (1.76)	mAP 44.55 (44.98)
Train: [2500/2566]	Time 0.329 (0.330)	Loss 1.15 (1.76)	mAP 42.14 (44.98)
Train: [12/50]	Time 0.330	Loss 1.76 	mAP 44.99
Test: [0/402]	Time 1.744 (1.744)	Precision 75.14 (75.14)	Recall 77.27 (77.27) 	 P_C 28.88 	 R_C 28.76 	 F_C 27.93 	 P_O 75.14 	 R_O 77.27 	 F_O 76.19
Test: [100/402]	Time 0.501 (0.512)	Precision 90.07 (77.76)	Recall 88.89 (82.59) 	 P_C 67.38 	 R_C 68.09 	 F_C 66.05 	 P_O 76.67 	 R_O 81.11 	 F_O 78.83
Test: [200/402]	Time 0.500 (0.506)	Precision 85.53 (78.38)	Recall 87.01 (83.30) 	 P_C 69.58 	 R_C 71.26 	 F_C 69.22 	 P_O 77.47 	 R_O 82.12 	 F_O 79.73
Test: [300/402]	Time 0.500 (0.504)	Precision 85.49 (78.76)	Recall 82.09 (83.79) 	 P_C 73.70 	 R_C 77.12 	 F_C 74.54 	 P_O 77.85 	 R_O 82.57 	 F_O 80.14
Test: [400/402]	Time 0.500 (0.503)	Precision 61.38 (78.07)	Recall 67.44 (83.68) 	 P_C 75.88 	 R_C 78.68 	 F_C 76.66 	 P_O 77.43 	 R_O 82.65 	 F_O 79.95
Test: [12/50]	  P_C 75.88 	 R_C 78.68 	 F_C 76.66 	 P_O 77.42 	 R_O 82.65 	 F_O 79.95 	 mAP 83.30
Train: [0/2566]	Time 0.981 (0.981)	Loss 1.76 (1.76)	mAP 42.08 (42.08)
Train: [100/2566]	Time 0.330 (0.336)	Loss 1.66 (1.76)	mAP 40.85 (44.80)
Train: [200/2566]	Time 0.330 (0.333)	Loss 1.55 (1.75)	mAP 45.78 (44.84)
Train: [300/2566]	Time 0.330 (0.332)	Loss 1.87 (1.76)	mAP 43.37 (45.10)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.56 (1.76)	mAP 36.10 (45.02)
Train: [500/2566]	Time 0.330 (0.332)	Loss 2.01 (1.76)	mAP 40.45 (45.01)
Train: [600/2566]	Time 0.331 (0.331)	Loss 1.66 (1.76)	mAP 42.56 (45.00)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.66 (1.77)	mAP 36.61 (44.95)
Train: [800/2566]	Time 0.329 (0.331)	Loss 2.42 (1.77)	mAP 49.92 (44.95)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.41 (1.77)	mAP 41.48 (44.92)
Train: [1000/2566]	Time 0.329 (0.331)	Loss 1.16 (1.77)	mAP 36.99 (45.01)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.55 (1.77)	mAP 43.63 (45.05)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.61 (1.77)	mAP 40.15 (45.05)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.50 (1.77)	mAP 39.84 (45.01)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.52 (1.76)	mAP 44.52 (45.01)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.07 (1.76)	mAP 47.03 (45.03)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.40 (1.76)	mAP 50.53 (45.09)
Train: [1700/2566]	Time 0.329 (0.331)	Loss 1.31 (1.76)	mAP 41.38 (45.09)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.73 (1.76)	mAP 49.67 (45.15)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.01 (1.76)	mAP 44.49 (45.19)
Train: [2000/2566]	Time 0.330 (0.330)	Loss 1.63 (1.76)	mAP 47.64 (45.17)
Train: [2100/2566]	Time 0.330 (0.330)	Loss 1.69 (1.76)	mAP 41.70 (45.16)
Train: [2200/2566]	Time 0.330 (0.330)	Loss 1.66 (1.76)	mAP 45.55 (45.15)
Train: [2300/2566]	Time 0.330 (0.330)	Loss 2.07 (1.76)	mAP 42.89 (45.17)
Train: [2400/2566]	Time 0.331 (0.330)	Loss 1.77 (1.76)	mAP 41.96 (45.17)
Train: [2500/2566]	Time 0.331 (0.330)	Loss 2.23 (1.76)	mAP 51.64 (45.17)
Train: [13/50]	Time 0.330	Loss 1.76 	mAP 45.16
Test: [0/402]	Time 1.818 (1.818)	Precision 76.60 (76.60)	Recall 78.12 (78.12) 	 P_C 30.96 	 R_C 31.49 	 F_C 30.09 	 P_O 76.60 	 R_O 78.12 	 F_O 77.36
Test: [100/402]	Time 0.499 (0.513)	Precision 85.71 (77.58)	Recall 90.20 (82.64) 	 P_C 67.79 	 R_C 68.87 	 F_C 66.82 	 P_O 76.64 	 R_O 81.32 	 F_O 78.91
Test: [200/402]	Time 0.500 (0.506)	Precision 82.38 (78.39)	Recall 87.01 (82.87) 	 P_C 69.72 	 R_C 71.71 	 F_C 69.70 	 P_O 77.72 	 R_O 81.88 	 F_O 79.74
Test: [300/402]	Time 0.500 (0.504)	Precision 84.06 (78.50)	Recall 81.34 (83.57) 	 P_C 74.74 	 R_C 76.71 	 F_C 74.76 	 P_O 77.77 	 R_O 82.46 	 F_O 80.05
Test: [400/402]	Time 0.500 (0.503)	Precision 59.31 (78.24)	Recall 70.35 (83.38) 	 P_C 76.80 	 R_C 78.06 	 F_C 76.66 	 P_O 77.59 	 R_O 82.51 	 F_O 79.97
Test: [13/50]	  P_C 76.81 	 R_C 78.06 	 F_C 76.66 	 P_O 77.58 	 R_O 82.50 	 F_O 79.97 	 mAP 83.28
Train: [0/2566]	Time 1.048 (1.048)	Loss 2.07 (2.07)	mAP 43.07 (43.07)
Train: [100/2566]	Time 0.330 (0.337)	Loss 2.67 (1.76)	mAP 48.28 (45.04)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.13 (1.75)	mAP 45.77 (44.95)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.80 (1.76)	mAP 55.72 (45.01)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.56 (1.76)	mAP 44.96 (45.16)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.76 (1.76)	mAP 48.67 (45.19)
Train: [600/2566]	Time 0.329 (0.331)	Loss 1.21 (1.76)	mAP 39.26 (45.18)
Train: [700/2566]	Time 0.329 (0.331)	Loss 1.86 (1.76)	mAP 52.86 (45.18)
Train: [800/2566]	Time 0.329 (0.331)	Loss 1.71 (1.77)	mAP 50.28 (45.07)
Train: [900/2566]	Time 0.329 (0.331)	Loss 1.70 (1.77)	mAP 39.66 (45.07)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.74 (1.77)	mAP 51.97 (45.14)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.68 (1.77)	mAP 39.51 (45.13)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.28 (1.77)	mAP 48.82 (45.10)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.14 (1.76)	mAP 44.48 (45.08)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.61 (1.76)	mAP 41.12 (45.11)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.18 (1.76)	mAP 44.87 (45.17)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.77 (1.75)	mAP 45.14 (45.17)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.83 (1.75)	mAP 40.24 (45.20)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.82 (1.75)	mAP 39.64 (45.25)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.10 (1.75)	mAP 43.30 (45.25)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.66 (1.75)	mAP 46.23 (45.20)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.65 (1.76)	mAP 41.42 (45.21)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.24 (1.76)	mAP 46.76 (45.21)
Train: [2300/2566]	Time 0.330 (0.330)	Loss 1.86 (1.76)	mAP 45.59 (45.20)
Train: [2400/2566]	Time 0.331 (0.330)	Loss 1.89 (1.76)	mAP 47.51 (45.20)
Train: [2500/2566]	Time 0.330 (0.330)	Loss 2.36 (1.75)	mAP 49.10 (45.18)
Train: [14/50]	Time 0.330	Loss 1.76 	mAP 45.20
Test: [0/402]	Time 1.770 (1.770)	Precision 73.81 (73.81)	Recall 79.26 (79.26) 	 P_C 31.53 	 R_C 32.23 	 F_C 30.37 	 P_O 73.81 	 R_O 79.26 	 F_O 76.44
Test: [100/402]	Time 0.500 (0.512)	Precision 84.81 (76.49)	Recall 87.58 (83.52) 	 P_C 63.73 	 R_C 70.19 	 F_C 65.31 	 P_O 75.18 	 R_O 82.10 	 F_O 78.49
Test: [200/402]	Time 0.500 (0.506)	Precision 79.38 (76.38)	Recall 88.31 (84.33) 	 P_C 66.17 	 R_C 72.75 	 F_C 68.34 	 P_O 75.48 	 R_O 83.23 	 F_O 79.16
Test: [300/402]	Time 0.500 (0.504)	Precision 82.43 (76.52)	Recall 82.84 (84.90) 	 P_C 71.08 	 R_C 78.33 	 F_C 73.85 	 P_O 75.47 	 R_O 83.83 	 F_O 79.43
Test: [400/402]	Time 0.500 (0.503)	Precision 56.67 (76.01)	Recall 69.19 (84.77) 	 P_C 73.80 	 R_C 79.90 	 F_C 75.90 	 P_O 75.13 	 R_O 83.94 	 F_O 79.29
Test: [14/50]	  P_C 73.80 	 R_C 79.90 	 F_C 75.90 	 P_O 75.12 	 R_O 83.93 	 F_O 79.28 	 mAP 83.54
Train: [0/2566]	Time 1.069 (1.069)	Loss 1.56 (1.56)	mAP 44.78 (44.78)
Train: [100/2566]	Time 0.331 (0.337)	Loss 1.81 (1.78)	mAP 34.77 (44.76)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.73 (1.75)	mAP 40.88 (45.13)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.19 (1.74)	mAP 46.77 (44.99)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.86 (1.74)	mAP 42.01 (45.03)
Train: [500/2566]	Time 0.330 (0.332)	Loss 2.03 (1.74)	mAP 41.68 (45.17)
Train: [600/2566]	Time 0.331 (0.331)	Loss 2.09 (1.75)	mAP 46.00 (45.24)
Train: [700/2566]	Time 0.331 (0.331)	Loss 1.83 (1.75)	mAP 49.04 (45.26)
Train: [800/2566]	Time 0.330 (0.331)	Loss 1.86 (1.75)	mAP 45.26 (45.30)
Train: [900/2566]	Time 0.331 (0.331)	Loss 1.66 (1.76)	mAP 56.03 (45.34)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 1.71 (1.75)	mAP 46.36 (45.32)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.87 (1.75)	mAP 46.59 (45.25)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.57 (1.75)	mAP 48.47 (45.29)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.76 (1.75)	mAP 39.13 (45.32)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 2.21 (1.75)	mAP 45.53 (45.37)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.70 (1.75)	mAP 44.03 (45.37)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.17 (1.75)	mAP 42.14 (45.40)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.31 (1.75)	mAP 39.79 (45.33)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.76 (1.75)	mAP 43.80 (45.29)
Train: [1900/2566]	Time 0.330 (0.330)	Loss 1.92 (1.75)	mAP 50.57 (45.29)
Train: [2000/2566]	Time 0.331 (0.330)	Loss 1.53 (1.75)	mAP 43.35 (45.28)
Train: [2100/2566]	Time 0.330 (0.330)	Loss 1.91 (1.75)	mAP 46.30 (45.27)
Train: [2200/2566]	Time 0.330 (0.330)	Loss 1.88 (1.75)	mAP 47.47 (45.25)
Train: [2300/2566]	Time 0.330 (0.330)	Loss 1.63 (1.75)	mAP 37.46 (45.24)
Train: [2400/2566]	Time 0.330 (0.330)	Loss 1.92 (1.75)	mAP 45.14 (45.25)
Train: [2500/2566]	Time 0.330 (0.330)	Loss 2.13 (1.75)	mAP 44.79 (45.25)
Train: [15/50]	Time 0.330	Loss 1.75 	mAP 45.25
Test: [0/402]	Time 1.815 (1.815)	Precision 74.47 (74.47)	Recall 79.55 (79.55) 	 P_C 30.44 	 R_C 32.85 	 F_C 29.87 	 P_O 74.47 	 R_O 79.55 	 F_O 76.92
Test: [100/402]	Time 0.500 (0.512)	Precision 87.18 (77.98)	Recall 88.89 (83.31) 	 P_C 66.35 	 R_C 69.76 	 F_C 66.15 	 P_O 76.26 	 R_O 82.11 	 F_O 79.08
Test: [200/402]	Time 0.501 (0.506)	Precision 81.63 (79.08)	Recall 86.58 (83.41) 	 P_C 69.45 	 R_C 72.44 	 F_C 69.75 	 P_O 77.80 	 R_O 82.52 	 F_O 80.09
Test: [300/402]	Time 0.500 (0.504)	Precision 84.34 (78.73)	Recall 83.08 (83.97) 	 P_C 73.13 	 R_C 77.98 	 F_C 74.55 	 P_O 77.41 	 R_O 83.02 	 F_O 80.12
Test: [400/402]	Time 0.500 (0.503)	Precision 59.80 (77.63)	Recall 69.19 (84.09) 	 P_C 75.23 	 R_C 79.18 	 F_C 76.50 	 P_O 76.48 	 R_O 83.36 	 F_O 79.77
Test: [15/50]	  P_C 75.23 	 R_C 79.18 	 F_C 76.49 	 P_O 76.48 	 R_O 83.35 	 F_O 79.77 	 mAP 83.39
Train: [0/2566]	Time 1.061 (1.061)	Loss 1.68 (1.68)	mAP 39.79 (39.79)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.94 (1.68)	mAP 44.46 (45.79)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.63 (1.71)	mAP 38.76 (45.63)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.42 (1.72)	mAP 45.12 (45.27)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.50 (1.74)	mAP 42.07 (45.40)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.54 (1.73)	mAP 44.80 (45.33)
Train: [600/2566]	Time 0.329 (0.331)	Loss 1.70 (1.73)	mAP 47.22 (45.26)
Train: [700/2566]	Time 0.330 (0.331)	Loss 2.05 (1.73)	mAP 39.69 (45.41)
Train: [800/2566]	Time 0.330 (0.331)	Loss 1.93 (1.73)	mAP 40.81 (45.27)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.82 (1.74)	mAP 43.23 (45.21)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 1.63 (1.74)	mAP 42.92 (45.27)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 2.18 (1.74)	mAP 46.88 (45.30)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.81 (1.74)	mAP 44.69 (45.23)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.83 (1.74)	mAP 53.89 (45.29)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.36 (1.74)	mAP 48.86 (45.28)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.11 (1.73)	mAP 40.69 (45.28)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.83 (1.73)	mAP 39.23 (45.24)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.82 (1.73)	mAP 45.04 (45.25)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.07 (1.74)	mAP 48.16 (45.22)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.54 (1.74)	mAP 47.34 (45.22)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.70 (1.74)	mAP 38.45 (45.23)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.44 (1.74)	mAP 42.19 (45.23)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.67 (1.74)	mAP 49.30 (45.24)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.38 (1.74)	mAP 38.77 (45.26)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.73 (1.74)	mAP 45.35 (45.27)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.73 (1.74)	mAP 46.06 (45.26)
Train: [16/50]	Time 0.330	Loss 1.74 	mAP 45.28
Test: [0/402]	Time 1.746 (1.746)	Precision 75.63 (75.63)	Recall 76.70 (76.70) 	 P_C 29.19 	 R_C 30.97 	 F_C 28.44 	 P_O 75.63 	 R_O 76.70 	 F_O 76.16
Test: [100/402]	Time 0.500 (0.512)	Precision 93.66 (80.28)	Recall 86.93 (81.81) 	 P_C 70.29 	 R_C 67.39 	 F_C 66.75 	 P_O 77.92 	 R_O 80.70 	 F_O 79.28
Test: [200/402]	Time 0.500 (0.506)	Precision 86.21 (81.44)	Recall 86.58 (82.23) 	 P_C 72.77 	 R_C 70.26 	 F_C 70.18 	 P_O 79.64 	 R_O 81.33 	 F_O 80.48
Test: [300/402]	Time 0.500 (0.504)	Precision 83.50 (80.61)	Recall 83.08 (83.10) 	 P_C 76.00 	 R_C 76.34 	 F_C 75.14 	 P_O 78.51 	 R_O 82.26 	 F_O 80.34
Test: [400/402]	Time 0.500 (0.503)	Precision 60.20 (78.96)	Recall 70.35 (83.35) 	 P_C 78.02 	 R_C 77.76 	 F_C 76.75 	 P_O 77.06 	 R_O 82.75 	 F_O 79.80
Test: [16/50]	  P_C 78.02 	 R_C 77.76 	 F_C 76.75 	 P_O 77.05 	 R_O 82.75 	 F_O 79.80 	 mAP 83.41
Train: [0/2566]	Time 1.011 (1.011)	Loss 1.50 (1.50)	mAP 48.08 (48.08)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.89 (1.74)	mAP 49.26 (45.36)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.39 (1.73)	mAP 47.44 (45.19)
Train: [300/2566]	Time 0.331 (0.332)	Loss 1.49 (1.74)	mAP 45.54 (45.36)
Train: [400/2566]	Time 0.331 (0.332)	Loss 2.06 (1.75)	mAP 44.08 (45.18)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.31 (1.75)	mAP 43.06 (45.21)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.71 (1.75)	mAP 49.45 (45.13)
Train: [700/2566]	Time 0.331 (0.331)	Loss 1.56 (1.75)	mAP 40.48 (45.19)
Train: [800/2566]	Time 0.331 (0.331)	Loss 2.28 (1.75)	mAP 47.30 (45.17)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.93 (1.75)	mAP 40.52 (45.22)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 1.46 (1.75)	mAP 44.07 (45.13)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 1.50 (1.75)	mAP 48.37 (45.15)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.70 (1.75)	mAP 51.44 (45.11)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 2.13 (1.74)	mAP 37.63 (45.10)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.07 (1.74)	mAP 49.53 (45.14)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.41 (1.75)	mAP 50.99 (45.22)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.52 (1.74)	mAP 46.74 (45.21)
Train: [1700/2566]	Time 0.329 (0.331)	Loss 1.63 (1.74)	mAP 46.35 (45.29)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.73 (1.74)	mAP 45.19 (45.29)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.09 (1.74)	mAP 45.26 (45.27)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.94 (1.74)	mAP 37.99 (45.28)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.96 (1.74)	mAP 51.80 (45.25)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.77 (1.74)	mAP 46.49 (45.24)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.59 (1.74)	mAP 45.52 (45.21)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.85 (1.74)	mAP 42.96 (45.17)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.57 (1.74)	mAP 44.77 (45.17)
Train: [17/50]	Time 0.331	Loss 1.74 	mAP 45.17
Test: [0/402]	Time 1.837 (1.837)	Precision 75.34 (75.34)	Recall 78.98 (78.98) 	 P_C 29.33 	 R_C 30.53 	 F_C 28.66 	 P_O 75.34 	 R_O 78.98 	 F_O 77.12
Test: [100/402]	Time 0.499 (0.513)	Precision 87.58 (77.88)	Recall 87.58 (83.51) 	 P_C 67.47 	 R_C 68.98 	 F_C 66.23 	 P_O 76.26 	 R_O 82.24 	 F_O 79.14
Test: [200/402]	Time 0.500 (0.506)	Precision 84.10 (78.40)	Recall 87.01 (83.96) 	 P_C 70.10 	 R_C 71.76 	 F_C 69.97 	 P_O 77.15 	 R_O 83.01 	 F_O 79.98
Test: [300/402]	Time 0.500 (0.504)	Precision 84.29 (78.07)	Recall 84.08 (84.60) 	 P_C 73.31 	 R_C 77.80 	 F_C 74.67 	 P_O 76.55 	 R_O 83.68 	 F_O 79.96
Test: [400/402]	Time 0.500 (0.503)	Precision 55.92 (76.80)	Recall 68.60 (84.65) 	 P_C 75.72 	 R_C 79.25 	 F_C 76.24 	 P_O 75.50 	 R_O 83.95 	 F_O 79.50
Test: [17/50]	  P_C 75.72 	 R_C 79.25 	 F_C 76.24 	 P_O 75.49 	 R_O 83.95 	 F_O 79.50 	 mAP 83.38
Train: [0/2566]	Time 1.051 (1.051)	Loss 1.61 (1.61)	mAP 41.28 (41.28)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.83 (1.73)	mAP 46.57 (45.27)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.73 (1.73)	mAP 48.83 (45.56)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.93 (1.73)	mAP 47.65 (45.41)
Train: [400/2566]	Time 0.331 (0.332)	Loss 2.03 (1.74)	mAP 50.58 (45.40)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.55 (1.75)	mAP 56.92 (45.29)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.93 (1.74)	mAP 43.89 (45.29)
Train: [700/2566]	Time 0.329 (0.332)	Loss 1.96 (1.74)	mAP 42.99 (45.24)
Train: [800/2566]	Time 0.331 (0.331)	Loss 1.79 (1.74)	mAP 37.11 (45.19)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.95 (1.74)	mAP 45.33 (45.15)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 1.75 (1.74)	mAP 47.23 (45.16)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 1.88 (1.74)	mAP 55.73 (45.14)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.85 (1.74)	mAP 53.07 (45.17)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.81 (1.74)	mAP 41.94 (45.12)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 2.33 (1.74)	mAP 46.67 (45.12)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.39 (1.74)	mAP 40.94 (45.17)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.66 (1.74)	mAP 45.28 (45.20)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.72 (1.74)	mAP 44.04 (45.17)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 2.16 (1.74)	mAP 55.70 (45.17)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.69 (1.74)	mAP 46.60 (45.20)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.78 (1.74)	mAP 44.77 (45.25)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.34 (1.74)	mAP 45.43 (45.23)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.66 (1.74)	mAP 57.75 (45.22)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.83 (1.74)	mAP 41.82 (45.23)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.40 (1.74)	mAP 45.47 (45.21)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.78 (1.73)	mAP 50.75 (45.21)
Train: [18/50]	Time 0.331	Loss 1.73 	mAP 45.23
Test: [0/402]	Time 1.752 (1.752)	Precision 74.27 (74.27)	Recall 79.55 (79.55) 	 P_C 29.97 	 R_C 32.14 	 F_C 29.96 	 P_O 74.27 	 R_O 79.55 	 F_O 76.82
Test: [100/402]	Time 0.500 (0.512)	Precision 82.63 (74.78)	Recall 90.20 (85.02) 	 P_C 64.57 	 R_C 71.50 	 F_C 66.25 	 P_O 73.75 	 R_O 83.51 	 F_O 78.33
Test: [200/402]	Time 0.500 (0.506)	Precision 76.01 (74.87)	Recall 89.18 (85.39) 	 P_C 66.27 	 R_C 73.85 	 F_C 68.77 	 P_O 74.13 	 R_O 84.30 	 F_O 78.89
Test: [300/402]	Time 0.500 (0.504)	Precision 79.25 (74.54)	Recall 84.58 (86.27) 	 P_C 70.63 	 R_C 79.69 	 F_C 73.97 	 P_O 73.52 	 R_O 85.27 	 F_O 78.96
Test: [400/402]	Time 0.500 (0.503)	Precision 53.98 (73.55)	Recall 70.93 (86.25) 	 P_C 72.38 	 R_C 81.33 	 F_C 75.89 	 P_O 72.68 	 R_O 85.49 	 F_O 78.56
Test: [18/50]	  P_C 72.38 	 R_C 81.33 	 F_C 75.89 	 P_O 72.67 	 R_O 85.48 	 F_O 78.56 	 mAP 83.45
Train: [0/2566]	Time 1.053 (1.053)	Loss 1.34 (1.34)	mAP 45.97 (45.97)
Train: [100/2566]	Time 0.330 (0.338)	Loss 1.59 (1.73)	mAP 45.68 (44.85)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.93 (1.72)	mAP 43.55 (44.92)
Train: [300/2566]	Time 0.331 (0.333)	Loss 2.02 (1.73)	mAP 42.46 (45.07)
Train: [400/2566]	Time 0.330 (0.332)	Loss 2.12 (1.72)	mAP 47.16 (45.05)
Train: [500/2566]	Time 0.329 (0.332)	Loss 1.37 (1.72)	mAP 47.84 (45.26)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.54 (1.73)	mAP 46.72 (45.22)
Train: [700/2566]	Time 0.331 (0.331)	Loss 1.35 (1.72)	mAP 44.80 (45.22)
Train: [800/2566]	Time 0.330 (0.331)	Loss 1.76 (1.72)	mAP 50.63 (45.30)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.93 (1.72)	mAP 44.50 (45.30)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.38 (1.72)	mAP 47.59 (45.33)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 1.76 (1.72)	mAP 38.29 (45.36)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.82 (1.72)	mAP 47.48 (45.38)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 2.09 (1.72)	mAP 39.30 (45.44)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.80 (1.72)	mAP 45.13 (45.45)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.80 (1.72)	mAP 44.59 (45.44)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.74 (1.72)	mAP 48.73 (45.42)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 2.01 (1.72)	mAP 46.67 (45.43)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.56 (1.72)	mAP 41.26 (45.38)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.77 (1.72)	mAP 44.38 (45.39)
Train: [2000/2566]	Time 0.329 (0.331)	Loss 1.78 (1.72)	mAP 49.19 (45.38)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.40 (1.72)	mAP 41.67 (45.37)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.91 (1.72)	mAP 49.44 (45.36)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.47 (1.72)	mAP 40.98 (45.35)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.38 (1.73)	mAP 38.55 (45.34)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.52 (1.73)	mAP 44.05 (45.32)
Train: [19/50]	Time 0.331	Loss 1.72 	mAP 45.32
Test: [0/402]	Time 1.831 (1.831)	Precision 69.98 (69.98)	Recall 80.11 (80.11) 	 P_C 29.27 	 R_C 33.18 	 F_C 29.11 	 P_O 69.98 	 R_O 80.11 	 F_O 74.70
Test: [100/402]	Time 0.500 (0.513)	Precision 85.62 (74.47)	Recall 89.54 (85.10) 	 P_C 65.05 	 R_C 71.58 	 F_C 65.67 	 P_O 72.82 	 R_O 84.03 	 F_O 78.02
Test: [200/402]	Time 0.500 (0.506)	Precision 75.37 (75.20)	Recall 88.74 (85.49) 	 P_C 67.06 	 R_C 74.49 	 F_C 69.08 	 P_O 74.02 	 R_O 84.67 	 F_O 78.98
Test: [300/402]	Time 0.500 (0.504)	Precision 81.45 (75.36)	Recall 84.08 (85.91) 	 P_C 71.84 	 R_C 79.52 	 F_C 74.27 	 P_O 74.04 	 R_O 85.05 	 F_O 79.16
Test: [400/402]	Time 0.500 (0.503)	Precision 55.75 (74.36)	Recall 73.26 (85.95) 	 P_C 73.54 	 R_C 81.03 	 F_C 76.02 	 P_O 73.27 	 R_O 85.31 	 F_O 78.83
Test: [19/50]	  P_C 73.54 	 R_C 81.03 	 F_C 76.02 	 P_O 73.26 	 R_O 85.31 	 F_O 78.83 	 mAP 83.56
Train: [0/2566]	Time 1.034 (1.034)	Loss 1.58 (1.58)	mAP 40.88 (40.88)
Train: [100/2566]	Time 0.329 (0.337)	Loss 1.80 (1.72)	mAP 50.31 (45.01)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.66 (1.74)	mAP 46.58 (45.21)
Train: [300/2566]	Time 0.331 (0.332)	Loss 1.63 (1.72)	mAP 45.33 (45.22)
Train: [400/2566]	Time 0.330 (0.332)	Loss 2.21 (1.72)	mAP 46.36 (45.10)
Train: [500/2566]	Time 0.330 (0.331)	Loss 1.83 (1.71)	mAP 50.81 (45.03)
Train: [600/2566]	Time 0.330 (0.331)	Loss 1.32 (1.70)	mAP 38.86 (44.97)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.29 (1.70)	mAP 44.71 (45.06)
Train: [800/2566]	Time 0.330 (0.331)	Loss 2.08 (1.70)	mAP 49.26 (45.05)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.39 (1.71)	mAP 39.45 (45.07)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.30 (1.71)	mAP 42.85 (45.05)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.86 (1.71)	mAP 41.73 (45.11)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.81 (1.71)	mAP 41.15 (45.15)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.60 (1.71)	mAP 57.30 (45.18)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.60 (1.71)	mAP 50.51 (45.19)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.96 (1.71)	mAP 50.73 (45.17)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 2.28 (1.71)	mAP 45.61 (45.17)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.60 (1.71)	mAP 52.53 (45.17)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.62 (1.72)	mAP 43.59 (45.23)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.63 (1.72)	mAP 40.78 (45.26)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.52 (1.72)	mAP 39.42 (45.23)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.63 (1.72)	mAP 43.63 (45.23)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.15 (1.72)	mAP 46.10 (45.24)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.57 (1.72)	mAP 44.90 (45.26)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.48 (1.72)	mAP 40.67 (45.24)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 2.23 (1.72)	mAP 42.34 (45.24)
Train: [20/50]	Time 0.330	Loss 1.72 	mAP 45.23
Test: [0/402]	Time 1.763 (1.763)	Precision 79.71 (79.71)	Recall 76.99 (76.99) 	 P_C 31.20 	 R_C 28.96 	 F_C 28.62 	 P_O 79.71 	 R_O 76.99 	 F_O 78.32
Test: [100/402]	Time 0.499 (0.512)	Precision 89.12 (81.95)	Recall 85.62 (80.81) 	 P_C 69.90 	 R_C 66.10 	 F_C 66.24 	 P_O 80.79 	 R_O 79.30 	 F_O 80.04
Test: [200/402]	Time 0.501 (0.506)	Precision 85.84 (82.98)	Recall 86.58 (81.40) 	 P_C 73.30 	 R_C 69.12 	 F_C 69.81 	 P_O 82.10 	 R_O 80.27 	 F_O 81.18
Test: [300/402]	Time 0.499 (0.504)	Precision 88.64 (82.91)	Recall 77.61 (81.87) 	 P_C 77.51 	 R_C 74.64 	 F_C 75.17 	 P_O 81.98 	 R_O 80.59 	 F_O 81.28
Test: [400/402]	Time 0.500 (0.503)	Precision 62.98 (82.16)	Recall 66.28 (81.73) 	 P_C 79.90 	 R_C 76.12 	 F_C 77.22 	 P_O 81.43 	 R_O 80.67 	 F_O 81.05
Test: [20/50]	  P_C 79.90 	 R_C 76.11 	 F_C 77.22 	 P_O 81.42 	 R_O 80.67 	 F_O 81.05 	 mAP 83.81
Train: [0/2566]	Time 1.046 (1.046)	Loss 1.59 (1.59)	mAP 48.33 (48.33)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.50 (1.64)	mAP 44.84 (44.84)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.83 (1.65)	mAP 42.40 (45.21)
Train: [300/2566]	Time 0.330 (0.332)	Loss 1.63 (1.68)	mAP 40.04 (45.28)
Train: [400/2566]	Time 0.329 (0.332)	Loss 1.59 (1.67)	mAP 38.41 (45.17)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.45 (1.68)	mAP 44.34 (45.31)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.85 (1.69)	mAP 49.87 (45.29)
Train: [700/2566]	Time 0.330 (0.331)	Loss 2.54 (1.70)	mAP 50.38 (45.29)
Train: [800/2566]	Time 0.331 (0.331)	Loss 1.79 (1.71)	mAP 41.76 (45.27)
Train: [900/2566]	Time 0.330 (0.331)	Loss 2.09 (1.71)	mAP 40.16 (45.25)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.51 (1.71)	mAP 46.76 (45.28)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 1.86 (1.71)	mAP 43.69 (45.26)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.51 (1.71)	mAP 48.76 (45.21)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.36 (1.72)	mAP 32.92 (45.23)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.42 (1.71)	mAP 44.55 (45.25)
Train: [1500/2566]	Time 0.329 (0.331)	Loss 1.95 (1.71)	mAP 46.76 (45.26)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.76 (1.71)	mAP 47.76 (45.26)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.49 (1.71)	mAP 40.92 (45.29)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 2.04 (1.71)	mAP 44.14 (45.27)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 2.04 (1.71)	mAP 48.46 (45.30)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 2.14 (1.71)	mAP 48.04 (45.28)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.62 (1.71)	mAP 47.33 (45.26)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.41 (1.72)	mAP 41.83 (45.27)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.30 (1.72)	mAP 36.35 (45.25)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.29 (1.72)	mAP 39.06 (45.26)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.32 (1.72)	mAP 45.15 (45.25)
Train: [21/50]	Time 0.330	Loss 1.72 	mAP 45.26
Test: [0/402]	Time 1.834 (1.834)	Precision 77.87 (77.87)	Recall 76.99 (76.99) 	 P_C 30.64 	 R_C 30.11 	 F_C 29.06 	 P_O 77.87 	 R_O 76.99 	 F_O 77.43
Test: [100/402]	Time 0.500 (0.513)	Precision 92.31 (81.75)	Recall 86.27 (81.14) 	 P_C 70.86 	 R_C 66.60 	 F_C 66.92 	 P_O 80.48 	 R_O 79.61 	 F_O 80.04
Test: [200/402]	Time 0.500 (0.506)	Precision 86.90 (82.24)	Recall 86.15 (81.80) 	 P_C 73.97 	 R_C 68.90 	 F_C 70.18 	 P_O 81.32 	 R_O 80.60 	 F_O 80.96
Test: [300/402]	Time 0.500 (0.504)	Precision 87.30 (82.42)	Recall 80.35 (82.01) 	 P_C 77.46 	 R_C 74.70 	 F_C 75.26 	 P_O 81.40 	 R_O 80.84 	 F_O 81.12
Test: [400/402]	Time 0.499 (0.503)	Precision 65.34 (81.56)	Recall 66.86 (81.92) 	 P_C 79.71 	 R_C 76.12 	 F_C 77.15 	 P_O 80.72 	 R_O 80.97 	 F_O 80.84
Test: [21/50]	  P_C 79.71 	 R_C 76.12 	 F_C 77.14 	 P_O 80.71 	 R_O 80.96 	 F_O 80.84 	 mAP 83.88
Train: [0/2566]	Time 1.060 (1.060)	Loss 1.68 (1.68)	mAP 45.19 (45.19)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.49 (1.70)	mAP 42.77 (45.48)
Train: [200/2566]	Time 0.329 (0.333)	Loss 1.73 (1.71)	mAP 46.51 (45.33)
Train: [300/2566]	Time 0.330 (0.332)	Loss 1.73 (1.70)	mAP 44.10 (45.27)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.36 (1.70)	mAP 46.87 (45.18)
Train: [500/2566]	Time 0.331 (0.331)	Loss 2.03 (1.70)	mAP 51.81 (45.19)
Train: [600/2566]	Time 0.330 (0.331)	Loss 1.64 (1.70)	mAP 44.37 (45.15)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.82 (1.69)	mAP 41.68 (45.12)
Train: [800/2566]	Time 0.330 (0.331)	Loss 1.78 (1.70)	mAP 47.27 (45.16)
Train: [900/2566]	Time 0.331 (0.331)	Loss 2.17 (1.69)	mAP 47.08 (45.22)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.23 (1.69)	mAP 44.44 (45.22)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.29 (1.69)	mAP 37.47 (45.21)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.52 (1.70)	mAP 43.04 (45.30)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.68 (1.70)	mAP 44.82 (45.31)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.24 (1.70)	mAP 35.97 (45.34)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.52 (1.70)	mAP 49.02 (45.35)
Train: [1600/2566]	Time 0.329 (0.331)	Loss 1.34 (1.70)	mAP 43.99 (45.39)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.81 (1.70)	mAP 44.13 (45.39)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.61 (1.70)	mAP 45.38 (45.37)
Train: [1900/2566]	Time 0.330 (0.330)	Loss 1.46 (1.70)	mAP 45.92 (45.39)
Train: [2000/2566]	Time 0.330 (0.330)	Loss 2.07 (1.71)	mAP 43.11 (45.40)
Train: [2100/2566]	Time 0.330 (0.330)	Loss 2.37 (1.71)	mAP 49.60 (45.42)
Train: [2200/2566]	Time 0.330 (0.330)	Loss 1.42 (1.71)	mAP 42.15 (45.43)
Train: [2300/2566]	Time 0.330 (0.330)	Loss 1.41 (1.71)	mAP 40.22 (45.40)
Train: [2400/2566]	Time 0.330 (0.330)	Loss 1.79 (1.71)	mAP 45.68 (45.38)
Train: [2500/2566]	Time 0.331 (0.330)	Loss 1.59 (1.71)	mAP 43.81 (45.38)
Train: [22/50]	Time 0.330	Loss 1.71 	mAP 45.37
Test: [0/402]	Time 1.794 (1.794)	Precision 74.40 (74.40)	Recall 79.26 (79.26) 	 P_C 30.31 	 R_C 31.35 	 F_C 29.67 	 P_O 74.40 	 R_O 79.26 	 F_O 76.75
Test: [100/402]	Time 0.499 (0.512)	Precision 86.62 (77.78)	Recall 88.89 (83.68) 	 P_C 66.25 	 R_C 69.92 	 F_C 66.72 	 P_O 76.63 	 R_O 82.15 	 F_O 79.30
Test: [200/402]	Time 0.499 (0.506)	Precision 81.89 (77.98)	Recall 86.15 (84.30) 	 P_C 69.34 	 R_C 72.14 	 F_C 69.81 	 P_O 77.08 	 R_O 83.18 	 F_O 80.02
Test: [300/402]	Time 0.500 (0.504)	Precision 84.48 (77.93)	Recall 82.59 (84.96) 	 P_C 73.15 	 R_C 78.20 	 F_C 74.89 	 P_O 76.90 	 R_O 83.86 	 F_O 80.23
Test: [400/402]	Time 0.500 (0.503)	Precision 60.40 (77.07)	Recall 70.93 (84.91) 	 P_C 74.95 	 R_C 79.92 	 F_C 76.82 	 P_O 76.15 	 R_O 84.05 	 F_O 79.90
Test: [22/50]	  P_C 74.95 	 R_C 79.92 	 F_C 76.81 	 P_O 76.14 	 R_O 84.04 	 F_O 79.89 	 mAP 83.88
Train: [0/2566]	Time 1.016 (1.016)	Loss 1.92 (1.92)	mAP 43.59 (43.59)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.27 (1.78)	mAP 41.07 (45.01)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.73 (1.76)	mAP 47.72 (45.23)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.80 (1.72)	mAP 51.11 (45.35)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.46 (1.71)	mAP 42.61 (45.40)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.78 (1.70)	mAP 47.57 (45.39)
Train: [600/2566]	Time 0.331 (0.332)	Loss 2.11 (1.70)	mAP 44.51 (45.37)
Train: [700/2566]	Time 0.331 (0.331)	Loss 1.78 (1.70)	mAP 43.63 (45.35)
Train: [800/2566]	Time 0.330 (0.331)	Loss 1.78 (1.70)	mAP 52.77 (45.31)
Train: [900/2566]	Time 0.331 (0.331)	Loss 1.99 (1.69)	mAP 46.48 (45.25)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.98 (1.69)	mAP 45.53 (45.27)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.80 (1.69)	mAP 44.36 (45.29)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.71 (1.70)	mAP 45.49 (45.21)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.27 (1.70)	mAP 33.90 (45.22)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.62 (1.70)	mAP 39.93 (45.24)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.01 (1.70)	mAP 42.74 (45.23)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.51 (1.70)	mAP 49.72 (45.28)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.65 (1.70)	mAP 48.43 (45.30)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 2.39 (1.70)	mAP 38.76 (45.28)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.72 (1.70)	mAP 48.21 (45.29)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.89 (1.70)	mAP 45.11 (45.29)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.76 (1.71)	mAP 44.36 (45.28)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.81 (1.71)	mAP 42.68 (45.29)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.74 (1.71)	mAP 39.86 (45.31)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.82 (1.71)	mAP 44.30 (45.32)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.63 (1.71)	mAP 39.85 (45.35)
Train: [23/50]	Time 0.331	Loss 1.71 	mAP 45.38
Test: [0/402]	Time 1.831 (1.831)	Precision 73.30 (73.30)	Recall 79.55 (79.55) 	 P_C 29.68 	 R_C 32.87 	 F_C 29.74 	 P_O 73.30 	 R_O 79.55 	 F_O 76.29
Test: [100/402]	Time 0.500 (0.513)	Precision 90.07 (77.88)	Recall 88.89 (83.92) 	 P_C 65.43 	 R_C 70.89 	 F_C 66.91 	 P_O 76.01 	 R_O 82.75 	 F_O 79.24
Test: [200/402]	Time 0.499 (0.506)	Precision 83.82 (78.77)	Recall 87.45 (84.26) 	 P_C 68.38 	 R_C 73.62 	 F_C 70.24 	 P_O 77.36 	 R_O 83.40 	 F_O 80.27
Test: [300/402]	Time 0.500 (0.504)	Precision 82.43 (78.45)	Recall 82.84 (84.97) 	 P_C 72.69 	 R_C 78.82 	 F_C 75.02 	 P_O 76.94 	 R_O 84.11 	 F_O 80.37
Test: [400/402]	Time 0.500 (0.503)	Precision 58.45 (77.40)	Recall 70.35 (84.99) 	 P_C 74.78 	 R_C 80.29 	 F_C 76.88 	 P_O 76.12 	 R_O 84.34 	 F_O 80.02
Test: [23/50]	  P_C 74.78 	 R_C 80.28 	 F_C 76.88 	 P_O 76.11 	 R_O 84.33 	 F_O 80.01 	 mAP 83.97
Train: [0/2566]	Time 0.983 (0.983)	Loss 1.33 (1.33)	mAP 45.11 (45.11)
Train: [100/2566]	Time 0.329 (0.336)	Loss 1.32 (1.75)	mAP 37.96 (45.47)
Train: [200/2566]	Time 0.331 (0.333)	Loss 1.89 (1.72)	mAP 48.51 (45.68)
Train: [300/2566]	Time 0.330 (0.332)	Loss 2.11 (1.73)	mAP 51.53 (45.42)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.37 (1.72)	mAP 42.88 (45.40)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.41 (1.71)	mAP 46.31 (45.37)
Train: [600/2566]	Time 0.331 (0.331)	Loss 1.92 (1.70)	mAP 48.40 (45.27)
Train: [700/2566]	Time 0.331 (0.331)	Loss 1.36 (1.70)	mAP 44.17 (45.22)
Train: [800/2566]	Time 0.331 (0.331)	Loss 1.62 (1.70)	mAP 44.42 (45.23)
Train: [900/2566]	Time 0.331 (0.331)	Loss 1.69 (1.70)	mAP 42.21 (45.28)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 1.31 (1.70)	mAP 41.35 (45.29)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.95 (1.70)	mAP 35.98 (45.27)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.78 (1.70)	mAP 46.34 (45.27)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.41 (1.70)	mAP 48.55 (45.29)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.88 (1.71)	mAP 40.96 (45.33)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.62 (1.71)	mAP 50.63 (45.35)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.81 (1.71)	mAP 50.22 (45.34)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.70 (1.71)	mAP 44.81 (45.34)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.56 (1.70)	mAP 48.43 (45.30)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.70 (1.70)	mAP 45.69 (45.31)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.94 (1.70)	mAP 39.73 (45.28)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.59 (1.70)	mAP 46.93 (45.28)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.61 (1.70)	mAP 45.97 (45.28)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.53 (1.70)	mAP 48.17 (45.30)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.65 (1.70)	mAP 46.62 (45.28)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.45 (1.70)	mAP 45.73 (45.27)
Train: [24/50]	Time 0.331	Loss 1.71 	mAP 45.29
Test: [0/402]	Time 1.801 (1.801)	Precision 74.14 (74.14)	Recall 79.83 (79.83) 	 P_C 28.87 	 R_C 31.09 	 F_C 28.31 	 P_O 74.14 	 R_O 79.83 	 F_O 76.88
Test: [100/402]	Time 0.500 (0.513)	Precision 84.34 (75.98)	Recall 91.50 (84.74) 	 P_C 65.68 	 R_C 70.42 	 F_C 66.36 	 P_O 74.83 	 R_O 83.34 	 F_O 78.86
Test: [200/402]	Time 0.500 (0.506)	Precision 81.78 (76.74)	Recall 87.45 (85.09) 	 P_C 68.47 	 R_C 73.14 	 F_C 69.55 	 P_O 75.85 	 R_O 84.06 	 F_O 79.74
Test: [300/402]	Time 0.500 (0.504)	Precision 83.96 (76.88)	Recall 83.33 (85.64) 	 P_C 72.53 	 R_C 79.17 	 F_C 74.93 	 P_O 75.88 	 R_O 84.58 	 F_O 80.00
Test: [400/402]	Time 0.500 (0.503)	Precision 56.81 (76.01)	Recall 70.35 (85.55) 	 P_C 74.41 	 R_C 80.65 	 F_C 76.72 	 P_O 75.15 	 R_O 84.70 	 F_O 79.64
Test: [24/50]	  P_C 74.41 	 R_C 80.65 	 F_C 76.72 	 P_O 75.15 	 R_O 84.69 	 F_O 79.64 	 mAP 83.96
Train: [0/2566]	Time 1.038 (1.038)	Loss 1.21 (1.21)	mAP 42.33 (42.33)
Train: [100/2566]	Time 0.331 (0.337)	Loss 1.70 (1.72)	mAP 44.86 (45.75)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.95 (1.71)	mAP 54.70 (45.87)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.36 (1.69)	mAP 46.85 (45.71)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.87 (1.68)	mAP 44.19 (45.43)
Train: [500/2566]	Time 0.331 (0.332)	Loss 2.07 (1.68)	mAP 45.45 (45.50)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.64 (1.68)	mAP 53.69 (45.51)
Train: [700/2566]	Time 0.331 (0.331)	Loss 1.71 (1.68)	mAP 42.89 (45.54)
Train: [800/2566]	Time 0.331 (0.331)	Loss 1.44 (1.69)	mAP 53.39 (45.56)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.52 (1.69)	mAP 37.80 (45.51)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.37 (1.69)	mAP 46.96 (45.51)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.21 (1.69)	mAP 45.04 (45.47)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.88 (1.69)	mAP 43.43 (45.47)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 2.00 (1.69)	mAP 47.40 (45.46)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 2.56 (1.69)	mAP 39.26 (45.47)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.92 (1.69)	mAP 45.89 (45.47)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.98 (1.69)	mAP 44.60 (45.49)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.66 (1.69)	mAP 46.75 (45.49)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.32 (1.70)	mAP 35.45 (45.46)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.75 (1.70)	mAP 48.46 (45.45)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.82 (1.70)	mAP 50.87 (45.45)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.76 (1.70)	mAP 43.69 (45.45)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.84 (1.70)	mAP 42.82 (45.43)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.17 (1.70)	mAP 43.57 (45.41)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.76 (1.70)	mAP 44.30 (45.41)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.97 (1.69)	mAP 47.18 (45.41)
Train: [25/50]	Time 0.331	Loss 1.70 	mAP 45.40
Test: [0/402]	Time 1.798 (1.798)	Precision 74.60 (74.60)	Recall 79.26 (79.26) 	 P_C 29.21 	 R_C 31.62 	 F_C 29.29 	 P_O 74.60 	 R_O 79.26 	 F_O 76.86
Test: [100/402]	Time 0.500 (0.512)	Precision 88.46 (77.58)	Recall 90.20 (84.08) 	 P_C 65.08 	 R_C 71.96 	 F_C 66.81 	 P_O 74.86 	 R_O 83.16 	 F_O 78.79
Test: [200/402]	Time 0.500 (0.506)	Precision 82.99 (79.01)	Recall 86.58 (84.06) 	 P_C 67.55 	 R_C 74.61 	 F_C 69.98 	 P_O 77.07 	 R_O 83.30 	 F_O 80.06
Test: [300/402]	Time 0.501 (0.504)	Precision 82.37 (78.77)	Recall 81.34 (84.77) 	 P_C 72.59 	 R_C 79.27 	 F_C 75.06 	 P_O 76.79 	 R_O 83.97 	 F_O 80.22
Test: [400/402]	Time 0.499 (0.503)	Precision 57.75 (77.62)	Recall 71.51 (84.88) 	 P_C 75.05 	 R_C 80.74 	 F_C 77.05 	 P_O 75.77 	 R_O 84.30 	 F_O 79.81
Test: [25/50]	  P_C 75.06 	 R_C 80.74 	 F_C 77.05 	 P_O 75.77 	 R_O 84.30 	 F_O 79.81 	 mAP 84.01
Train: [0/2566]	Time 1.026 (1.026)	Loss 1.83 (1.83)	mAP 48.61 (48.61)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.57 (1.66)	mAP 51.84 (45.09)
Train: [200/2566]	Time 0.330 (0.334)	Loss 2.08 (1.67)	mAP 43.83 (45.23)
Train: [300/2566]	Time 0.329 (0.332)	Loss 1.80 (1.68)	mAP 41.95 (45.49)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.56 (1.69)	mAP 49.67 (45.37)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.71 (1.70)	mAP 48.81 (45.51)
Train: [600/2566]	Time 0.330 (0.331)	Loss 2.08 (1.70)	mAP 50.50 (45.58)
Train: [700/2566]	Time 0.331 (0.331)	Loss 1.62 (1.71)	mAP 49.40 (45.61)
Train: [800/2566]	Time 0.330 (0.331)	Loss 1.66 (1.71)	mAP 50.18 (45.57)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.95 (1.70)	mAP 40.71 (45.51)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 1.65 (1.70)	mAP 44.06 (45.46)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 1.68 (1.70)	mAP 44.06 (45.46)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.38 (1.70)	mAP 39.04 (45.41)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.75 (1.69)	mAP 43.19 (45.34)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.91 (1.69)	mAP 46.45 (45.39)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.69 (1.70)	mAP 42.03 (45.40)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.47 (1.70)	mAP 41.59 (45.39)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.68 (1.69)	mAP 50.21 (45.42)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.63 (1.69)	mAP 42.64 (45.43)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.03 (1.70)	mAP 45.96 (45.42)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 2.01 (1.70)	mAP 49.68 (45.47)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.49 (1.70)	mAP 43.50 (45.47)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.81 (1.70)	mAP 48.44 (45.44)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.73 (1.70)	mAP 43.07 (45.42)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 2.21 (1.70)	mAP 47.78 (45.40)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.56 (1.70)	mAP 40.75 (45.40)
Train: [26/50]	Time 0.331	Loss 1.70 	mAP 45.41
Test: [0/402]	Time 1.808 (1.808)	Precision 73.88 (73.88)	Recall 79.55 (79.55) 	 P_C 31.54 	 R_C 32.71 	 F_C 30.90 	 P_O 73.88 	 R_O 79.55 	 F_O 76.61
Test: [100/402]	Time 0.499 (0.513)	Precision 91.10 (78.74)	Recall 86.93 (83.17) 	 P_C 68.77 	 R_C 68.91 	 F_C 67.37 	 P_O 77.23 	 R_O 81.78 	 F_O 79.44
Test: [200/402]	Time 0.500 (0.506)	Precision 80.16 (79.31)	Recall 85.71 (83.81) 	 P_C 71.06 	 R_C 71.73 	 F_C 70.51 	 P_O 78.13 	 R_O 82.77 	 F_O 80.38
Test: [300/402]	Time 0.500 (0.504)	Precision 82.75 (79.37)	Recall 82.34 (84.39) 	 P_C 74.93 	 R_C 77.36 	 F_C 75.45 	 P_O 78.01 	 R_O 83.35 	 F_O 80.59
Test: [400/402]	Time 0.500 (0.503)	Precision 60.00 (78.35)	Recall 69.77 (84.46) 	 P_C 77.23 	 R_C 79.18 	 F_C 77.57 	 P_O 77.16 	 R_O 83.66 	 F_O 80.28
Test: [26/50]	  P_C 77.22 	 R_C 79.18 	 F_C 77.57 	 P_O 77.16 	 R_O 83.66 	 F_O 80.28 	 mAP 84.13
Train: [0/2566]	Time 1.021 (1.021)	Loss 1.88 (1.88)	mAP 39.14 (39.14)
Train: [100/2566]	Time 0.331 (0.337)	Loss 1.69 (1.74)	mAP 46.67 (45.86)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.69 (1.69)	mAP 46.60 (45.76)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.56 (1.68)	mAP 38.35 (45.63)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.35 (1.68)	mAP 58.02 (45.60)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.47 (1.69)	mAP 43.83 (45.50)
Train: [600/2566]	Time 0.329 (0.332)	Loss 1.50 (1.69)	mAP 38.91 (45.49)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.52 (1.69)	mAP 45.82 (45.40)
Train: [800/2566]	Time 0.329 (0.331)	Loss 1.50 (1.69)	mAP 37.36 (45.42)
Train: [900/2566]	Time 0.331 (0.331)	Loss 1.33 (1.70)	mAP 52.10 (45.43)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.39 (1.69)	mAP 40.68 (45.43)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.88 (1.69)	mAP 47.38 (45.49)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.24 (1.69)	mAP 45.17 (45.46)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.72 (1.69)	mAP 52.05 (45.42)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.54 (1.69)	mAP 44.41 (45.40)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.43 (1.69)	mAP 33.31 (45.38)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.76 (1.69)	mAP 50.62 (45.37)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.91 (1.69)	mAP 43.04 (45.37)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.58 (1.69)	mAP 44.18 (45.35)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.77 (1.69)	mAP 48.45 (45.41)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.67 (1.69)	mAP 48.01 (45.41)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.54 (1.69)	mAP 39.69 (45.39)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.69 (1.69)	mAP 47.77 (45.38)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.64 (1.69)	mAP 46.73 (45.41)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.83 (1.69)	mAP 50.03 (45.41)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.60 (1.69)	mAP 39.08 (45.42)
Train: [27/50]	Time 0.331	Loss 1.69 	mAP 45.42
Test: [0/402]	Time 1.815 (1.815)	Precision 79.77 (79.77)	Recall 77.27 (77.27) 	 P_C 31.22 	 R_C 29.52 	 F_C 29.32 	 P_O 79.77 	 R_O 77.27 	 F_O 78.50
Test: [100/402]	Time 0.500 (0.513)	Precision 91.84 (83.08)	Recall 88.24 (80.76) 	 P_C 70.35 	 R_C 67.48 	 F_C 67.31 	 P_O 81.56 	 R_O 79.32 	 F_O 80.42
Test: [200/402]	Time 0.500 (0.506)	Precision 89.14 (83.70)	Recall 85.28 (81.35) 	 P_C 73.42 	 R_C 70.04 	 F_C 70.70 	 P_O 82.61 	 R_O 80.22 	 F_O 81.40
Test: [300/402]	Time 0.500 (0.504)	Precision 87.50 (83.50)	Recall 80.10 (82.10) 	 P_C 77.53 	 R_C 75.53 	 F_C 75.76 	 P_O 82.21 	 R_O 80.96 	 F_O 81.58
Test: [400/402]	Time 0.499 (0.503)	Precision 62.23 (82.43)	Recall 68.02 (82.21) 	 P_C 79.87 	 R_C 77.17 	 F_C 77.85 	 P_O 81.29 	 R_O 81.29 	 F_O 81.29
Test: [27/50]	  P_C 79.87 	 R_C 77.17 	 F_C 77.85 	 P_O 81.28 	 R_O 81.29 	 F_O 81.28 	 mAP 84.28
Train: [0/2566]	Time 0.975 (0.975)	Loss 1.69 (1.69)	mAP 45.39 (45.39)
Train: [100/2566]	Time 0.330 (0.336)	Loss 1.78 (1.68)	mAP 48.82 (46.05)
Train: [200/2566]	Time 0.336 (0.333)	Loss 1.93 (1.69)	mAP 47.04 (45.80)
Train: [300/2566]	Time 0.332 (0.333)	Loss 1.80 (1.70)	mAP 46.48 (45.71)
Train: [400/2566]	Time 0.330 (0.333)	Loss 1.41 (1.70)	mAP 51.28 (45.74)
Train: [500/2566]	Time 0.330 (0.334)	Loss 1.79 (1.70)	mAP 43.03 (45.46)
Train: [600/2566]	Time 0.331 (0.334)	Loss 1.62 (1.69)	mAP 45.56 (45.49)
Train: [700/2566]	Time 0.330 (0.333)	Loss 1.41 (1.70)	mAP 46.11 (45.48)
Train: [800/2566]	Time 0.329 (0.333)	Loss 1.49 (1.69)	mAP 47.49 (45.49)
Train: [900/2566]	Time 0.332 (0.333)	Loss 1.61 (1.69)	mAP 48.46 (45.43)
Train: [1000/2566]	Time 0.331 (0.332)	Loss 1.80 (1.69)	mAP 46.72 (45.41)
Train: [1100/2566]	Time 0.330 (0.332)	Loss 1.26 (1.68)	mAP 48.74 (45.41)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 1.62 (1.68)	mAP 43.47 (45.41)
Train: [1300/2566]	Time 0.331 (0.332)	Loss 1.84 (1.68)	mAP 44.73 (45.46)
Train: [1400/2566]	Time 0.330 (0.332)	Loss 1.73 (1.69)	mAP 44.07 (45.48)
Train: [1500/2566]	Time 0.330 (0.332)	Loss 1.33 (1.69)	mAP 43.10 (45.49)
Train: [1600/2566]	Time 0.330 (0.332)	Loss 1.91 (1.69)	mAP 43.82 (45.48)
Train: [1700/2566]	Time 0.331 (0.332)	Loss 1.41 (1.69)	mAP 45.29 (45.47)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.46 (1.68)	mAP 43.68 (45.48)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.17 (1.68)	mAP 46.21 (45.46)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.50 (1.68)	mAP 38.90 (45.47)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.80 (1.68)	mAP 46.45 (45.45)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.56 (1.68)	mAP 40.65 (45.47)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.58 (1.68)	mAP 45.23 (45.48)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.80 (1.68)	mAP 41.81 (45.45)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.67 (1.69)	mAP 42.87 (45.47)
Train: [28/50]	Time 0.331	Loss 1.68 	mAP 45.48
Test: [0/402]	Time 1.823 (1.823)	Precision 70.42 (70.42)	Recall 81.82 (81.82) 	 P_C 29.57 	 R_C 35.29 	 F_C 30.22 	 P_O 70.42 	 R_O 81.82 	 F_O 75.69
Test: [100/402]	Time 0.500 (0.512)	Precision 86.88 (72.05)	Recall 90.85 (86.96) 	 P_C 60.66 	 R_C 74.96 	 F_C 65.60 	 P_O 69.40 	 R_O 86.31 	 F_O 76.94
Test: [200/402]	Time 0.500 (0.506)	Precision 80.31 (73.43)	Recall 88.31 (86.87) 	 P_C 63.48 	 R_C 77.20 	 F_C 68.64 	 P_O 71.34 	 R_O 86.33 	 F_O 78.12
Test: [300/402]	Time 0.501 (0.504)	Precision 76.55 (73.60)	Recall 86.07 (87.49) 	 P_C 68.60 	 R_C 82.47 	 F_C 74.00 	 P_O 71.23 	 R_O 86.97 	 F_O 78.32
Test: [400/402]	Time 0.500 (0.503)	Precision 52.16 (72.12)	Recall 77.33 (87.73) 	 P_C 70.28 	 R_C 83.69 	 F_C 75.74 	 P_O 70.03 	 R_O 87.40 	 F_O 77.76
Test: [28/50]	  P_C 70.27 	 R_C 83.69 	 F_C 75.74 	 P_O 70.03 	 R_O 87.39 	 F_O 77.75 	 mAP 84.18
Train: [0/2566]	Time 0.997 (0.997)	Loss 1.57 (1.57)	mAP 49.00 (49.00)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.49 (1.70)	mAP 42.61 (45.39)
Train: [200/2566]	Time 0.330 (0.333)	Loss 1.78 (1.69)	mAP 44.59 (45.57)
Train: [300/2566]	Time 0.330 (0.332)	Loss 1.78 (1.69)	mAP 37.49 (45.83)
Train: [400/2566]	Time 0.341 (0.332)	Loss 1.75 (1.68)	mAP 46.45 (45.76)
Train: [500/2566]	Time 0.340 (0.332)	Loss 1.48 (1.68)	mAP 42.68 (45.78)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.57 (1.69)	mAP 32.47 (45.66)
Train: [700/2566]	Time 0.329 (0.332)	Loss 1.62 (1.69)	mAP 42.51 (45.60)
Train: [800/2566]	Time 0.329 (0.332)	Loss 1.29 (1.69)	mAP 39.96 (45.58)
Train: [900/2566]	Time 0.336 (0.332)	Loss 1.90 (1.69)	mAP 49.82 (45.56)
Train: [1000/2566]	Time 0.329 (0.332)	Loss 1.57 (1.69)	mAP 38.27 (45.53)
Train: [1100/2566]	Time 0.330 (0.332)	Loss 1.63 (1.69)	mAP 42.65 (45.52)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.45 (1.69)	mAP 46.33 (45.51)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.54 (1.69)	mAP 42.71 (45.53)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.77 (1.68)	mAP 44.90 (45.54)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.85 (1.68)	mAP 38.53 (45.52)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.84 (1.68)	mAP 37.26 (45.46)
Train: [1700/2566]	Time 0.329 (0.331)	Loss 1.62 (1.68)	mAP 54.05 (45.50)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.65 (1.68)	mAP 51.17 (45.47)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.98 (1.68)	mAP 46.17 (45.48)
Train: [2000/2566]	Time 0.332 (0.331)	Loss 1.86 (1.68)	mAP 47.78 (45.48)
Train: [2100/2566]	Time 0.339 (0.331)	Loss 1.24 (1.68)	mAP 50.58 (45.46)
Train: [2200/2566]	Time 0.329 (0.331)	Loss 1.93 (1.68)	mAP 35.78 (45.44)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.75 (1.68)	mAP 44.63 (45.44)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.83 (1.68)	mAP 51.57 (45.46)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.84 (1.68)	mAP 46.50 (45.45)
Train: [29/50]	Time 0.331	Loss 1.68 	mAP 45.45
Test: [0/402]	Time 1.853 (1.853)	Precision 71.61 (71.61)	Recall 80.97 (80.97) 	 P_C 28.72 	 R_C 33.40 	 F_C 29.28 	 P_O 71.61 	 R_O 80.97 	 F_O 76.00
Test: [100/402]	Time 0.500 (0.513)	Precision 85.19 (74.19)	Recall 90.20 (85.90) 	 P_C 64.28 	 R_C 72.37 	 F_C 66.65 	 P_O 72.35 	 R_O 84.90 	 F_O 78.12
Test: [200/402]	Time 0.500 (0.506)	Precision 79.30 (74.97)	Recall 87.88 (86.16) 	 P_C 66.71 	 R_C 75.22 	 F_C 69.66 	 P_O 73.59 	 R_O 85.38 	 F_O 79.04
Test: [300/402]	Time 0.500 (0.504)	Precision 80.52 (74.94)	Recall 84.33 (86.80) 	 P_C 71.34 	 R_C 80.78 	 F_C 74.93 	 P_O 73.54 	 R_O 85.95 	 F_O 79.27
Test: [400/402]	Time 0.500 (0.503)	Precision 54.81 (74.05)	Recall 76.16 (86.84) 	 P_C 73.37 	 R_C 82.35 	 F_C 76.93 	 P_O 72.86 	 R_O 86.20 	 F_O 78.97
Test: [29/50]	  P_C 73.37 	 R_C 82.35 	 F_C 76.93 	 P_O 72.86 	 R_O 86.20 	 F_O 78.97 	 mAP 84.31
Train: [0/2566]	Time 1.030 (1.030)	Loss 1.99 (1.99)	mAP 49.05 (49.05)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.57 (1.63)	mAP 39.97 (45.49)
Train: [200/2566]	Time 0.330 (0.334)	Loss 2.01 (1.67)	mAP 45.25 (45.64)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.48 (1.66)	mAP 44.94 (45.57)
Train: [400/2566]	Time 0.330 (0.332)	Loss 2.10 (1.66)	mAP 51.09 (45.81)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.59 (1.66)	mAP 41.88 (45.70)
Train: [600/2566]	Time 0.331 (0.331)	Loss 1.43 (1.66)	mAP 42.26 (45.59)
Train: [700/2566]	Time 0.333 (0.331)	Loss 1.69 (1.66)	mAP 42.25 (45.66)
Train: [800/2566]	Time 0.330 (0.331)	Loss 1.68 (1.66)	mAP 45.98 (45.66)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.41 (1.66)	mAP 49.41 (45.67)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 1.97 (1.67)	mAP 44.61 (45.67)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.79 (1.67)	mAP 52.21 (45.68)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.60 (1.67)	mAP 37.92 (45.66)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.53 (1.67)	mAP 38.85 (45.67)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.77 (1.67)	mAP 41.50 (45.66)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.58 (1.67)	mAP 50.73 (45.60)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.50 (1.67)	mAP 51.09 (45.60)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.40 (1.67)	mAP 41.71 (45.59)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.50 (1.67)	mAP 38.89 (45.64)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.74 (1.67)	mAP 40.86 (45.61)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.79 (1.67)	mAP 49.15 (45.61)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.42 (1.68)	mAP 46.20 (45.57)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.74 (1.68)	mAP 49.46 (45.56)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.36 (1.68)	mAP 47.82 (45.59)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.70 (1.68)	mAP 46.68 (45.61)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.93 (1.68)	mAP 47.04 (45.60)
Train: [30/50]	Time 0.331	Loss 1.68 	mAP 45.59
Test: [0/402]	Time 1.769 (1.769)	Precision 76.29 (76.29)	Recall 79.55 (79.55) 	 P_C 31.18 	 R_C 32.03 	 F_C 30.70 	 P_O 76.29 	 R_O 79.55 	 F_O 77.89
Test: [100/402]	Time 0.504 (0.514)	Precision 87.18 (78.50)	Recall 88.89 (83.49) 	 P_C 68.17 	 R_C 68.64 	 F_C 67.01 	 P_O 77.71 	 R_O 82.03 	 F_O 79.81
Test: [200/402]	Time 0.500 (0.508)	Precision 81.20 (79.13)	Recall 87.88 (83.98) 	 P_C 70.99 	 R_C 71.64 	 F_C 70.43 	 P_O 78.54 	 R_O 82.87 	 F_O 80.65
Test: [300/402]	Time 0.500 (0.505)	Precision 84.22 (79.32)	Recall 82.34 (84.49) 	 P_C 75.22 	 R_C 77.53 	 F_C 75.60 	 P_O 78.67 	 R_O 83.31 	 F_O 80.92
Test: [400/402]	Time 0.500 (0.504)	Precision 64.84 (78.84)	Recall 68.60 (84.31) 	 P_C 77.70 	 R_C 79.11 	 F_C 77.48 	 P_O 78.31 	 R_O 83.37 	 F_O 80.76
Test: [30/50]	  P_C 77.70 	 R_C 79.11 	 F_C 77.48 	 P_O 78.30 	 R_O 83.37 	 F_O 80.76 	 mAP 84.37
Train: [0/2566]	Time 1.031 (1.031)	Loss 1.42 (1.42)	mAP 41.20 (41.20)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.40 (1.67)	mAP 46.40 (44.62)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.87 (1.66)	mAP 40.22 (44.88)
Train: [300/2566]	Time 0.332 (0.334)	Loss 1.55 (1.67)	mAP 46.89 (45.17)
Train: [400/2566]	Time 0.331 (0.334)	Loss 2.15 (1.68)	mAP 43.13 (45.41)
Train: [500/2566]	Time 0.332 (0.333)	Loss 1.35 (1.67)	mAP 46.63 (45.45)
Train: [600/2566]	Time 0.330 (0.333)	Loss 2.04 (1.67)	mAP 42.22 (45.43)
Train: [700/2566]	Time 0.329 (0.332)	Loss 1.47 (1.67)	mAP 44.30 (45.52)
Train: [800/2566]	Time 0.330 (0.332)	Loss 1.42 (1.68)	mAP 41.40 (45.59)
Train: [900/2566]	Time 0.330 (0.332)	Loss 1.40 (1.68)	mAP 43.85 (45.53)
Train: [1000/2566]	Time 0.329 (0.332)	Loss 0.97 (1.68)	mAP 40.50 (45.50)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.60 (1.68)	mAP 44.59 (45.52)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.59 (1.68)	mAP 38.85 (45.50)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.50 (1.68)	mAP 48.54 (45.50)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.82 (1.68)	mAP 45.09 (45.53)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.46 (1.68)	mAP 44.88 (45.52)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.61 (1.68)	mAP 52.39 (45.54)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.32 (1.67)	mAP 47.61 (45.55)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.42 (1.68)	mAP 51.47 (45.52)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 2.21 (1.68)	mAP 46.83 (45.52)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.39 (1.68)	mAP 48.53 (45.52)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.81 (1.68)	mAP 45.40 (45.54)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.08 (1.68)	mAP 46.25 (45.53)
Train: [2300/2566]	Time 0.329 (0.331)	Loss 2.46 (1.67)	mAP 44.94 (45.53)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.68 (1.67)	mAP 51.02 (45.51)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.67 (1.67)	mAP 48.65 (45.51)
Train: [31/50]	Time 0.331	Loss 1.67 	mAP 45.51
Test: [0/402]	Time 1.804 (1.804)	Precision 77.09 (77.09)	Recall 78.41 (78.41) 	 P_C 29.72 	 R_C 31.12 	 F_C 29.47 	 P_O 77.09 	 R_O 78.41 	 F_O 77.75
Test: [100/402]	Time 0.500 (0.512)	Precision 87.10 (78.87)	Recall 88.24 (83.55) 	 P_C 68.70 	 R_C 69.92 	 F_C 67.64 	 P_O 77.59 	 R_O 82.23 	 F_O 79.84
Test: [200/402]	Time 0.499 (0.506)	Precision 85.04 (80.21)	Recall 86.15 (83.74) 	 P_C 71.25 	 R_C 72.49 	 F_C 70.86 	 P_O 79.20 	 R_O 82.75 	 F_O 80.93
Test: [300/402]	Time 0.500 (0.504)	Precision 85.05 (79.97)	Recall 82.09 (84.59) 	 P_C 74.73 	 R_C 78.50 	 F_C 75.77 	 P_O 78.69 	 R_O 83.62 	 F_O 81.08
Test: [400/402]	Time 0.500 (0.503)	Precision 60.39 (78.86)	Recall 72.67 (84.62) 	 P_C 76.29 	 R_C 80.18 	 F_C 77.60 	 P_O 77.74 	 R_O 83.87 	 F_O 80.69
Test: [31/50]	  P_C 76.29 	 R_C 80.17 	 F_C 77.60 	 P_O 77.74 	 R_O 83.86 	 F_O 80.68 	 mAP 84.41
Train: [0/2566]	Time 0.998 (0.998)	Loss 1.65 (1.65)	mAP 41.90 (41.90)
Train: [100/2566]	Time 0.331 (0.337)	Loss 1.65 (1.66)	mAP 48.57 (44.38)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.57 (1.65)	mAP 44.04 (44.87)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.69 (1.66)	mAP 47.84 (45.24)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.72 (1.65)	mAP 45.88 (45.15)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.39 (1.65)	mAP 49.27 (45.16)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.81 (1.65)	mAP 46.53 (45.16)
Train: [700/2566]	Time 0.331 (0.332)	Loss 1.69 (1.65)	mAP 43.82 (45.28)
Train: [800/2566]	Time 0.331 (0.331)	Loss 2.08 (1.65)	mAP 49.03 (45.35)
Train: [900/2566]	Time 0.330 (0.331)	Loss 2.00 (1.65)	mAP 38.32 (45.37)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 2.01 (1.65)	mAP 44.32 (45.46)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.54 (1.65)	mAP 49.19 (45.51)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.47 (1.65)	mAP 39.89 (45.49)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.66 (1.66)	mAP 36.92 (45.52)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.68 (1.66)	mAP 40.36 (45.51)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.52 (1.66)	mAP 48.12 (45.54)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.26 (1.66)	mAP 33.44 (45.55)
Train: [1700/2566]	Time 0.329 (0.331)	Loss 1.89 (1.66)	mAP 47.43 (45.53)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.97 (1.66)	mAP 45.25 (45.54)
Train: [1900/2566]	Time 0.329 (0.331)	Loss 1.52 (1.67)	mAP 48.77 (45.55)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 2.10 (1.67)	mAP 48.84 (45.54)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.77 (1.67)	mAP 56.44 (45.53)
Train: [2200/2566]	Time 0.329 (0.331)	Loss 1.53 (1.67)	mAP 48.98 (45.51)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.76 (1.67)	mAP 37.22 (45.50)
Train: [2400/2566]	Time 0.329 (0.331)	Loss 1.93 (1.67)	mAP 53.78 (45.51)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.87 (1.67)	mAP 43.67 (45.51)
Train: [32/50]	Time 0.330	Loss 1.67 	mAP 45.52
Test: [0/402]	Time 1.804 (1.804)	Precision 71.11 (71.11)	Recall 80.40 (80.40) 	 P_C 28.41 	 R_C 31.06 	 F_C 28.09 	 P_O 71.11 	 R_O 80.40 	 F_O 75.47
Test: [100/402]	Time 0.500 (0.512)	Precision 87.50 (74.70)	Recall 91.50 (85.67) 	 P_C 64.63 	 R_C 71.96 	 F_C 66.72 	 P_O 72.98 	 R_O 84.70 	 F_O 78.41
Test: [200/402]	Time 0.499 (0.506)	Precision 79.76 (75.77)	Recall 87.01 (85.83) 	 P_C 67.38 	 R_C 74.67 	 F_C 69.94 	 P_O 74.48 	 R_O 85.06 	 F_O 79.42
Test: [300/402]	Time 0.500 (0.504)	Precision 80.86 (75.85)	Recall 84.08 (86.40) 	 P_C 71.58 	 R_C 80.17 	 F_C 74.95 	 P_O 74.34 	 R_O 85.61 	 F_O 79.58
Test: [400/402]	Time 0.500 (0.503)	Precision 54.24 (74.66)	Recall 74.42 (86.52) 	 P_C 74.14 	 R_C 81.64 	 F_C 76.92 	 P_O 73.37 	 R_O 85.92 	 F_O 79.15
Test: [32/50]	  P_C 74.14 	 R_C 81.63 	 F_C 76.92 	 P_O 73.37 	 R_O 85.92 	 F_O 79.15 	 mAP 84.55
Train: [0/2566]	Time 1.065 (1.065)	Loss 1.75 (1.75)	mAP 44.77 (44.77)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.83 (1.67)	mAP 46.34 (45.28)
Train: [200/2566]	Time 0.330 (0.333)	Loss 1.64 (1.67)	mAP 41.86 (45.30)
Train: [300/2566]	Time 0.330 (0.332)	Loss 1.99 (1.67)	mAP 41.96 (45.38)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.81 (1.66)	mAP 50.29 (45.54)
Train: [500/2566]	Time 0.330 (0.331)	Loss 1.91 (1.66)	mAP 50.35 (45.70)
Train: [600/2566]	Time 0.332 (0.331)	Loss 1.55 (1.67)	mAP 46.40 (45.69)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.33 (1.67)	mAP 41.06 (45.75)
Train: [800/2566]	Time 0.331 (0.331)	Loss 1.57 (1.66)	mAP 49.19 (45.67)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.42 (1.66)	mAP 41.02 (45.64)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 2.03 (1.67)	mAP 51.04 (45.64)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.70 (1.66)	mAP 38.03 (45.57)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.42 (1.67)	mAP 40.10 (45.61)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.90 (1.66)	mAP 51.61 (45.61)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.97 (1.66)	mAP 48.52 (45.61)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.92 (1.66)	mAP 53.65 (45.61)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.17 (1.66)	mAP 42.30 (45.61)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.63 (1.66)	mAP 41.63 (45.53)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.77 (1.66)	mAP 52.17 (45.56)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.78 (1.66)	mAP 51.93 (45.56)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 2.00 (1.66)	mAP 49.17 (45.57)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.32 (1.66)	mAP 40.73 (45.56)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.42 (1.66)	mAP 42.67 (45.57)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 2.09 (1.66)	mAP 49.74 (45.53)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.79 (1.66)	mAP 41.61 (45.54)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.49 (1.67)	mAP 54.46 (45.52)
Train: [33/50]	Time 0.331	Loss 1.67 	mAP 45.51
Test: [0/402]	Time 1.857 (1.857)	Precision 77.07 (77.07)	Recall 79.26 (79.26) 	 P_C 28.78 	 R_C 30.15 	 F_C 28.58 	 P_O 77.07 	 R_O 79.26 	 F_O 78.15
Test: [100/402]	Time 0.500 (0.513)	Precision 89.93 (79.84)	Recall 87.58 (82.81) 	 P_C 68.59 	 R_C 68.86 	 F_C 67.20 	 P_O 78.59 	 R_O 81.44 	 F_O 79.99
Test: [200/402]	Time 0.500 (0.506)	Precision 85.78 (80.89)	Recall 86.15 (83.24) 	 P_C 71.30 	 R_C 71.76 	 F_C 70.62 	 P_O 79.91 	 R_O 82.21 	 F_O 81.04
Test: [300/402]	Time 0.500 (0.504)	Precision 86.40 (80.97)	Recall 80.60 (83.81) 	 P_C 75.63 	 R_C 77.31 	 F_C 75.74 	 P_O 79.87 	 R_O 82.73 	 F_O 81.27
Test: [400/402]	Time 0.500 (0.503)	Precision 62.16 (80.23)	Recall 66.86 (83.67) 	 P_C 77.98 	 R_C 78.94 	 F_C 77.69 	 P_O 79.30 	 R_O 82.81 	 F_O 81.02
Test: [33/50]	  P_C 77.97 	 R_C 78.94 	 F_C 77.68 	 P_O 79.30 	 R_O 82.80 	 F_O 81.01 	 mAP 84.62
Train: [0/2566]	Time 1.034 (1.034)	Loss 1.81 (1.81)	mAP 44.74 (44.74)
Train: [100/2566]	Time 0.331 (0.337)	Loss 1.65 (1.65)	mAP 44.08 (45.09)
Train: [200/2566]	Time 0.330 (0.334)	Loss 2.23 (1.68)	mAP 44.09 (44.90)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.77 (1.67)	mAP 49.74 (44.95)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.67 (1.66)	mAP 50.90 (45.11)
Train: [500/2566]	Time 0.329 (0.332)	Loss 1.36 (1.65)	mAP 50.39 (45.30)
Train: [600/2566]	Time 0.330 (0.331)	Loss 1.72 (1.66)	mAP 49.42 (45.35)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.61 (1.66)	mAP 44.41 (45.49)
Train: [800/2566]	Time 0.330 (0.331)	Loss 1.50 (1.66)	mAP 47.02 (45.51)
Train: [900/2566]	Time 0.331 (0.331)	Loss 1.39 (1.66)	mAP 41.92 (45.51)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.28 (1.66)	mAP 48.37 (45.43)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 1.76 (1.65)	mAP 39.91 (45.43)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.58 (1.65)	mAP 47.46 (45.38)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 2.00 (1.66)	mAP 44.46 (45.43)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.83 (1.66)	mAP 46.39 (45.42)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.83 (1.66)	mAP 41.56 (45.44)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 2.03 (1.66)	mAP 47.76 (45.43)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.49 (1.66)	mAP 40.65 (45.47)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.41 (1.66)	mAP 48.78 (45.49)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.61 (1.66)	mAP 40.79 (45.51)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.75 (1.66)	mAP 40.41 (45.48)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.66 (1.66)	mAP 40.14 (45.46)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 2.30 (1.66)	mAP 49.10 (45.45)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.51 (1.66)	mAP 42.92 (45.44)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.97 (1.66)	mAP 43.54 (45.45)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.53 (1.66)	mAP 37.48 (45.45)
Train: [34/50]	Time 0.331	Loss 1.66 	mAP 45.46
Test: [0/402]	Time 1.845 (1.845)	Precision 79.53 (79.53)	Recall 77.27 (77.27) 	 P_C 30.78 	 R_C 30.03 	 F_C 29.15 	 P_O 79.53 	 R_O 77.27 	 F_O 78.39
Test: [100/402]	Time 0.500 (0.513)	Precision 91.72 (81.86)	Recall 86.93 (81.42) 	 P_C 71.51 	 R_C 66.68 	 F_C 67.46 	 P_O 80.93 	 R_O 79.56 	 F_O 80.24
Test: [200/402]	Time 0.500 (0.506)	Precision 85.41 (82.59)	Recall 86.15 (82.16) 	 P_C 74.13 	 R_C 69.79 	 F_C 71.00 	 P_O 81.90 	 R_O 80.79 	 F_O 81.34
Test: [300/402]	Time 0.499 (0.504)	Precision 87.95 (82.94)	Recall 79.85 (82.55) 	 P_C 77.72 	 R_C 75.66 	 F_C 76.09 	 P_O 82.17 	 R_O 81.17 	 F_O 81.67
Test: [400/402]	Time 0.499 (0.503)	Precision 62.78 (82.28)	Recall 65.70 (82.34) 	 P_C 80.05 	 R_C 77.44 	 F_C 78.00 	 P_O 81.68 	 R_O 81.20 	 F_O 81.44
Test: [34/50]	  P_C 80.05 	 R_C 77.44 	 F_C 78.00 	 P_O 81.67 	 R_O 81.19 	 F_O 81.43 	 mAP 84.45
Train: [0/2566]	Time 1.042 (1.042)	Loss 1.77 (1.77)	mAP 45.57 (45.57)
Train: [100/2566]	Time 0.329 (0.337)	Loss 1.14 (1.63)	mAP 42.99 (46.34)
Train: [200/2566]	Time 0.330 (0.333)	Loss 1.57 (1.63)	mAP 41.74 (45.80)
Train: [300/2566]	Time 0.330 (0.332)	Loss 1.47 (1.64)	mAP 45.94 (45.90)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.63 (1.64)	mAP 45.23 (45.73)
Train: [500/2566]	Time 0.330 (0.331)	Loss 1.68 (1.64)	mAP 48.95 (45.55)
Train: [600/2566]	Time 0.330 (0.331)	Loss 1.39 (1.65)	mAP 43.54 (45.43)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.44 (1.64)	mAP 50.71 (45.42)
Train: [800/2566]	Time 0.331 (0.331)	Loss 1.64 (1.65)	mAP 56.44 (45.51)
Train: [900/2566]	Time 0.329 (0.331)	Loss 1.79 (1.65)	mAP 46.01 (45.51)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 1.93 (1.64)	mAP 45.95 (45.51)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.46 (1.64)	mAP 45.37 (45.53)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.94 (1.64)	mAP 44.94 (45.55)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.35 (1.64)	mAP 41.82 (45.53)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.76 (1.65)	mAP 42.09 (45.53)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.74 (1.65)	mAP 50.06 (45.55)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.48 (1.65)	mAP 46.89 (45.56)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.12 (1.65)	mAP 45.52 (45.58)
Train: [1800/2566]	Time 0.329 (0.331)	Loss 1.39 (1.65)	mAP 40.72 (45.59)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.62 (1.65)	mAP 40.64 (45.59)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.51 (1.65)	mAP 45.78 (45.59)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.52 (1.65)	mAP 44.52 (45.58)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.54 (1.65)	mAP 43.31 (45.61)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.23 (1.65)	mAP 43.84 (45.60)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.54 (1.65)	mAP 53.01 (45.58)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.67 (1.65)	mAP 43.68 (45.61)
Train: [35/50]	Time 0.331	Loss 1.66 	mAP 45.63
Test: [0/402]	Time 1.837 (1.837)	Precision 74.13 (74.13)	Recall 78.98 (78.98) 	 P_C 29.13 	 R_C 31.98 	 F_C 29.37 	 P_O 74.13 	 R_O 78.98 	 F_O 76.48
Test: [100/402]	Time 0.499 (0.513)	Precision 89.54 (78.48)	Recall 89.54 (83.98) 	 P_C 68.21 	 R_C 70.64 	 F_C 68.20 	 P_O 77.39 	 R_O 82.53 	 F_O 79.88
Test: [200/402]	Time 0.500 (0.506)	Precision 82.30 (79.05)	Recall 86.58 (84.47) 	 P_C 70.51 	 R_C 73.01 	 F_C 71.10 	 P_O 78.14 	 R_O 83.39 	 F_O 80.68
Test: [300/402]	Time 0.499 (0.504)	Precision 85.31 (78.95)	Recall 82.34 (85.12) 	 P_C 74.56 	 R_C 78.42 	 F_C 75.85 	 P_O 77.97 	 R_O 83.99 	 F_O 80.87
Test: [400/402]	Time 0.499 (0.503)	Precision 63.44 (78.32)	Recall 68.60 (84.94) 	 P_C 76.53 	 R_C 80.04 	 F_C 77.71 	 P_O 77.52 	 R_O 84.06 	 F_O 80.66
Test: [35/50]	  P_C 76.54 	 R_C 80.04 	 F_C 77.72 	 P_O 77.52 	 R_O 84.06 	 F_O 80.66 	 mAP 84.65
Train: [0/2566]	Time 0.989 (0.989)	Loss 1.49 (1.49)	mAP 46.29 (46.29)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.40 (1.62)	mAP 47.34 (45.89)
Train: [200/2566]	Time 0.331 (0.334)	Loss 2.05 (1.63)	mAP 39.48 (45.59)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.27 (1.65)	mAP 47.71 (45.52)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.65 (1.64)	mAP 48.49 (45.65)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.03 (1.65)	mAP 45.03 (45.58)
Train: [600/2566]	Time 0.331 (0.331)	Loss 1.84 (1.65)	mAP 43.06 (45.50)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.57 (1.65)	mAP 42.31 (45.58)
Train: [800/2566]	Time 0.330 (0.331)	Loss 1.25 (1.65)	mAP 51.20 (45.65)
Train: [900/2566]	Time 0.331 (0.331)	Loss 1.50 (1.65)	mAP 38.40 (45.66)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.77 (1.64)	mAP 50.28 (45.65)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.65 (1.64)	mAP 47.76 (45.67)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 2.11 (1.65)	mAP 52.96 (45.69)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 2.45 (1.65)	mAP 45.73 (45.68)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.09 (1.65)	mAP 46.41 (45.73)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.73 (1.65)	mAP 42.18 (45.69)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 2.20 (1.65)	mAP 49.76 (45.72)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.46 (1.65)	mAP 49.85 (45.72)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.80 (1.65)	mAP 43.17 (45.72)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.40 (1.65)	mAP 39.72 (45.73)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.53 (1.65)	mAP 46.94 (45.72)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.70 (1.65)	mAP 43.84 (45.71)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.37 (1.65)	mAP 42.47 (45.69)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.59 (1.65)	mAP 41.53 (45.67)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.37 (1.65)	mAP 50.11 (45.67)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.29 (1.65)	mAP 42.76 (45.65)
Train: [36/50]	Time 0.331	Loss 1.65 	mAP 45.65
Test: [0/402]	Time 1.808 (1.808)	Precision 73.99 (73.99)	Recall 78.41 (78.41) 	 P_C 30.37 	 R_C 31.73 	 F_C 29.52 	 P_O 73.99 	 R_O 78.41 	 F_O 76.14
Test: [100/402]	Time 0.499 (0.512)	Precision 88.96 (77.90)	Recall 89.54 (84.49) 	 P_C 67.30 	 R_C 72.06 	 F_C 68.19 	 P_O 75.97 	 R_O 83.39 	 F_O 79.51
Test: [200/402]	Time 0.500 (0.506)	Precision 84.03 (79.46)	Recall 86.58 (84.46) 	 P_C 69.76 	 R_C 74.37 	 F_C 71.35 	 P_O 78.01 	 R_O 83.59 	 F_O 80.70
Test: [300/402]	Time 0.500 (0.504)	Precision 83.76 (79.37)	Recall 82.09 (85.12) 	 P_C 73.85 	 R_C 79.83 	 F_C 76.15 	 P_O 77.84 	 R_O 84.20 	 F_O 80.90
Test: [400/402]	Time 0.501 (0.503)	Precision 57.80 (78.54)	Recall 73.26 (85.08) 	 P_C 75.67 	 R_C 81.28 	 F_C 77.94 	 P_O 77.16 	 R_O 84.38 	 F_O 80.61
Test: [36/50]	  P_C 75.67 	 R_C 81.28 	 F_C 77.93 	 P_O 77.16 	 R_O 84.38 	 F_O 80.61 	 mAP 84.69
Train: [0/2566]	Time 1.027 (1.027)	Loss 1.81 (1.81)	mAP 47.01 (47.01)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.72 (1.67)	mAP 39.90 (45.75)
Train: [200/2566]	Time 0.331 (0.334)	Loss 2.03 (1.67)	mAP 55.72 (45.59)
Train: [300/2566]	Time 0.330 (0.332)	Loss 1.69 (1.67)	mAP 46.11 (45.50)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.99 (1.65)	mAP 36.47 (45.43)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.51 (1.65)	mAP 49.03 (45.53)
Train: [600/2566]	Time 0.330 (0.331)	Loss 1.40 (1.65)	mAP 40.81 (45.52)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.44 (1.64)	mAP 44.83 (45.57)
Train: [800/2566]	Time 0.330 (0.331)	Loss 1.33 (1.64)	mAP 40.30 (45.52)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.51 (1.64)	mAP 47.43 (45.48)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.28 (1.64)	mAP 45.15 (45.58)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 1.43 (1.63)	mAP 43.33 (45.54)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.41 (1.64)	mAP 42.72 (45.55)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.58 (1.63)	mAP 51.32 (45.53)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.41 (1.64)	mAP 45.68 (45.57)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.53 (1.64)	mAP 48.73 (45.57)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.64 (1.64)	mAP 41.42 (45.56)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.92 (1.64)	mAP 45.16 (45.53)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.37 (1.64)	mAP 49.36 (45.54)
Train: [1900/2566]	Time 0.329 (0.331)	Loss 1.71 (1.64)	mAP 44.64 (45.56)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.48 (1.64)	mAP 39.18 (45.53)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 2.10 (1.64)	mAP 35.95 (45.53)
Train: [2200/2566]	Time 0.330 (0.330)	Loss 1.19 (1.64)	mAP 46.98 (45.56)
Train: [2300/2566]	Time 0.330 (0.330)	Loss 1.46 (1.64)	mAP 43.73 (45.59)
Train: [2400/2566]	Time 0.329 (0.330)	Loss 1.38 (1.64)	mAP 47.22 (45.61)
Train: [2500/2566]	Time 0.329 (0.330)	Loss 2.29 (1.64)	mAP 39.96 (45.58)
Train: [37/50]	Time 0.330	Loss 1.64 	mAP 45.59
Test: [0/402]	Time 1.826 (1.826)	Precision 76.42 (76.42)	Recall 80.11 (80.11) 	 P_C 31.15 	 R_C 32.45 	 F_C 30.16 	 P_O 76.42 	 R_O 80.11 	 F_O 78.22
Test: [100/402]	Time 0.500 (0.513)	Precision 90.13 (78.22)	Recall 89.54 (84.16) 	 P_C 67.61 	 R_C 70.65 	 F_C 67.69 	 P_O 76.69 	 R_O 82.89 	 F_O 79.67
Test: [200/402]	Time 0.500 (0.506)	Precision 82.86 (79.48)	Recall 87.88 (84.34) 	 P_C 70.28 	 R_C 73.34 	 F_C 71.11 	 P_O 78.31 	 R_O 83.38 	 F_O 80.77
Test: [300/402]	Time 0.500 (0.504)	Precision 84.69 (79.76)	Recall 82.59 (84.91) 	 P_C 74.96 	 R_C 78.57 	 F_C 76.19 	 P_O 78.49 	 R_O 83.91 	 F_O 81.11
Test: [400/402]	Time 0.501 (0.503)	Precision 62.69 (78.74)	Recall 70.35 (84.95) 	 P_C 77.20 	 R_C 80.17 	 F_C 77.86 	 P_O 77.64 	 R_O 84.20 	 F_O 80.79
Test: [37/50]	  P_C 77.20 	 R_C 80.16 	 F_C 77.86 	 P_O 77.63 	 R_O 84.19 	 F_O 80.78 	 mAP 84.73
Train: [0/2566]	Time 0.989 (0.989)	Loss 1.77 (1.77)	mAP 49.45 (49.45)
Train: [100/2566]	Time 0.330 (0.337)	Loss 2.23 (1.68)	mAP 50.47 (46.34)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.92 (1.66)	mAP 51.07 (45.92)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.86 (1.65)	mAP 44.15 (46.01)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.56 (1.65)	mAP 44.37 (46.00)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.61 (1.65)	mAP 50.46 (45.84)
Train: [600/2566]	Time 0.330 (0.332)	Loss 2.05 (1.65)	mAP 47.09 (45.76)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.33 (1.64)	mAP 50.29 (45.71)
Train: [800/2566]	Time 0.331 (0.331)	Loss 1.98 (1.64)	mAP 38.58 (45.71)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.68 (1.64)	mAP 38.52 (45.74)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.63 (1.64)	mAP 51.44 (45.74)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.55 (1.64)	mAP 48.05 (45.74)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.40 (1.64)	mAP 51.21 (45.76)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.40 (1.64)	mAP 45.94 (45.82)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.88 (1.63)	mAP 50.16 (45.81)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.92 (1.64)	mAP 44.64 (45.78)
Train: [1600/2566]	Time 0.329 (0.331)	Loss 1.54 (1.64)	mAP 47.69 (45.81)
Train: [1700/2566]	Time 0.329 (0.331)	Loss 0.89 (1.64)	mAP 44.43 (45.81)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.64 (1.64)	mAP 48.31 (45.78)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.87 (1.64)	mAP 43.36 (45.78)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.63 (1.65)	mAP 40.54 (45.76)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.60 (1.64)	mAP 48.93 (45.75)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.64 (1.65)	mAP 46.97 (45.72)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.85 (1.65)	mAP 51.12 (45.70)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.45 (1.64)	mAP 39.56 (45.71)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.48 (1.64)	mAP 46.02 (45.72)
Train: [38/50]	Time 0.331	Loss 1.64 	mAP 45.69
Test: [0/402]	Time 1.791 (1.791)	Precision 75.96 (75.96)	Recall 78.98 (78.98) 	 P_C 29.76 	 R_C 31.62 	 F_C 29.63 	 P_O 75.96 	 R_O 78.98 	 F_O 77.44
Test: [100/402]	Time 0.500 (0.512)	Precision 90.26 (79.40)	Recall 90.85 (83.72) 	 P_C 67.45 	 R_C 70.59 	 F_C 67.77 	 P_O 77.92 	 R_O 82.37 	 F_O 80.08
Test: [200/402]	Time 0.500 (0.506)	Precision 84.39 (79.77)	Recall 86.58 (84.38) 	 P_C 69.97 	 R_C 73.33 	 F_C 70.99 	 P_O 78.68 	 R_O 83.35 	 F_O 80.95
Test: [300/402]	Time 0.499 (0.504)	Precision 84.63 (79.70)	Recall 83.58 (84.98) 	 P_C 74.70 	 R_C 78.65 	 F_C 76.11 	 P_O 78.54 	 R_O 83.95 	 F_O 81.15
Test: [400/402]	Time 0.500 (0.503)	Precision 60.51 (78.79)	Recall 68.60 (84.92) 	 P_C 76.82 	 R_C 80.27 	 F_C 77.97 	 P_O 77.83 	 R_O 84.09 	 F_O 80.84
Test: [38/50]	  P_C 76.82 	 R_C 80.26 	 F_C 77.97 	 P_O 77.83 	 R_O 84.09 	 F_O 80.84 	 mAP 84.83
Train: [0/2566]	Time 1.009 (1.009)	Loss 1.41 (1.41)	mAP 39.49 (39.49)
Train: [100/2566]	Time 0.329 (0.337)	Loss 1.77 (1.59)	mAP 52.80 (45.43)
Train: [200/2566]	Time 0.331 (0.333)	Loss 2.11 (1.61)	mAP 45.92 (45.77)
Train: [300/2566]	Time 0.330 (0.332)	Loss 1.75 (1.62)	mAP 48.69 (45.80)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.46 (1.63)	mAP 46.61 (45.73)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.36 (1.63)	mAP 46.96 (45.78)
Train: [600/2566]	Time 0.330 (0.331)	Loss 1.74 (1.64)	mAP 44.05 (45.81)
Train: [700/2566]	Time 0.331 (0.331)	Loss 0.88 (1.63)	mAP 43.44 (45.92)
Train: [800/2566]	Time 0.330 (0.331)	Loss 1.55 (1.63)	mAP 46.54 (45.79)
Train: [900/2566]	Time 0.331 (0.331)	Loss 1.49 (1.63)	mAP 45.24 (45.76)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 1.67 (1.63)	mAP 41.82 (45.81)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 1.76 (1.63)	mAP 40.65 (45.79)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.54 (1.63)	mAP 53.14 (45.77)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.56 (1.63)	mAP 45.03 (45.83)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.40 (1.62)	mAP 46.86 (45.82)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 2.07 (1.63)	mAP 51.97 (45.76)
Train: [1600/2566]	Time 0.329 (0.331)	Loss 1.22 (1.63)	mAP 47.52 (45.73)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.91 (1.63)	mAP 46.89 (45.68)
Train: [1800/2566]	Time 0.329 (0.331)	Loss 1.37 (1.63)	mAP 47.51 (45.69)
Train: [1900/2566]	Time 0.329 (0.331)	Loss 1.91 (1.63)	mAP 35.49 (45.72)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 2.08 (1.63)	mAP 47.68 (45.72)
Train: [2100/2566]	Time 0.329 (0.331)	Loss 1.73 (1.64)	mAP 45.42 (45.72)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.53 (1.64)	mAP 48.42 (45.72)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.47 (1.64)	mAP 45.89 (45.72)
Train: [2400/2566]	Time 0.331 (0.330)	Loss 1.68 (1.64)	mAP 47.50 (45.72)
Train: [2500/2566]	Time 0.330 (0.330)	Loss 1.57 (1.64)	mAP 39.69 (45.70)
Train: [39/50]	Time 0.330	Loss 1.64 	mAP 45.67
Test: [0/402]	Time 1.797 (1.797)	Precision 73.75 (73.75)	Recall 79.83 (79.83) 	 P_C 29.59 	 R_C 32.29 	 F_C 29.34 	 P_O 73.75 	 R_O 79.83 	 F_O 76.67
Test: [100/402]	Time 0.500 (0.512)	Precision 87.10 (78.15)	Recall 88.24 (84.56) 	 P_C 67.62 	 R_C 71.28 	 F_C 67.86 	 P_O 76.20 	 R_O 83.47 	 F_O 79.67
Test: [200/402]	Time 0.500 (0.506)	Precision 84.52 (79.06)	Recall 87.45 (84.66) 	 P_C 69.78 	 R_C 73.87 	 F_C 70.76 	 P_O 77.57 	 R_O 83.84 	 F_O 80.58
Test: [300/402]	Time 0.500 (0.504)	Precision 82.56 (78.84)	Recall 83.58 (85.45) 	 P_C 74.12 	 R_C 79.39 	 F_C 75.87 	 P_O 77.19 	 R_O 84.64 	 F_O 80.74
Test: [400/402]	Time 0.500 (0.503)	Precision 58.41 (77.57)	Recall 72.67 (85.61) 	 P_C 75.96 	 R_C 81.12 	 F_C 77.77 	 P_O 76.10 	 R_O 85.02 	 F_O 80.31
Test: [39/50]	  P_C 75.96 	 R_C 81.11 	 F_C 77.76 	 P_O 76.10 	 R_O 85.01 	 F_O 80.31 	 mAP 84.87
Train: [0/2566]	Time 1.041 (1.041)	Loss 1.82 (1.82)	mAP 47.06 (47.06)
Train: [100/2566]	Time 0.331 (0.337)	Loss 1.62 (1.64)	mAP 50.09 (45.86)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.49 (1.64)	mAP 38.56 (45.45)
Train: [300/2566]	Time 0.330 (0.333)	Loss 0.96 (1.64)	mAP 43.84 (45.67)
Train: [400/2566]	Time 0.332 (0.332)	Loss 1.57 (1.63)	mAP 44.43 (45.67)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.48 (1.63)	mAP 40.78 (45.73)
Train: [600/2566]	Time 0.331 (0.331)	Loss 2.48 (1.64)	mAP 38.96 (45.74)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.78 (1.64)	mAP 48.45 (45.69)
Train: [800/2566]	Time 0.331 (0.331)	Loss 1.38 (1.64)	mAP 49.05 (45.73)
Train: [900/2566]	Time 0.329 (0.331)	Loss 1.97 (1.64)	mAP 49.00 (45.71)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.78 (1.63)	mAP 44.70 (45.72)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.37 (1.63)	mAP 53.12 (45.73)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.56 (1.63)	mAP 37.18 (45.67)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.68 (1.63)	mAP 52.68 (45.64)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.96 (1.63)	mAP 49.26 (45.65)
Train: [1500/2566]	Time 0.329 (0.331)	Loss 1.75 (1.63)	mAP 49.97 (45.64)
Train: [1600/2566]	Time 0.329 (0.331)	Loss 1.63 (1.63)	mAP 44.76 (45.61)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.38 (1.63)	mAP 49.60 (45.60)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.87 (1.63)	mAP 48.09 (45.65)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.46 (1.63)	mAP 50.96 (45.64)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.95 (1.63)	mAP 42.74 (45.62)
Train: [2100/2566]	Time 0.329 (0.331)	Loss 1.44 (1.63)	mAP 38.27 (45.61)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.56 (1.63)	mAP 41.60 (45.62)
Train: [2300/2566]	Time 0.332 (0.331)	Loss 1.64 (1.63)	mAP 44.32 (45.64)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.64 (1.63)	mAP 44.70 (45.63)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 2.01 (1.63)	mAP 40.58 (45.63)
Train: [40/50]	Time 0.331	Loss 1.63 	mAP 45.65
Test: [0/402]	Time 1.819 (1.819)	Precision 73.56 (73.56)	Recall 79.83 (79.83) 	 P_C 30.74 	 R_C 33.91 	 F_C 30.36 	 P_O 73.56 	 R_O 79.83 	 F_O 76.57
Test: [100/402]	Time 0.500 (0.513)	Precision 87.34 (78.21)	Recall 90.20 (84.56) 	 P_C 66.27 	 R_C 71.77 	 F_C 67.65 	 P_O 76.24 	 R_O 83.52 	 F_O 79.71
Test: [200/402]	Time 0.499 (0.506)	Precision 82.93 (79.07)	Recall 88.31 (84.81) 	 P_C 68.45 	 R_C 74.85 	 F_C 70.73 	 P_O 77.69 	 R_O 83.98 	 F_O 80.71
Test: [300/402]	Time 0.499 (0.504)	Precision 82.35 (79.03)	Recall 83.58 (85.37) 	 P_C 73.37 	 R_C 79.98 	 F_C 75.90 	 P_O 77.52 	 R_O 84.52 	 F_O 80.87
Test: [400/402]	Time 0.499 (0.503)	Precision 58.53 (77.90)	Recall 73.84 (85.50) 	 P_C 75.46 	 R_C 81.46 	 F_C 77.89 	 P_O 76.55 	 R_O 84.86 	 F_O 80.49
Test: [40/50]	  P_C 75.45 	 R_C 81.45 	 F_C 77.88 	 P_O 76.55 	 R_O 84.86 	 F_O 80.49 	 mAP 84.80
Train: [0/2566]	Time 1.046 (1.046)	Loss 1.69 (1.69)	mAP 36.32 (36.32)
Train: [100/2566]	Time 0.329 (0.337)	Loss 1.99 (1.61)	mAP 46.97 (45.52)
Train: [200/2566]	Time 0.330 (0.334)	Loss 2.07 (1.62)	mAP 46.10 (45.50)
Train: [300/2566]	Time 0.330 (0.332)	Loss 1.47 (1.62)	mAP 47.38 (45.89)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.22 (1.63)	mAP 47.88 (45.89)
Train: [500/2566]	Time 0.331 (0.331)	Loss 1.79 (1.63)	mAP 52.15 (45.92)
Train: [600/2566]	Time 0.330 (0.331)	Loss 1.34 (1.63)	mAP 38.96 (45.79)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.93 (1.63)	mAP 45.59 (45.70)
Train: [800/2566]	Time 0.331 (0.331)	Loss 2.05 (1.63)	mAP 49.07 (45.68)
Train: [900/2566]	Time 0.331 (0.331)	Loss 1.57 (1.64)	mAP 47.81 (45.68)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.21 (1.64)	mAP 46.06 (45.66)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.44 (1.64)	mAP 43.59 (45.67)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.86 (1.65)	mAP 49.24 (45.65)
Train: [1300/2566]	Time 0.332 (0.331)	Loss 1.83 (1.64)	mAP 34.97 (45.60)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.93 (1.64)	mAP 48.77 (45.60)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.60 (1.64)	mAP 50.45 (45.64)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.84 (1.64)	mAP 39.41 (45.62)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.77 (1.64)	mAP 43.84 (45.60)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 2.16 (1.64)	mAP 46.95 (45.61)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.88 (1.64)	mAP 44.88 (45.60)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.60 (1.64)	mAP 46.25 (45.61)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.70 (1.64)	mAP 44.13 (45.61)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.79 (1.63)	mAP 40.53 (45.63)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.95 (1.64)	mAP 42.85 (45.62)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.67 (1.63)	mAP 49.00 (45.64)
Train: [2500/2566]	Time 0.330 (0.331)	Loss 1.47 (1.63)	mAP 44.39 (45.68)
Train: [41/50]	Time 0.331	Loss 1.63 	mAP 45.69
Test: [0/402]	Time 1.792 (1.792)	Precision 71.87 (71.87)	Recall 79.83 (79.83) 	 P_C 27.35 	 R_C 32.20 	 F_C 28.32 	 P_O 71.87 	 R_O 79.83 	 F_O 75.64
Test: [100/402]	Time 0.499 (0.512)	Precision 89.10 (76.95)	Recall 90.85 (85.25) 	 P_C 66.65 	 R_C 71.94 	 F_C 67.99 	 P_O 75.38 	 R_O 84.08 	 F_O 79.49
Test: [200/402]	Time 0.499 (0.506)	Precision 83.27 (77.94)	Recall 88.31 (85.41) 	 P_C 68.96 	 R_C 74.63 	 F_C 70.98 	 P_O 76.76 	 R_O 84.54 	 F_O 80.46
Test: [300/402]	Time 0.499 (0.504)	Precision 82.37 (78.15)	Recall 84.83 (85.84) 	 P_C 73.37 	 R_C 80.00 	 F_C 76.04 	 P_O 76.89 	 R_O 84.92 	 F_O 80.71
Test: [400/402]	Time 0.500 (0.503)	Precision 60.48 (77.15)	Recall 73.84 (85.89) 	 P_C 75.28 	 R_C 81.55 	 F_C 77.96 	 P_O 76.10 	 R_O 85.19 	 F_O 80.39
Test: [41/50]	  P_C 75.28 	 R_C 81.55 	 F_C 77.95 	 P_O 76.10 	 R_O 85.19 	 F_O 80.39 	 mAP 84.89
Train: [0/2566]	Time 1.008 (1.008)	Loss 1.85 (1.85)	mAP 42.45 (42.45)
Train: [100/2566]	Time 0.331 (0.337)	Loss 2.00 (1.66)	mAP 52.05 (45.68)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.66 (1.65)	mAP 44.08 (45.70)
Train: [300/2566]	Time 0.333 (0.332)	Loss 1.50 (1.65)	mAP 39.75 (45.56)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.52 (1.65)	mAP 46.52 (45.38)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.54 (1.64)	mAP 51.48 (45.42)
Train: [600/2566]	Time 0.330 (0.331)	Loss 1.55 (1.64)	mAP 44.39 (45.53)
Train: [700/2566]	Time 0.332 (0.331)	Loss 1.35 (1.64)	mAP 39.74 (45.54)
Train: [800/2566]	Time 0.330 (0.331)	Loss 1.36 (1.64)	mAP 49.45 (45.66)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.81 (1.64)	mAP 53.19 (45.71)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.41 (1.63)	mAP 41.21 (45.79)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.44 (1.63)	mAP 45.48 (45.85)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.58 (1.63)	mAP 47.46 (45.81)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.50 (1.62)	mAP 37.59 (45.76)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.81 (1.63)	mAP 38.73 (45.76)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.75 (1.63)	mAP 49.05 (45.71)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.50 (1.63)	mAP 42.27 (45.70)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.50 (1.63)	mAP 46.52 (45.66)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.71 (1.63)	mAP 44.81 (45.68)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.61 (1.63)	mAP 52.33 (45.64)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.81 (1.63)	mAP 48.26 (45.63)
Train: [2100/2566]	Time 0.329 (0.331)	Loss 1.34 (1.63)	mAP 49.48 (45.63)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.30 (1.63)	mAP 50.79 (45.64)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.67 (1.62)	mAP 46.11 (45.67)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.74 (1.62)	mAP 48.05 (45.72)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.42 (1.62)	mAP 47.53 (45.70)
Train: [42/50]	Time 0.330	Loss 1.62 	mAP 45.69
Test: [0/402]	Time 1.769 (1.769)	Precision 75.14 (75.14)	Recall 78.98 (78.98) 	 P_C 30.30 	 R_C 32.40 	 F_C 29.90 	 P_O 75.14 	 R_O 78.98 	 F_O 77.01
Test: [100/402]	Time 0.500 (0.512)	Precision 88.54 (78.61)	Recall 90.85 (84.29) 	 P_C 66.79 	 R_C 71.75 	 F_C 68.09 	 P_O 77.00 	 R_O 83.13 	 F_O 79.95
Test: [200/402]	Time 0.500 (0.506)	Precision 83.68 (79.39)	Recall 86.58 (84.67) 	 P_C 69.11 	 R_C 74.50 	 F_C 71.09 	 P_O 78.24 	 R_O 83.76 	 F_O 80.91
Test: [300/402]	Time 0.500 (0.504)	Precision 84.21 (79.45)	Recall 83.58 (85.35) 	 P_C 74.13 	 R_C 79.92 	 F_C 76.43 	 P_O 78.19 	 R_O 84.41 	 F_O 81.18
Test: [400/402]	Time 0.500 (0.503)	Precision 61.88 (78.44)	Recall 72.67 (85.42) 	 P_C 76.03 	 R_C 81.36 	 F_C 78.28 	 P_O 77.34 	 R_O 84.71 	 F_O 80.86
Test: [42/50]	  P_C 76.03 	 R_C 81.36 	 F_C 78.28 	 P_O 77.34 	 R_O 84.71 	 F_O 80.86 	 mAP 84.91
Train: [0/2566]	Time 1.014 (1.014)	Loss 1.52 (1.52)	mAP 43.33 (43.33)
Train: [100/2566]	Time 0.331 (0.337)	Loss 1.56 (1.61)	mAP 40.85 (46.19)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.58 (1.61)	mAP 44.02 (45.73)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.62 (1.61)	mAP 46.81 (45.81)
Train: [400/2566]	Time 0.331 (0.332)	Loss 2.12 (1.62)	mAP 43.68 (45.63)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.48 (1.61)	mAP 42.06 (45.48)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.67 (1.61)	mAP 42.98 (45.57)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.27 (1.60)	mAP 47.10 (45.51)
Train: [800/2566]	Time 0.330 (0.331)	Loss 1.36 (1.61)	mAP 51.20 (45.52)
Train: [900/2566]	Time 0.331 (0.331)	Loss 1.47 (1.61)	mAP 51.88 (45.58)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.12 (1.60)	mAP 46.54 (45.60)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 1.41 (1.61)	mAP 38.38 (45.68)
Train: [1200/2566]	Time 0.332 (0.331)	Loss 1.26 (1.60)	mAP 42.96 (45.67)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 2.24 (1.61)	mAP 48.50 (45.64)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.11 (1.61)	mAP 46.59 (45.62)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.66 (1.61)	mAP 44.89 (45.66)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.69 (1.61)	mAP 35.50 (45.68)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 2.07 (1.61)	mAP 42.98 (45.70)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.79 (1.61)	mAP 46.03 (45.70)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.94 (1.61)	mAP 49.10 (45.67)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.70 (1.61)	mAP 47.28 (45.71)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 2.24 (1.62)	mAP 43.07 (45.74)
Train: [2200/2566]	Time 0.329 (0.331)	Loss 1.47 (1.62)	mAP 42.38 (45.74)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.93 (1.62)	mAP 51.06 (45.76)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.54 (1.62)	mAP 43.95 (45.73)
Train: [2500/2566]	Time 0.329 (0.331)	Loss 1.75 (1.62)	mAP 55.85 (45.76)
Train: [43/50]	Time 0.331	Loss 1.62 	mAP 45.77
Test: [0/402]	Time 1.767 (1.767)	Precision 76.80 (76.80)	Recall 78.98 (78.98) 	 P_C 29.86 	 R_C 31.79 	 F_C 29.67 	 P_O 76.80 	 R_O 78.98 	 F_O 77.87
Test: [100/402]	Time 0.500 (0.512)	Precision 89.61 (79.00)	Recall 90.20 (84.14) 	 P_C 68.43 	 R_C 71.00 	 F_C 68.29 	 P_O 77.41 	 R_O 82.95 	 F_O 80.09
Test: [200/402]	Time 0.500 (0.506)	Precision 83.95 (79.89)	Recall 88.31 (84.53) 	 P_C 71.02 	 R_C 73.53 	 F_C 71.46 	 P_O 78.74 	 R_O 83.61 	 F_O 81.10
Test: [300/402]	Time 0.500 (0.504)	Precision 83.67 (80.07)	Recall 82.84 (85.06) 	 P_C 75.50 	 R_C 78.93 	 F_C 76.62 	 P_O 78.79 	 R_O 84.10 	 F_O 81.36
Test: [400/402]	Time 0.500 (0.503)	Precision 60.30 (79.19)	Recall 69.77 (85.07) 	 P_C 77.22 	 R_C 80.61 	 F_C 78.46 	 P_O 78.09 	 R_O 84.33 	 F_O 81.09
Test: [43/50]	  P_C 77.21 	 R_C 80.60 	 F_C 78.45 	 P_O 78.08 	 R_O 84.33 	 F_O 81.09 	 mAP 85.03
Train: [0/2566]	Time 1.050 (1.050)	Loss 1.26 (1.26)	mAP 47.84 (47.84)
Train: [100/2566]	Time 0.331 (0.337)	Loss 1.54 (1.61)	mAP 59.86 (46.89)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.91 (1.62)	mAP 46.20 (46.03)
Train: [300/2566]	Time 0.330 (0.333)	Loss 1.58 (1.61)	mAP 47.09 (45.92)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.41 (1.61)	mAP 41.89 (45.74)
Train: [500/2566]	Time 0.331 (0.332)	Loss 1.70 (1.62)	mAP 49.45 (45.81)
Train: [600/2566]	Time 0.329 (0.332)	Loss 1.75 (1.62)	mAP 50.59 (45.87)
Train: [700/2566]	Time 0.330 (0.331)	Loss 2.17 (1.62)	mAP 47.18 (45.91)
Train: [800/2566]	Time 0.329 (0.331)	Loss 1.27 (1.62)	mAP 51.46 (45.93)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.85 (1.62)	mAP 42.62 (45.88)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 1.45 (1.61)	mAP 39.32 (45.87)
Train: [1100/2566]	Time 0.331 (0.331)	Loss 1.74 (1.62)	mAP 48.29 (45.89)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.40 (1.62)	mAP 43.50 (45.90)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.77 (1.62)	mAP 45.00 (45.85)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.19 (1.62)	mAP 40.59 (45.83)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.73 (1.61)	mAP 54.23 (45.81)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.64 (1.61)	mAP 47.81 (45.84)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.71 (1.61)	mAP 40.75 (45.79)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.19 (1.61)	mAP 42.79 (45.77)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.74 (1.62)	mAP 43.42 (45.78)
Train: [2000/2566]	Time 0.329 (0.331)	Loss 1.37 (1.62)	mAP 42.81 (45.79)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.95 (1.62)	mAP 45.57 (45.77)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.72 (1.62)	mAP 41.49 (45.78)
Train: [2300/2566]	Time 0.330 (0.331)	Loss 1.41 (1.62)	mAP 39.12 (45.77)
Train: [2400/2566]	Time 0.331 (0.330)	Loss 1.69 (1.62)	mAP 48.59 (45.76)
Train: [2500/2566]	Time 0.330 (0.330)	Loss 1.90 (1.62)	mAP 49.20 (45.75)
Train: [44/50]	Time 0.330	Loss 1.62 	mAP 45.74
Test: [0/402]	Time 1.772 (1.772)	Precision 73.81 (73.81)	Recall 79.26 (79.26) 	 P_C 28.48 	 R_C 31.80 	 F_C 28.95 	 P_O 73.81 	 R_O 79.26 	 F_O 76.44
Test: [100/402]	Time 0.499 (0.512)	Precision 87.90 (77.31)	Recall 90.20 (85.08) 	 P_C 66.75 	 R_C 71.94 	 F_C 67.99 	 P_O 75.79 	 R_O 83.86 	 F_O 79.62
Test: [200/402]	Time 0.500 (0.506)	Precision 83.27 (78.16)	Recall 88.31 (85.41) 	 P_C 69.14 	 R_C 74.48 	 F_C 71.06 	 P_O 77.00 	 R_O 84.47 	 F_O 80.56
Test: [300/402]	Time 0.500 (0.504)	Precision 82.72 (78.35)	Recall 83.33 (85.96) 	 P_C 73.82 	 R_C 80.02 	 F_C 76.30 	 P_O 77.16 	 R_O 84.98 	 F_O 80.88
Test: [400/402]	Time 0.500 (0.503)	Precision 60.09 (77.50)	Recall 74.42 (85.91) 	 P_C 75.59 	 R_C 81.56 	 F_C 78.08 	 P_O 76.52 	 R_O 85.14 	 F_O 80.60
Test: [44/50]	  P_C 75.58 	 R_C 81.56 	 F_C 78.07 	 P_O 76.52 	 R_O 85.14 	 F_O 80.60 	 mAP 85.03
Train: [0/2566]	Time 1.025 (1.025)	Loss 1.23 (1.23)	mAP 53.39 (53.39)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.56 (1.60)	mAP 44.06 (46.56)
Train: [200/2566]	Time 0.330 (0.333)	Loss 1.45 (1.61)	mAP 43.74 (46.27)
Train: [300/2566]	Time 0.331 (0.332)	Loss 2.34 (1.60)	mAP 46.60 (46.38)
Train: [400/2566]	Time 0.331 (0.332)	Loss 1.72 (1.60)	mAP 35.55 (46.18)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.35 (1.62)	mAP 42.26 (46.07)
Train: [600/2566]	Time 0.330 (0.331)	Loss 1.66 (1.62)	mAP 51.53 (45.92)
Train: [700/2566]	Time 0.332 (0.331)	Loss 1.76 (1.61)	mAP 44.58 (45.88)
Train: [800/2566]	Time 0.331 (0.331)	Loss 1.73 (1.61)	mAP 50.22 (45.88)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.66 (1.62)	mAP 40.38 (45.90)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.14 (1.62)	mAP 51.06 (45.89)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.84 (1.62)	mAP 49.01 (45.82)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 2.07 (1.62)	mAP 42.20 (45.88)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.10 (1.62)	mAP 41.64 (45.91)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.66 (1.62)	mAP 42.50 (45.88)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.43 (1.62)	mAP 46.38 (45.91)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.20 (1.62)	mAP 50.17 (45.88)
Train: [1700/2566]	Time 0.331 (0.331)	Loss 1.76 (1.62)	mAP 43.31 (45.89)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.57 (1.62)	mAP 53.85 (45.90)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.33 (1.62)	mAP 47.86 (45.90)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.56 (1.62)	mAP 37.62 (45.88)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.24 (1.62)	mAP 42.40 (45.85)
Train: [2200/2566]	Time 0.332 (0.331)	Loss 1.47 (1.61)	mAP 45.83 (45.85)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.66 (1.61)	mAP 40.27 (45.84)
Train: [2400/2566]	Time 0.331 (0.331)	Loss 1.73 (1.62)	mAP 40.39 (45.84)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 0.87 (1.62)	mAP 50.20 (45.83)
Train: [45/50]	Time 0.331	Loss 1.62 	mAP 45.82
Test: [0/402]	Time 1.822 (1.822)	Precision 75.20 (75.20)	Recall 79.26 (79.26) 	 P_C 29.36 	 R_C 31.80 	 F_C 29.36 	 P_O 75.20 	 R_O 79.26 	 F_O 77.18
Test: [100/402]	Time 0.499 (0.513)	Precision 89.61 (78.50)	Recall 90.20 (84.37) 	 P_C 67.47 	 R_C 71.25 	 F_C 68.12 	 P_O 77.14 	 R_O 83.05 	 F_O 79.99
Test: [200/402]	Time 0.499 (0.506)	Precision 84.17 (79.25)	Recall 87.45 (84.84) 	 P_C 70.12 	 R_C 73.79 	 F_C 71.29 	 P_O 78.20 	 R_O 83.82 	 F_O 80.91
Test: [300/402]	Time 0.500 (0.504)	Precision 84.09 (79.57)	Recall 82.84 (85.31) 	 P_C 75.02 	 R_C 79.11 	 F_C 76.51 	 P_O 78.46 	 R_O 84.25 	 F_O 81.25
Test: [400/402]	Time 0.500 (0.503)	Precision 60.59 (78.76)	Recall 71.51 (85.25) 	 P_C 76.82 	 R_C 80.77 	 F_C 78.35 	 P_O 77.83 	 R_O 84.42 	 F_O 80.99
Test: [45/50]	  P_C 76.82 	 R_C 80.77 	 F_C 78.35 	 P_O 77.83 	 R_O 84.42 	 F_O 80.99 	 mAP 85.03
Train: [0/2566]	Time 1.003 (1.003)	Loss 1.46 (1.46)	mAP 44.69 (44.69)
Train: [100/2566]	Time 0.329 (0.337)	Loss 1.82 (1.66)	mAP 47.45 (45.66)
Train: [200/2566]	Time 0.329 (0.333)	Loss 1.37 (1.62)	mAP 46.44 (45.80)
Train: [300/2566]	Time 0.330 (0.332)	Loss 1.45 (1.62)	mAP 47.19 (45.86)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.47 (1.61)	mAP 41.77 (45.56)
Train: [500/2566]	Time 0.330 (0.331)	Loss 1.72 (1.61)	mAP 45.44 (45.64)
Train: [600/2566]	Time 0.330 (0.331)	Loss 1.63 (1.62)	mAP 46.67 (45.71)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.32 (1.62)	mAP 44.02 (45.81)
Train: [800/2566]	Time 0.331 (0.331)	Loss 1.83 (1.62)	mAP 50.41 (45.83)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.99 (1.62)	mAP 49.75 (45.80)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.73 (1.61)	mAP 49.13 (45.72)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.69 (1.62)	mAP 51.20 (45.84)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.54 (1.61)	mAP 48.87 (45.82)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.54 (1.61)	mAP 56.53 (45.78)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.43 (1.61)	mAP 48.08 (45.79)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 2.19 (1.61)	mAP 44.45 (45.78)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.74 (1.61)	mAP 44.94 (45.76)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.62 (1.61)	mAP 52.87 (45.78)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.67 (1.61)	mAP 47.05 (45.78)
Train: [1900/2566]	Time 0.331 (0.331)	Loss 1.87 (1.61)	mAP 46.84 (45.81)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.16 (1.61)	mAP 49.95 (45.77)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.44 (1.61)	mAP 46.07 (45.78)
Train: [2200/2566]	Time 0.330 (0.331)	Loss 1.31 (1.61)	mAP 47.69 (45.75)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.43 (1.61)	mAP 57.55 (45.76)
Train: [2400/2566]	Time 0.331 (0.330)	Loss 2.18 (1.61)	mAP 39.19 (45.76)
Train: [2500/2566]	Time 0.330 (0.330)	Loss 2.06 (1.61)	mAP 50.46 (45.77)
Train: [46/50]	Time 0.330	Loss 1.61 	mAP 45.78
Test: [0/402]	Time 1.819 (1.819)	Precision 73.56 (73.56)	Recall 79.83 (79.83) 	 P_C 29.73 	 R_C 32.87 	 F_C 29.72 	 P_O 73.56 	 R_O 79.83 	 F_O 76.57
Test: [100/402]	Time 0.499 (0.513)	Precision 87.97 (77.76)	Recall 90.85 (84.83) 	 P_C 66.84 	 R_C 71.98 	 F_C 68.14 	 P_O 76.14 	 R_O 83.71 	 F_O 79.75
Test: [200/402]	Time 0.500 (0.506)	Precision 83.47 (78.91)	Recall 87.45 (85.10) 	 P_C 69.12 	 R_C 74.51 	 F_C 71.06 	 P_O 77.65 	 R_O 84.24 	 F_O 80.81
Test: [300/402]	Time 0.500 (0.504)	Precision 83.70 (78.97)	Recall 84.33 (85.74) 	 P_C 74.35 	 R_C 79.87 	 F_C 76.45 	 P_O 77.59 	 R_O 84.84 	 F_O 81.05
Test: [400/402]	Time 0.500 (0.503)	Precision 59.62 (78.01)	Recall 72.09 (85.75) 	 P_C 76.20 	 R_C 81.41 	 F_C 78.27 	 P_O 76.83 	 R_O 85.08 	 F_O 80.75
Test: [46/50]	  P_C 76.19 	 R_C 81.41 	 F_C 78.27 	 P_O 76.83 	 R_O 85.08 	 F_O 80.74 	 mAP 85.04
Train: [0/2566]	Time 0.988 (0.988)	Loss 1.52 (1.52)	mAP 40.36 (40.36)
Train: [100/2566]	Time 0.330 (0.337)	Loss 1.41 (1.58)	mAP 56.35 (45.77)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.42 (1.59)	mAP 44.37 (45.94)
Train: [300/2566]	Time 0.331 (0.332)	Loss 1.79 (1.60)	mAP 46.52 (46.04)
Train: [400/2566]	Time 0.329 (0.332)	Loss 1.49 (1.60)	mAP 46.29 (45.76)
Train: [500/2566]	Time 0.329 (0.331)	Loss 1.29 (1.59)	mAP 44.98 (45.74)
Train: [600/2566]	Time 0.329 (0.331)	Loss 2.15 (1.60)	mAP 44.69 (45.83)
Train: [700/2566]	Time 0.332 (0.331)	Loss 1.72 (1.60)	mAP 52.13 (45.87)
Train: [800/2566]	Time 0.331 (0.331)	Loss 2.21 (1.60)	mAP 44.80 (45.88)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.57 (1.60)	mAP 45.91 (45.92)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.95 (1.61)	mAP 49.47 (45.91)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.79 (1.61)	mAP 47.29 (45.84)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.86 (1.61)	mAP 44.78 (45.87)
Train: [1300/2566]	Time 0.331 (0.331)	Loss 1.31 (1.61)	mAP 40.78 (45.88)
Train: [1400/2566]	Time 0.330 (0.331)	Loss 1.47 (1.61)	mAP 42.74 (45.89)
Train: [1500/2566]	Time 0.331 (0.331)	Loss 1.86 (1.61)	mAP 48.68 (45.86)
Train: [1600/2566]	Time 0.330 (0.331)	Loss 1.52 (1.61)	mAP 44.38 (45.90)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.48 (1.61)	mAP 47.53 (45.87)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.42 (1.61)	mAP 41.21 (45.90)
Train: [1900/2566]	Time 0.330 (0.331)	Loss 1.57 (1.61)	mAP 49.75 (45.86)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.66 (1.61)	mAP 47.79 (45.82)
Train: [2100/2566]	Time 0.331 (0.331)	Loss 1.62 (1.61)	mAP 47.70 (45.80)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.96 (1.61)	mAP 47.36 (45.76)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.55 (1.61)	mAP 42.21 (45.78)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.56 (1.61)	mAP 42.90 (45.76)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.98 (1.61)	mAP 48.14 (45.78)
Train: [47/50]	Time 0.330	Loss 1.61 	mAP 45.78
Test: [0/402]	Time 1.825 (1.825)	Precision 75.34 (75.34)	Recall 78.98 (78.98) 	 P_C 29.51 	 R_C 31.79 	 F_C 29.38 	 P_O 75.34 	 R_O 78.98 	 F_O 77.12
Test: [100/402]	Time 0.500 (0.513)	Precision 89.61 (79.16)	Recall 90.20 (84.31) 	 P_C 68.05 	 R_C 71.30 	 F_C 68.36 	 P_O 77.59 	 R_O 83.05 	 F_O 80.23
Test: [200/402]	Time 0.500 (0.506)	Precision 84.10 (80.06)	Recall 87.01 (84.65) 	 P_C 70.40 	 R_C 73.89 	 F_C 71.45 	 P_O 78.91 	 R_O 83.68 	 F_O 81.23
Test: [300/402]	Time 0.500 (0.504)	Precision 83.84 (80.03)	Recall 82.59 (85.29) 	 P_C 75.21 	 R_C 79.31 	 F_C 76.66 	 P_O 78.82 	 R_O 84.28 	 F_O 81.46
Test: [400/402]	Time 0.500 (0.503)	Precision 62.44 (79.17)	Recall 71.51 (85.28) 	 P_C 77.10 	 R_C 80.90 	 F_C 78.52 	 P_O 78.15 	 R_O 84.49 	 F_O 81.20
Test: [47/50]	  P_C 77.10 	 R_C 80.90 	 F_C 78.52 	 P_O 78.14 	 R_O 84.49 	 F_O 81.19 	 mAP 85.09
Train: [0/2566]	Time 1.019 (1.019)	Loss 1.13 (1.13)	mAP 44.06 (44.06)
Train: [100/2566]	Time 0.331 (0.337)	Loss 1.51 (1.59)	mAP 44.15 (45.63)
Train: [200/2566]	Time 0.331 (0.334)	Loss 1.17 (1.58)	mAP 42.03 (45.56)
Train: [300/2566]	Time 0.331 (0.333)	Loss 2.05 (1.59)	mAP 44.98 (45.67)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.30 (1.61)	mAP 46.64 (45.75)
Train: [500/2566]	Time 0.330 (0.332)	Loss 1.27 (1.61)	mAP 47.85 (45.78)
Train: [600/2566]	Time 0.330 (0.332)	Loss 1.51 (1.61)	mAP 47.42 (45.71)
Train: [700/2566]	Time 0.331 (0.331)	Loss 1.65 (1.61)	mAP 41.25 (45.64)
Train: [800/2566]	Time 0.330 (0.331)	Loss 1.48 (1.61)	mAP 54.71 (45.62)
Train: [900/2566]	Time 0.330 (0.331)	Loss 1.27 (1.60)	mAP 46.58 (45.68)
Train: [1000/2566]	Time 0.331 (0.331)	Loss 1.59 (1.60)	mAP 50.58 (45.64)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.90 (1.60)	mAP 47.80 (45.69)
Train: [1200/2566]	Time 0.331 (0.331)	Loss 1.39 (1.60)	mAP 38.91 (45.72)
Train: [1300/2566]	Time 0.332 (0.331)	Loss 1.24 (1.60)	mAP 35.52 (45.76)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.33 (1.60)	mAP 45.80 (45.75)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.40 (1.60)	mAP 50.04 (45.79)
Train: [1600/2566]	Time 0.331 (0.331)	Loss 1.23 (1.60)	mAP 39.78 (45.77)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.63 (1.60)	mAP 46.80 (45.72)
Train: [1800/2566]	Time 0.331 (0.331)	Loss 1.33 (1.60)	mAP 41.91 (45.74)
Train: [1900/2566]	Time 0.332 (0.331)	Loss 1.42 (1.60)	mAP 52.70 (45.72)
Train: [2000/2566]	Time 0.331 (0.331)	Loss 1.57 (1.60)	mAP 41.84 (45.74)
Train: [2100/2566]	Time 0.332 (0.331)	Loss 1.71 (1.60)	mAP 42.95 (45.71)
Train: [2200/2566]	Time 0.331 (0.331)	Loss 1.57 (1.60)	mAP 47.09 (45.74)
Train: [2300/2566]	Time 0.333 (0.331)	Loss 1.86 (1.60)	mAP 49.92 (45.76)
Train: [2400/2566]	Time 0.333 (0.331)	Loss 1.92 (1.60)	mAP 48.00 (45.76)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.85 (1.60)	mAP 46.88 (45.78)
Train: [48/50]	Time 0.331	Loss 1.60 	mAP 45.77
Test: [0/402]	Time 1.769 (1.769)	Precision 73.42 (73.42)	Recall 79.26 (79.26) 	 P_C 27.94 	 R_C 32.04 	 F_C 28.63 	 P_O 73.42 	 R_O 79.26 	 F_O 76.23
Test: [100/402]	Time 0.500 (0.513)	Precision 88.54 (78.15)	Recall 90.85 (84.77) 	 P_C 66.86 	 R_C 71.80 	 F_C 68.02 	 P_O 76.55 	 R_O 83.58 	 F_O 79.91
Test: [200/402]	Time 0.500 (0.507)	Precision 84.02 (79.00)	Recall 88.74 (85.08) 	 P_C 69.27 	 R_C 74.42 	 F_C 71.07 	 P_O 77.83 	 R_O 84.16 	 F_O 80.87
Test: [300/402]	Time 0.500 (0.505)	Precision 83.50 (79.03)	Recall 83.08 (85.72) 	 P_C 74.18 	 R_C 79.89 	 F_C 76.39 	 P_O 77.81 	 R_O 84.75 	 F_O 81.13
Test: [400/402]	Time 0.501 (0.504)	Precision 61.06 (78.22)	Recall 73.84 (85.68) 	 P_C 76.17 	 R_C 81.49 	 F_C 78.36 	 P_O 77.20 	 R_O 84.93 	 F_O 80.88
Test: [48/50]	  P_C 76.17 	 R_C 81.48 	 F_C 78.36 	 P_O 77.19 	 R_O 84.93 	 F_O 80.88 	 mAP 85.09
Train: [0/2566]	Time 1.028 (1.028)	Loss 1.51 (1.51)	mAP 45.58 (45.58)
Train: [100/2566]	Time 0.332 (0.338)	Loss 1.35 (1.61)	mAP 45.06 (45.78)
Train: [200/2566]	Time 0.330 (0.335)	Loss 1.42 (1.62)	mAP 45.60 (45.58)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.78 (1.62)	mAP 50.06 (45.58)
Train: [400/2566]	Time 0.331 (0.333)	Loss 1.56 (1.61)	mAP 51.34 (45.62)
Train: [500/2566]	Time 0.336 (0.333)	Loss 1.49 (1.61)	mAP 50.61 (45.64)
Train: [600/2566]	Time 0.331 (0.332)	Loss 1.80 (1.61)	mAP 51.96 (45.57)
Train: [700/2566]	Time 0.330 (0.332)	Loss 1.19 (1.62)	mAP 47.42 (45.69)
Train: [800/2566]	Time 0.332 (0.332)	Loss 1.65 (1.62)	mAP 48.27 (45.71)
Train: [900/2566]	Time 0.331 (0.332)	Loss 1.65 (1.62)	mAP 42.83 (45.72)
Train: [1000/2566]	Time 0.333 (0.332)	Loss 1.63 (1.62)	mAP 42.84 (45.68)
Train: [1100/2566]	Time 0.330 (0.332)	Loss 1.73 (1.61)	mAP 44.85 (45.68)
Train: [1200/2566]	Time 0.331 (0.332)	Loss 1.50 (1.61)	mAP 43.02 (45.66)
Train: [1300/2566]	Time 0.332 (0.332)	Loss 1.76 (1.61)	mAP 41.98 (45.67)
Train: [1400/2566]	Time 0.335 (0.332)	Loss 2.35 (1.61)	mAP 53.20 (45.67)
Train: [1500/2566]	Time 0.330 (0.332)	Loss 1.71 (1.61)	mAP 49.54 (45.66)
Train: [1600/2566]	Time 0.331 (0.332)	Loss 1.27 (1.61)	mAP 46.39 (45.67)
Train: [1700/2566]	Time 0.331 (0.332)	Loss 1.46 (1.61)	mAP 50.20 (45.63)
Train: [1800/2566]	Time 0.331 (0.332)	Loss 1.49 (1.61)	mAP 51.06 (45.66)
Train: [1900/2566]	Time 0.331 (0.332)	Loss 1.46 (1.61)	mAP 46.46 (45.68)
Train: [2000/2566]	Time 0.331 (0.332)	Loss 1.54 (1.61)	mAP 44.62 (45.71)
Train: [2100/2566]	Time 0.330 (0.332)	Loss 1.28 (1.61)	mAP 45.99 (45.69)
Train: [2200/2566]	Time 0.330 (0.332)	Loss 1.19 (1.61)	mAP 46.26 (45.71)
Train: [2300/2566]	Time 0.331 (0.331)	Loss 1.28 (1.61)	mAP 47.42 (45.75)
Train: [2400/2566]	Time 0.330 (0.331)	Loss 1.96 (1.61)	mAP 41.41 (45.76)
Train: [2500/2566]	Time 0.336 (0.331)	Loss 1.83 (1.61)	mAP 51.28 (45.75)
Train: [49/50]	Time 0.331	Loss 1.61 	mAP 45.75
Test: [0/402]	Time 1.862 (1.862)	Precision 74.73 (74.73)	Recall 78.98 (78.98) 	 P_C 29.37 	 R_C 31.79 	 F_C 29.29 	 P_O 74.73 	 R_O 78.98 	 F_O 76.80
Test: [100/402]	Time 0.500 (0.514)	Precision 89.61 (78.69)	Recall 90.20 (84.48) 	 P_C 67.34 	 R_C 71.47 	 F_C 68.10 	 P_O 77.12 	 R_O 83.26 	 F_O 80.07
Test: [200/402]	Time 0.503 (0.507)	Precision 83.82 (79.46)	Recall 87.45 (84.87) 	 P_C 69.83 	 R_C 74.11 	 F_C 71.23 	 P_O 78.30 	 R_O 83.92 	 F_O 81.01
Test: [300/402]	Time 0.500 (0.505)	Precision 83.75 (79.47)	Recall 83.33 (85.53) 	 P_C 74.75 	 R_C 79.63 	 F_C 76.55 	 P_O 78.25 	 R_O 84.53 	 F_O 81.27
Test: [400/402]	Time 0.500 (0.504)	Precision 61.08 (78.60)	Recall 72.09 (85.49) 	 P_C 76.61 	 R_C 81.21 	 F_C 78.41 	 P_O 77.59 	 R_O 84.72 	 F_O 81.00
Test: [49/50]	  P_C 76.60 	 R_C 81.20 	 F_C 78.41 	 P_O 77.58 	 R_O 84.72 	 F_O 80.99 	 mAP 85.08
Train: [0/2566]	Time 1.031 (1.031)	Loss 1.35 (1.35)	mAP 43.35 (43.35)
Train: [100/2566]	Time 0.333 (0.338)	Loss 1.29 (1.65)	mAP 38.49 (45.73)
Train: [200/2566]	Time 0.330 (0.334)	Loss 1.49 (1.63)	mAP 42.10 (45.65)
Train: [300/2566]	Time 0.331 (0.333)	Loss 1.47 (1.62)	mAP 41.86 (45.62)
Train: [400/2566]	Time 0.330 (0.332)	Loss 1.37 (1.63)	mAP 44.61 (45.74)
Train: [500/2566]	Time 0.329 (0.332)	Loss 1.48 (1.63)	mAP 51.16 (45.72)
Train: [600/2566]	Time 0.329 (0.332)	Loss 1.78 (1.62)	mAP 50.36 (45.70)
Train: [700/2566]	Time 0.330 (0.331)	Loss 1.69 (1.62)	mAP 48.86 (45.76)
Train: [800/2566]	Time 0.331 (0.331)	Loss 1.55 (1.63)	mAP 44.54 (45.78)
Train: [900/2566]	Time 0.330 (0.331)	Loss 2.23 (1.63)	mAP 44.91 (45.77)
Train: [1000/2566]	Time 0.330 (0.331)	Loss 1.31 (1.62)	mAP 48.47 (45.78)
Train: [1100/2566]	Time 0.330 (0.331)	Loss 1.48 (1.63)	mAP 44.65 (45.84)
Train: [1200/2566]	Time 0.330 (0.331)	Loss 1.53 (1.62)	mAP 39.65 (45.86)
Train: [1300/2566]	Time 0.330 (0.331)	Loss 1.81 (1.62)	mAP 48.47 (45.84)
Train: [1400/2566]	Time 0.331 (0.331)	Loss 1.62 (1.62)	mAP 44.47 (45.78)
Train: [1500/2566]	Time 0.330 (0.331)	Loss 1.92 (1.61)	mAP 44.04 (45.78)
Train: [1600/2566]	Time 0.329 (0.331)	Loss 1.53 (1.61)	mAP 43.31 (45.76)
Train: [1700/2566]	Time 0.330 (0.331)	Loss 1.70 (1.61)	mAP 45.74 (45.76)
Train: [1800/2566]	Time 0.330 (0.331)	Loss 1.50 (1.61)	mAP 42.86 (45.75)
Train: [1900/2566]	Time 0.332 (0.331)	Loss 1.43 (1.61)	mAP 47.96 (45.75)
Train: [2000/2566]	Time 0.330 (0.331)	Loss 1.73 (1.61)	mAP 52.30 (45.77)
Train: [2100/2566]	Time 0.330 (0.331)	Loss 1.34 (1.61)	mAP 42.93 (45.77)
Train: [2200/2566]	Time 0.331 (0.330)	Loss 1.71 (1.61)	mAP 45.51 (45.76)
Train: [2300/2566]	Time 0.330 (0.330)	Loss 1.29 (1.61)	mAP 43.25 (45.78)
Train: [2400/2566]	Time 0.331 (0.330)	Loss 1.60 (1.60)	mAP 45.63 (45.82)
Train: [2500/2566]	Time 0.331 (0.331)	Loss 1.43 (1.60)	mAP 38.17 (45.79)
Train: [50/50]	Time 0.330	Loss 1.60 	mAP 45.80
Test: [0/402]	Time 1.804 (1.804)	Precision 74.73 (74.73)	Recall 78.98 (78.98) 	 P_C 29.29 	 R_C 31.79 	 F_C 29.28 	 P_O 74.73 	 R_O 78.98 	 F_O 76.80
Test: [100/402]	Time 0.500 (0.513)	Precision 89.68 (78.33)	Recall 90.85 (84.66) 	 P_C 67.40 	 R_C 71.62 	 F_C 68.19 	 P_O 76.74 	 R_O 83.47 	 F_O 79.96
Test: [200/402]	Time 0.499 (0.506)	Precision 84.10 (79.28)	Recall 87.01 (84.99) 	 P_C 69.88 	 R_C 74.29 	 F_C 71.32 	 P_O 78.09 	 R_O 84.06 	 F_O 80.97
Test: [300/402]	Time 0.500 (0.504)	Precision 83.50 (79.26)	Recall 83.08 (85.64) 	 P_C 74.66 	 R_C 79.73 	 F_C 76.55 	 P_O 77.97 	 R_O 84.69 	 F_O 81.19
Test: [400/402]	Time 0.500 (0.503)	Precision 61.58 (78.36)	Recall 72.67 (85.65) 	 P_C 76.50 	 R_C 81.30 	 F_C 78.38 	 P_O 77.27 	 R_O 84.91 	 F_O 80.91
Test: [50/50]	  P_C 76.49 	 R_C 81.30 	 F_C 78.37 	 P_O 77.26 	 R_O 84.91 	 F_O 80.91 	 mAP 85.09
Evaluating the best model
Evaluate with threshold 0.50
... loading pretrained weights from ./output/coco-DualCoop-RN101-cosine-bs32-e50/model_best.pth.tar
Test: [0/402]	Time 1.891 (1.891)	Precision 74.73 (74.73)	Recall 78.98 (78.98) 	 P_C 29.29 	 R_C 31.79 	 F_C 29.28 	 P_O 74.73 	 R_O 78.98 	 F_O 76.80
Test: [100/402]	Time 0.500 (0.513)	Precision 89.68 (78.33)	Recall 90.85 (84.66) 	 P_C 67.40 	 R_C 71.62 	 F_C 68.19 	 P_O 76.74 	 R_O 83.47 	 F_O 79.96
Test: [200/402]	Time 0.500 (0.507)	Precision 84.10 (79.28)	Recall 87.01 (84.99) 	 P_C 69.88 	 R_C 74.29 	 F_C 71.32 	 P_O 78.09 	 R_O 84.06 	 F_O 80.97
Test: [300/402]	Time 0.506 (0.505)	Precision 83.50 (79.26)	Recall 83.08 (85.64) 	 P_C 74.66 	 R_C 79.73 	 F_C 76.55 	 P_O 77.97 	 R_O 84.69 	 F_O 81.19
Test: [400/402]	Time 0.501 (0.504)	Precision 61.58 (78.36)	Recall 72.67 (85.65) 	 P_C 76.50 	 R_C 81.30 	 F_C 78.38 	 P_O 77.27 	 R_O 84.91 	 F_O 80.91
Test: [50/50]	  P_C 76.49 	 R_C 81.30 	 F_C 78.37 	 P_O 77.26 	 R_O 84.91 	 F_O 80.91 	 mAP 85.09
