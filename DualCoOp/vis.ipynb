{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from  matplotlib import pyplot as plt\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torchvision.transforms import InterpolationMode\n",
    "BICUBIC = InterpolationMode.BICUBIC\n",
    "# from segment_anything import sam_model_registry, SamPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _ = clip.load(\"RN101\", device=device)\n",
    "model.eval()\n",
    "preprocess =  Compose([Resize((224, 224), interpolation=BICUBIC), ToTensor(),\n",
    "    Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_img = Image.open(\"/home/samyakr2/Redundancy/DualCoOp/demo.jpg\") # /home/samyakr2/Redundancy/demo.jpg\n",
    "cv2_img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "image = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "all_texts = ['person'] #['','airplane', 'bag', 'bed', 'bedclothes', 'bench', 'bicycle', 'bird', 'boat', 'book', 'bottle', 'building', 'bus', 'cabinet', 'car', 'cat', 'ceiling', 'chair', 'cloth', 'computer', 'cow', 'cup', 'curtain', 'dog', 'door', 'fence', 'floor', 'flower', 'food', 'grass', 'ground', 'horse', 'keyboard', 'light', 'motorbike', 'mountain', 'mouse', 'person', 'plate', 'platform', 'potted plant', 'road', 'rock', 'sheep', 'shelves', 'sidewalk', 'sign', 'sky', 'snow', 'sofa', 'table', 'track', 'train', 'tree', 'truck', 'tv monitor', 'wall', 'water', 'window', 'wood']\n",
    "target_texts = ['','bench', 'person', 'ground', 'building']\n",
    "\n",
    "\n",
    "all_texts_captions = []\n",
    "all_texts_test = ['person', 'not person']\n",
    "for ele in all_texts_test:\n",
    "    all_texts_captions.append('A photo of ' + ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'clip' has no attribute 'encode_text_with_prompt_ensemble'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m image_features \u001b[38;5;241m/\u001b[39m image_features\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     print('image_features', image_features.shape)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Prompt ensemble for text features with normalization\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mencode_text_with_prompt_ensemble(model, all_texts, device)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     print('text_features', text_features.shape)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Similarity map from image tokens with min-max norm and resize, B,H,W,N\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     features \u001b[38;5;241m=\u001b[39m image_features \u001b[38;5;241m@\u001b[39m text_features\u001b[38;5;241m.\u001b[39mt()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'clip' has no attribute 'encode_text_with_prompt_ensemble'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Extract image features\n",
    "    image_features = model.encode_image(image)\n",
    "    image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "#     print('image_features', image_features.shape)\n",
    "    # Prompt ensemble for text features with normalization\n",
    "    text_features = clip.encode_text_with_prompt_ensemble(model, all_texts, device)\n",
    "#     print('text_features', text_features.shape)\n",
    "    \n",
    "    # Similarity map from image tokens with min-max norm and resize, B,H,W,N\n",
    "    features = image_features @ text_features.t()\n",
    "    similarity_map = clip.get_similarity_map(features[:, 1:, :], cv2_img.shape[:2])\n",
    "\n",
    "    # Draw similarity map\n",
    "    for b in range(similarity_map.shape[0]):\n",
    "        for n in range(similarity_map.shape[-1]):\n",
    "            if all_texts[n] not in target_texts:\n",
    "                continue\n",
    "            vis = (similarity_map[b, :, :, n].cpu().numpy() * 255).astype('uint8')\n",
    "            vis = cv2.applyColorMap(vis, cv2.COLORMAP_JET)\n",
    "            vis = cv2_img * 0.4 + vis * 0.6\n",
    "            vis = cv2.cvtColor(vis.astype('uint8'), cv2.COLOR_BGR2RGB)\n",
    "            print('CLIP:', all_texts[n])\n",
    "            plt.imshow(vis)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Extract image features\n",
    "    image_features = model.encode_image(image)\n",
    "    image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "#     print('image_features', image_features.shape)\n",
    "    # Prompt ensemble for text features with normalization\n",
    "    text_features = clip.encode_text_with_prompt_ensemble(model, all_texts_captions, device)\n",
    "#     print('text_features', text_features.shape)\n",
    "    \n",
    "    # Similarity map from image tokens with min-max norm and resize, B,H,W,N\n",
    "    features = image_features @ text_features.t()\n",
    "    similarity_map = clip.get_similarity_map(features[:, 1:, :], cv2_img.shape[:2])\n",
    "\n",
    "    # Draw similarity map\n",
    "    for b in range(similarity_map.shape[0]):\n",
    "        for n in range(similarity_map.shape[-1]):\n",
    "#             if all_texts[n] not in target_texts:\n",
    "#                 continue\n",
    "            vis = (similarity_map[b, :, :, n].cpu().numpy() * 255).astype('uint8')\n",
    "            vis = cv2.applyColorMap(vis, cv2.COLORMAP_JET)\n",
    "            vis = cv2_img * 0.4 + vis * 0.6\n",
    "            vis = cv2.cvtColor(vis.astype('uint8'), cv2.COLOR_BGR2RGB)\n",
    "            print('CLIP:', all_texts[n])\n",
    "            plt.imshow(vis)\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
